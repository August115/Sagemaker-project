{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26151a6",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e8afb4",
   "metadata": {},
   "source": [
    "In an investment organization, analysts read news articles to help make investment\n",
    "decisions. To streamline the process of selecting articles a model is needed to select\n",
    "semantic information from the data, identify similar articles from the set and then\n",
    "provide the recommendations to analysts on what articles to read next. A\n",
    "20newsgroups dataset containing 20 topics in approximately 20,000 documents and\n",
    "AWS services such as: Amazon SageMaker, Amazon SageMaker Notebooks,\n",
    "Amazon SageMaker Built-in Algorithms, AWS SDK for Python 3, Amazon S3,\n",
    "WordPress, and Amazon LightSail will be used to develop the model and evaluate the\n",
    "results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ca749",
   "metadata": {},
   "source": [
    "## **Dataset**\n",
    "\n",
    "The ML model will be trained on the 20newsgroups dataset - a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The 20newsgroups dataset is curated by Carnegie Mellon University School of Computer Science and publicly available from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d6a96",
   "metadata": {},
   "source": [
    "## **Process**\n",
    "\n",
    "The scikit-learn dataset is naturally broken down into a training set and test set. Using APIs provided by scikit-learn the dataset is striped of headers, footers, and quotes to preprocess the raw text data into machine readable numeric values. \n",
    "\n",
    "As shown below, the entries in the dataset are plain text paragraphs. They will be processes into a suitable data format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8711ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit_learn==0.22.2.post1\n",
      "  Downloading scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit_learn==0.22.2.post1) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit_learn==0.22.2.post1) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit_learn==0.22.2.post1) (1.5.3)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "Successfully installed scikit-learn-0.22.2.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"scikit_learn==0.22.2.post1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32db96f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.twenty_newsgroups module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')['data']\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test')['data']\n",
    "NUM_TOPICS = 30\n",
    "NUM_NEIGHBORS = 10\n",
    "BUCKET = 'sagemaker-aw'\n",
    "PREFIX = '20newsgroups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f969ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(newsgroups_train)):\n",
    "    newsgroups_train[i] = strip_newsgroup_header(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_quoting(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_footer(newsgroups_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5994319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3cc162",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "For the data to be machine readable it needs to be tokenized to a numeric format by assigning a token (integer id) to each word in a sentence. The total number of tokens can be limited to 2000 by counting the most frequent tokens and retaining the top 2000. This could be done because less frequent words can be ignored because they will have a diminishing impact on the topic model. \n",
    "\n",
    "A Bags of Words (BoW) model can be used to convert the documents into a vector to keep track of the amount of times a token appears in the training example. WordNetLemmatizer, from the nltk package, and CountVectorizer are used to token count. Lemmatization aims to return actual words by using nouns for lemmatizing words into lemmas. \n",
    "\n",
    "The rule to consider only words longer than two characters, that start with a letter, and match the token pattern is used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85401177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc8493",
   "metadata": {},
   "source": [
    "The CountVectorizer API uses three hyperparameters to help train the model.\n",
    "* Max_features – used to set the vocabulary size. Limited to 2000 because a large vocabulary with infrequent words would add noise to the data.\n",
    "* Max_df – ignores words that occur in more than max_df% of documents. Ensures most extremely frequent words are removed. Removes the few words that occur in all of the documents\n",
    "* Min_dif – ignores words that occur less than min_dif% of documents. Ensures that extremely rare words are not included. \n",
    "\n",
    "The BoW vectors are shuffled to generate training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d68bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2000\n",
      "Done. Time elapsed: 38.90s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "\n",
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "newidx = np.random.permutation(idx) # this will be the labels fed into the KNN model for training\n",
    "# Need to store these permutations:\n",
    "\n",
    "vectors = vectors[newidx]\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e74897",
   "metadata": {},
   "source": [
    "Because all the parameters in the NTM model are np.float32 type the input data needs to also be in np.float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d730b685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2469e8",
   "metadata": {},
   "source": [
    "Now the data is split into training data (80%) and testing data (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b7269c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 2000) (2263, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Convert data into training and validation data\n",
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = vectors[:n_train, :]\n",
    "val_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "\n",
    "print(train_vectors.shape,val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b807741",
   "metadata": {},
   "source": [
    "The training, validation and output paths are defined and data uploaded to S3 bucket for the model to access during training. \n",
    "Write_spmatrix_to_sparse_tensor is used to convert scipy sparse matrix (raw vectors) into RcordIO Protobuf format to upload to the S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b51e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://sagemaker-aw/20newsgroups/train\n",
      "Validation set location s3://sagemaker-aw/20newsgroups/val\n",
      "Trained model will be saved at s3://sagemaker-aw/20newsgroups/output\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50c84d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part0.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part1.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part2.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part3.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part4.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part5.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part6.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/train/train_part7.pbr\n",
      "Uploaded data to s3://sagemaker-aw/20newsgroups/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))\n",
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f64f9",
   "metadata": {},
   "source": [
    "To use the built-in SageMaker algorithms the location of the NGM container in ECR needs to be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2357749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4607a84",
   "metadata": {},
   "source": [
    "In the API call to sagemaker.estimator.Estimator the type and count of instances is specified for the training job. Since the dataset is small a CPU instance is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ff9a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=2, \n",
    "                                    instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c4c27",
   "metadata": {},
   "source": [
    "Now the hyperparameters for the topic model are to be set. \n",
    "* num_topics – number of topics to be extracted\n",
    "* feature_dim – set to the vocabulary size\n",
    "* mini_batch_size – batch size for each worker instance\n",
    "* epochs – max number of epochs to train for\n",
    "* num_patience_epochs / tolerance – controls early stopping behavior. Improvements smaller than tolerance will be considered non improvement and the algorithm will stop training if within the last epochs there were no imporvemnts on validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159eec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(num_topics=NUM_TOPICS, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c10aa3",
   "metadata": {},
   "source": [
    "To have each worker go through a different portion of the full dataset to provide different gradients within epochs the distribution is specified to be ShardedByS3Key for the training data channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "626e90d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 05:59:42 Starting - Starting the training job...\n",
      "2021-06-14 05:59:54 Starting - Launching requested ML instancesProfilerReport-1623650381: InProgress\n",
      "......\n",
      "2021-06-14 06:01:05 Starting - Preparing the instances for training......\n",
      "2021-06-14 06:02:11 Downloading - Downloading input data...\n",
      "2021-06-14 06:02:39 Training - Downloading the training image...\n",
      "2021-06-14 06:03:03 Training - Training image download completed. Training in progress.\u001b[35mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2000', 'num_topics': '30', 'num_patience_epochs': '5', 'epochs': '100', 'tolerance': '0.001', 'mini_batch_size': '128'}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '128', 'epochs': '100', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '5', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '2000', 'num_topics': '30'}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] nvidia-smi: took 0.028 seconds to run.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Launching parameter server for role server\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-207-136.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/884153b6-d355-423f-b068-2b6ec29ab9f0', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/1034aba2-a9da-4e99-a3f3-bc4e84d0c66d', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/1034aba2-a9da-4e99-a3f3-bc4e84d0c66d', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-207-136.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/884153b6-d355-423f-b068-2b6ec29ab9f0', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/1034aba2-a9da-4e99-a3f3-bc4e84d0c66d', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/1034aba2-a9da-4e99-a3f3-bc4e84d0c66d', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.0.229.127', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-207-136.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/884153b6-d355-423f-b068-2b6ec29ab9f0', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/1034aba2-a9da-4e99-a3f3-bc4e84d0c66d', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/1034aba2-a9da-4e99-a3f3-bc4e84d0c66d', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.0.229.127', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[35mProcess 33 is a shell:server.\u001b[0m\n",
      "\u001b[35mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Using default worker.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:04.774] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Initializing\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] None\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] vocab.txt\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Vocab file is not provided\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:04 INFO 140618438534976] Create Store: dist_async\u001b[0m\n",
      "\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2000', 'num_topics': '30', 'num_patience_epochs': '5', 'epochs': '100', 'tolerance': '0.001', 'mini_batch_size': '128'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '128', 'epochs': '100', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '5', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '2000', 'num_topics': '30'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] nvidia-smi: took 0.028 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-229-127.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/483702d8-981e-465b-9596-b215cd0a58cd', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc08929d-57ff-4110-8c88-63772a465560', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/bc08929d-57ff-4110-8c88-63772a465560', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-229-127.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/483702d8-981e-465b-9596-b215cd0a58cd', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc08929d-57ff-4110-8c88-63772a465560', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/bc08929d-57ff-4110-8c88-63772a465560', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'scheduler', 'DMLC_PS_ROOT_URI': '10.0.229.127', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-229-127.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/483702d8-981e-465b-9596-b215cd0a58cd', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc08929d-57ff-4110-8c88-63772a465560', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/bc08929d-57ff-4110-8c88-63772a465560', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-229-127.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/483702d8-981e-465b-9596-b215cd0a58cd', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc08929d-57ff-4110-8c88-63772a465560', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/bc08929d-57ff-4110-8c88-63772a465560', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.0.229.127', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-0-229-127.ec2.internal', 'TRAINING_JOB_NAME': 'ntm-2021-06-14-05-59-41-841', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/ntm-2021-06-14-05-59-41-841', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/483702d8-981e-465b-9596-b215cd0a58cd', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/bc08929d-57ff-4110-8c88-63772a465560', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/bc08929d-57ff-4110-8c88-63772a465560', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.0.229.127', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '2', 'DMLC_NUM_WORKER': '2'}\u001b[0m\n",
      "\u001b[34mProcess 34 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 35 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:06.635] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Initializing\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] None\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] vocab.txt\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Vocab file is not provided\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:06 INFO 140515461125952] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650587.4062383, \"EndTime\": 1623650587.4062674, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:07.406] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 778, \"num_examples\": 1, \"num_bytes\": 33712}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:07 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:07 INFO 140515461125952] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650587.4534683, \"EndTime\": 1623650587.4534976, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:07.453] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 2689, \"num_examples\": 1, \"num_bytes\": 23284}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:07 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:07 INFO 140618438534976] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:08.654] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1246, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] # Finished training epoch 1 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] Loss (name: value) total: 6.9803469975789385\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] Loss (name: value) kld: 0.023227549623697996\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] Loss (name: value) recons: 6.957119497987959\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] Loss (name: value) logppx: 6.9803469975789385\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.9803469975789385\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650587.4067168, \"EndTime\": 1623650588.6604433, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3608.062441529519 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:08 INFO 140515461125952] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:08.717] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1263, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] # Finished training epoch 1 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] Loss (name: value) total: 6.9550830192036095\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] Loss (name: value) kld: 0.014925423925483806\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] Loss (name: value) recons: 6.940157625410292\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] Loss (name: value) logppx: 6.9550830192036095\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=6.9550830192036095\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] #progress_metric: host=algo-2, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650587.4539351, \"EndTime\": 1623650588.7230153, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3566.6620785752298 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:08 INFO 140618438534976] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:10.061] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1337, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] # Finished training epoch 2 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] Loss (name: value) total: 6.876203411155277\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] Loss (name: value) kld: 0.002089997909529807\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] Loss (name: value) recons: 6.874113480250041\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] Loss (name: value) logppx: 6.876203411155277\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=6.876203411155277\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] Timing: train: 1.34s, val: 0.00s, epoch: 1.34s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] #progress_metric: host=algo-2, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650588.7233155, \"EndTime\": 1623650590.0664377, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9054.0, \"count\": 1, \"min\": 9054, \"max\": 9054}, \"Total Batches Seen\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3370.221849875469 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:10 INFO 140618438534976] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:11.470] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1402, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] # Finished training epoch 3 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] Loss (name: value) total: 6.873073372575972\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] Loss (name: value) kld: 0.005471322610295222\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] Loss (name: value) recons: 6.867602056927151\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] Loss (name: value) logppx: 6.873073372575972\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=6.873073372575972\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] Timing: train: 1.41s, val: 0.00s, epoch: 1.41s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] #progress_metric: host=algo-2, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650590.0666463, \"EndTime\": 1623650591.4757645, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13581.0, \"count\": 1, \"min\": 13581, \"max\": 13581}, \"Total Batches Seen\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3212.3540103640617 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:11 INFO 140618438534976] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:09.964] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1303, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] # Finished training epoch 2 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] Loss (name: value) total: 6.891729619767931\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] Loss (name: value) kld: 0.0021757399291446847\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] Loss (name: value) recons: 6.889553851551479\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] Loss (name: value) logppx: 6.891729619767931\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.891729619767931\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] Timing: train: 1.31s, val: 0.01s, epoch: 1.31s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650588.6607063, \"EndTime\": 1623650589.9745615, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9048.0, \"count\": 1, \"min\": 9048, \"max\": 9048}, \"Total Batches Seen\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3441.9658836894982 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:09 INFO 140515461125952] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:11.310] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1332, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] # Finished training epoch 3 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] Loss (name: value) total: 6.8869218826293945\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] Loss (name: value) kld: 0.004959050481071851\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] Loss (name: value) recons: 6.881962789429559\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] Loss (name: value) logppx: 6.8869218826293945\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.8869218826293945\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] Timing: train: 1.34s, val: 0.00s, epoch: 1.34s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650589.9755163, \"EndTime\": 1623650591.3157542, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13572.0, \"count\": 1, \"min\": 13572, \"max\": 13572}, \"Total Batches Seen\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3375.0513231068708 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:11 INFO 140515461125952] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:12.666] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1348, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] # Finished training epoch 4 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] Loss (name: value) total: 6.878575020366245\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] Loss (name: value) kld: 0.00648623236338608\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] Loss (name: value) recons: 6.872088756826189\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] Loss (name: value) logppx: 6.878575020366245\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.878575020366245\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] Timing: train: 1.35s, val: 0.00s, epoch: 1.36s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650591.316158, \"EndTime\": 1623650592.6729279, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18096.0, \"count\": 1, \"min\": 18096, \"max\": 18096}, \"Total Batches Seen\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3334.0598196992623 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:12 INFO 140515461125952] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:12.818] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1341, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] # Finished training epoch 4 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] Loss (name: value) total: 6.862863520781199\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] Loss (name: value) kld: 0.005705883373997899\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] Loss (name: value) recons: 6.857157634364234\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] Loss (name: value) logppx: 6.862863520781199\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=6.862863520781199\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] Timing: train: 1.34s, val: 0.00s, epoch: 1.35s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] #progress_metric: host=algo-2, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650591.4760373, \"EndTime\": 1623650592.8246572, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18108.0, \"count\": 1, \"min\": 18108, \"max\": 18108}, \"Total Batches Seen\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3356.427811471976 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:12 INFO 140618438534976] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:14.253] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1579, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] # Finished training epoch 5 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] Loss (name: value) total: 6.878127449088627\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] Loss (name: value) kld: 0.008412189044368764\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] Loss (name: value) recons: 6.8697152270211115\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] Loss (name: value) logppx: 6.878127449088627\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.878127449088627\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] Timing: train: 1.58s, val: 0.01s, epoch: 1.59s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650592.6731653, \"EndTime\": 1623650594.2604675, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 22620.0, \"count\": 1, \"min\": 22620, \"max\": 22620}, \"Total Batches Seen\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=2849.206044481176 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:14 INFO 140515461125952] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:14.211] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1384, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] # Finished training epoch 5 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] Loss (name: value) total: 6.867727862464057\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] Loss (name: value) kld: 0.009460837712847732\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] Loss (name: value) recons: 6.8582670953538685\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] Loss (name: value) logppx: 6.867727862464057\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=6.867727862464057\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] Timing: train: 1.39s, val: 0.00s, epoch: 1.39s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] #progress_metric: host=algo-2, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650592.8250847, \"EndTime\": 1623650594.2133753, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 22635.0, \"count\": 1, \"min\": 22635, \"max\": 22635}, \"Total Batches Seen\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3260.4929214029744 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:14 INFO 140618438534976] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:15.515] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1252, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] # Finished training epoch 6 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] Loss (name: value) total: 6.874089346991645\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] Loss (name: value) kld: 0.01108000959114482\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] Loss (name: value) recons: 6.86300931374232\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] Loss (name: value) logppx: 6.874089346991645\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.874089346991645\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] patience losses:[6.9803469975789385, 6.891729619767931, 6.8869218826293945, 6.878575020366245, 6.878127449088627] min patience loss:6.878127449088627 current loss:6.874089346991645 absolute loss difference:0.004038102096981966\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650594.261553, \"EndTime\": 1623650595.5200665, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 27144.0, \"count\": 1, \"min\": 27144, \"max\": 27144}, \"Total Batches Seen\": {\"sum\": 216.0, \"count\": 1, \"min\": 216, \"max\": 216}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3594.239243534642 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:15 INFO 140515461125952] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:15.455] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1241, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] # Finished training epoch 6 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] Loss (name: value) total: 6.857095082600911\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] Loss (name: value) kld: 0.009423197654541582\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] Loss (name: value) recons: 6.847671826680501\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] Loss (name: value) logppx: 6.857095082600911\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=6.857095082600911\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] patience losses:[6.9550830192036095, 6.876203411155277, 6.873073372575972, 6.862863520781199, 6.867727862464057] min patience loss:6.862863520781199 current loss:6.857095082600911 absolute loss difference:0.005768438180288271\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] #progress_metric: host=algo-2, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650594.2136827, \"EndTime\": 1623650595.459927, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 27162.0, \"count\": 1, \"min\": 27162, \"max\": 27162}, \"Total Batches Seen\": {\"sum\": 216.0, \"count\": 1, \"min\": 216, \"max\": 216}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3632.0662431337864 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:15 INFO 140618438534976] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:16.766] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1246, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] # Finished training epoch 7 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] Loss (name: value) total: 6.875245557890998\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] Loss (name: value) kld: 0.01250037387944758\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] Loss (name: value) recons: 6.86274520556132\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] Loss (name: value) logppx: 6.875245557890998\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.875245557890998\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] patience losses:[6.891729619767931, 6.8869218826293945, 6.878575020366245, 6.878127449088627, 6.874089346991645] min patience loss:6.874089346991645 current loss:6.875245557890998 absolute loss difference:0.0011562108993530273\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650595.5203989, \"EndTime\": 1623650596.7682908, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 31668.0, \"count\": 1, \"min\": 31668, \"max\": 31668}, \"Total Batches Seen\": {\"sum\": 252.0, \"count\": 1, \"min\": 252, \"max\": 252}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3624.8569250795367 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:16 INFO 140515461125952] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:16.665] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1205, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] # Finished training epoch 7 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] Loss (name: value) total: 6.855464789602491\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] Loss (name: value) kld: 0.011566169495280419\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] Loss (name: value) recons: 6.8438986208703785\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] Loss (name: value) logppx: 6.855464789602491\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=6.855464789602491\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] patience losses:[6.876203411155277, 6.873073372575972, 6.862863520781199, 6.867727862464057, 6.857095082600911] min patience loss:6.857095082600911 current loss:6.855464789602491 absolute loss difference:0.001630292998419769\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] #progress_metric: host=algo-2, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650595.4602165, \"EndTime\": 1623650596.6700037, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 31689.0, \"count\": 1, \"min\": 31689, \"max\": 31689}, \"Total Batches Seen\": {\"sum\": 252.0, \"count\": 1, \"min\": 252, \"max\": 252}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3741.57580687496 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:16 INFO 140618438534976] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:17.902] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1231, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] # Finished training epoch 8 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] Loss (name: value) total: 6.854239218764835\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] Loss (name: value) kld: 0.013160461314126022\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] Loss (name: value) recons: 6.841078711880578\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] Loss (name: value) logppx: 6.854239218764835\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=6.854239218764835\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] patience losses:[6.873073372575972, 6.862863520781199, 6.867727862464057, 6.857095082600911, 6.855464789602491] min patience loss:6.855464789602491 current loss:6.854239218764835 absolute loss difference:0.0012255708376560648\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] Timing: train: 1.23s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] #progress_metric: host=algo-2, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650596.6704016, \"EndTime\": 1623650597.9072013, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 36216.0, \"count\": 1, \"min\": 36216, \"max\": 36216}, \"Total Batches Seen\": {\"sum\": 288.0, \"count\": 1, \"min\": 288, \"max\": 288}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3659.5328369183208 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:17 INFO 140618438534976] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:18.033] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1265, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] # Finished training epoch 8 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] Loss (name: value) total: 6.870396998193529\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] Loss (name: value) kld: 0.012252798200481467\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] Loss (name: value) recons: 6.858144190576342\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] Loss (name: value) logppx: 6.870396998193529\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.870396998193529\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] patience losses:[6.8869218826293945, 6.878575020366245, 6.878127449088627, 6.874089346991645, 6.875245557890998] min patience loss:6.874089346991645 current loss:6.870396998193529 absolute loss difference:0.003692348798115752\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650596.7685783, \"EndTime\": 1623650598.0388532, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 36192.0, \"count\": 1, \"min\": 36192, \"max\": 36192}, \"Total Batches Seen\": {\"sum\": 288.0, \"count\": 1, \"min\": 288, \"max\": 288}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3560.9593567125803 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:18 INFO 140515461125952] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:19.285] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1245, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] # Finished training epoch 9 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] Loss (name: value) total: 6.871569997734493\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] Loss (name: value) kld: 0.015068539599370625\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] Loss (name: value) recons: 6.856501426961687\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] Loss (name: value) logppx: 6.871569997734493\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.871569997734493\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] patience losses:[6.878575020366245, 6.878127449088627, 6.874089346991645, 6.875245557890998, 6.870396998193529] min patience loss:6.870396998193529 current loss:6.871569997734493 absolute loss difference:0.0011729995409641703\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650598.039582, \"EndTime\": 1623650599.2869208, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 40716.0, \"count\": 1, \"min\": 40716, \"max\": 40716}, \"Total Batches Seen\": {\"sum\": 324.0, \"count\": 1, \"min\": 324, \"max\": 324}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3626.415646366672 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:19 INFO 140515461125952] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:19.079] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1171, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] # Finished training epoch 9 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] Loss (name: value) total: 6.852634065681034\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] Loss (name: value) kld: 0.013472688740067598\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] Loss (name: value) recons: 6.839161396026611\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] Loss (name: value) logppx: 6.852634065681034\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=6.852634065681034\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] patience losses:[6.862863520781199, 6.867727862464057, 6.857095082600911, 6.855464789602491, 6.854239218764835] min patience loss:6.854239218764835 current loss:6.852634065681034 absolute loss difference:0.0016051530838012695\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] #progress_metric: host=algo-2, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650597.9076836, \"EndTime\": 1623650599.08558, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 40743.0, \"count\": 1, \"min\": 40743, \"max\": 40743}, \"Total Batches Seen\": {\"sum\": 324.0, \"count\": 1, \"min\": 324, \"max\": 324}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3842.884247254593 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:19 INFO 140618438534976] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:20.330] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1244, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] # Finished training epoch 10 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] Loss (name: value) total: 6.851705491542816\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] Loss (name: value) kld: 0.01661858449612434\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] Loss (name: value) recons: 6.835086968210009\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] Loss (name: value) logppx: 6.851705491542816\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=6.851705491542816\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] patience losses:[6.867727862464057, 6.857095082600911, 6.855464789602491, 6.854239218764835, 6.852634065681034] min patience loss:6.852634065681034 current loss:6.851705491542816 absolute loss difference:0.0009285741382178969\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] #progress_metric: host=algo-2, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650599.0858529, \"EndTime\": 1623650600.3361926, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 45270.0, \"count\": 1, \"min\": 45270, \"max\": 45270}, \"Total Batches Seen\": {\"sum\": 360.0, \"count\": 1, \"min\": 360, \"max\": 360}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3620.159636497167 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:20 INFO 140618438534976] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:21.530] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1193, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] # Finished training epoch 11 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] Loss (name: value) total: 6.84608440928989\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] Loss (name: value) kld: 0.016528837602689035\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] Loss (name: value) recons: 6.829555544588301\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] Loss (name: value) logppx: 6.84608440928989\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=6.84608440928989\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] patience losses:[6.857095082600911, 6.855464789602491, 6.854239218764835, 6.852634065681034, 6.851705491542816] min patience loss:6.851705491542816 current loss:6.84608440928989 absolute loss difference:0.005621082252925902\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] #progress_metric: host=algo-2, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650600.3366544, \"EndTime\": 1623650601.5349898, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 49797.0, \"count\": 1, \"min\": 49797, \"max\": 49797}, \"Total Batches Seen\": {\"sum\": 396.0, \"count\": 1, \"min\": 396, \"max\": 396}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3777.284153107649 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:21 INFO 140618438534976] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:20.517] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1229, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] # Finished training epoch 10 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] Loss (name: value) total: 6.867071542474958\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] Loss (name: value) kld: 0.016033476386736665\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] Loss (name: value) recons: 6.851038058598836\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] Loss (name: value) logppx: 6.867071542474958\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.867071542474958\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] patience losses:[6.878127449088627, 6.874089346991645, 6.875245557890998, 6.870396998193529, 6.871569997734493] min patience loss:6.870396998193529 current loss:6.867071542474958 absolute loss difference:0.00332545571857068\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] Timing: train: 1.23s, val: 0.01s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650599.2874217, \"EndTime\": 1623650600.5248017, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 45240.0, \"count\": 1, \"min\": 45240, \"max\": 45240}, \"Total Batches Seen\": {\"sum\": 360.0, \"count\": 1, \"min\": 360, \"max\": 360}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3654.9113155289847 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:20 INFO 140515461125952] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:21.734] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1206, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] # Finished training epoch 11 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] Loss (name: value) total: 6.864891423119439\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] Loss (name: value) kld: 0.018261595629155636\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] Loss (name: value) recons: 6.846629818280538\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] Loss (name: value) logppx: 6.864891423119439\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.864891423119439\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] patience losses:[6.874089346991645, 6.875245557890998, 6.870396998193529, 6.871569997734493, 6.867071542474958] min patience loss:6.867071542474958 current loss:6.864891423119439 absolute loss difference:0.0021801193555193166\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650600.525515, \"EndTime\": 1623650601.739449, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 49764.0, \"count\": 1, \"min\": 49764, \"max\": 49764}, \"Total Batches Seen\": {\"sum\": 396.0, \"count\": 1, \"min\": 396, \"max\": 396}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3726.326338455919 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:21 INFO 140515461125952] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:22.707] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1172, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] # Finished training epoch 12 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] Loss (name: value) total: 6.846516668796539\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] Loss (name: value) kld: 0.020568902681892116\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] Loss (name: value) recons: 6.825947774781121\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] Loss (name: value) logppx: 6.846516668796539\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=6.846516668796539\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] patience losses:[6.855464789602491, 6.854239218764835, 6.852634065681034, 6.851705491542816, 6.84608440928989] min patience loss:6.84608440928989 current loss:6.846516668796539 absolute loss difference:0.00043225950664904644\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] #progress_metric: host=algo-2, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650601.5352879, \"EndTime\": 1623650602.7088294, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54324.0, \"count\": 1, \"min\": 54324, \"max\": 54324}, \"Total Batches Seen\": {\"sum\": 432.0, \"count\": 1, \"min\": 432, \"max\": 432}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3857.0729624823575 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:22 INFO 140618438534976] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:22.929] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1189, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] # Finished training epoch 12 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] Loss (name: value) total: 6.856354859140184\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] Loss (name: value) kld: 0.01906356553081423\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] Loss (name: value) recons: 6.837291267183092\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] Loss (name: value) logppx: 6.856354859140184\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.856354859140184\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] patience losses:[6.875245557890998, 6.870396998193529, 6.871569997734493, 6.867071542474958, 6.864891423119439] min patience loss:6.864891423119439 current loss:6.856354859140184 absolute loss difference:0.00853656397925473\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650601.7397113, \"EndTime\": 1623650602.9355204, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54288.0, \"count\": 1, \"min\": 54288, \"max\": 54288}, \"Total Batches Seen\": {\"sum\": 432.0, \"count\": 1, \"min\": 432, \"max\": 432}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3782.411585103921 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:22 INFO 140515461125952] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:24.133] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1195, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] # Finished training epoch 13 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] Loss (name: value) total: 6.848663959238264\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] Loss (name: value) kld: 0.02649136803423365\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] Loss (name: value) recons: 6.82217264175415\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] Loss (name: value) logppx: 6.848663959238264\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.848663959238264\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] patience losses:[6.870396998193529, 6.871569997734493, 6.867071542474958, 6.864891423119439, 6.856354859140184] min patience loss:6.856354859140184 current loss:6.848663959238264 absolute loss difference:0.0076908999019202895\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650602.9360306, \"EndTime\": 1623650604.1381333, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 58812.0, \"count\": 1, \"min\": 58812, \"max\": 58812}, \"Total Batches Seen\": {\"sum\": 468.0, \"count\": 1, \"min\": 468, \"max\": 468}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3762.9235728181525 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:24 INFO 140515461125952] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:23.929] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] # Finished training epoch 13 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] Loss (name: value) total: 6.833452343940735\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] Loss (name: value) kld: 0.024148433533911075\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] Loss (name: value) recons: 6.809303992324406\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] Loss (name: value) logppx: 6.833452343940735\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=6.833452343940735\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] patience losses:[6.854239218764835, 6.852634065681034, 6.851705491542816, 6.84608440928989, 6.846516668796539] min patience loss:6.84608440928989 current loss:6.833452343940735 absolute loss difference:0.012632065349155397\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] #progress_metric: host=algo-2, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650602.709085, \"EndTime\": 1623650603.9342065, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 58851.0, \"count\": 1, \"min\": 58851, \"max\": 58851}, \"Total Batches Seen\": {\"sum\": 468.0, \"count\": 1, \"min\": 468, \"max\": 468}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3694.716612902661 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:23 INFO 140618438534976] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:25.451] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1312, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] # Finished training epoch 14 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] Loss (name: value) total: 6.8329996599091425\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] Loss (name: value) kld: 0.03645114502352145\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] Loss (name: value) recons: 6.79654848575592\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] Loss (name: value) logppx: 6.8329996599091425\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.8329996599091425\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] patience losses:[6.871569997734493, 6.867071542474958, 6.864891423119439, 6.856354859140184, 6.848663959238264] min patience loss:6.848663959238264 current loss:6.8329996599091425 absolute loss difference:0.01566429932912161\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] Timing: train: 1.31s, val: 0.00s, epoch: 1.32s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650604.138421, \"EndTime\": 1623650605.4571905, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 63336.0, \"count\": 1, \"min\": 63336, \"max\": 63336}, \"Total Batches Seen\": {\"sum\": 504.0, \"count\": 1, \"min\": 504, \"max\": 504}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3430.058961848046 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:25 INFO 140515461125952] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:25.273] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1339, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] # Finished training epoch 14 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] Loss (name: value) total: 6.819742268986172\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] Loss (name: value) kld: 0.033811634975588985\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] Loss (name: value) recons: 6.78593064016766\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] Loss (name: value) logppx: 6.819742268986172\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=6.819742268986172\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] patience losses:[6.852634065681034, 6.851705491542816, 6.84608440928989, 6.846516668796539, 6.833452343940735] min patience loss:6.833452343940735 current loss:6.819742268986172 absolute loss difference:0.013710074954563112\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] Timing: train: 1.34s, val: 0.01s, epoch: 1.35s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] #progress_metric: host=algo-2, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650603.9344974, \"EndTime\": 1623650605.281308, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 63378.0, \"count\": 1, \"min\": 63378, \"max\": 63378}, \"Total Batches Seen\": {\"sum\": 504.0, \"count\": 1, \"min\": 504, \"max\": 504}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3360.86988754805 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:25 INFO 140618438534976] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:26.703] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1244, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] # Finished training epoch 15 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] Loss (name: value) total: 6.809053785271114\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] Loss (name: value) kld: 0.04721171326107449\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] Loss (name: value) recons: 6.761842124991947\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] Loss (name: value) logppx: 6.809053785271114\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.809053785271114\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] patience losses:[6.867071542474958, 6.864891423119439, 6.856354859140184, 6.848663959238264, 6.8329996599091425] min patience loss:6.8329996599091425 current loss:6.809053785271114 absolute loss difference:0.02394587463802811\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650605.4575229, \"EndTime\": 1623650606.7075005, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 67860.0, \"count\": 1, \"min\": 67860, \"max\": 67860}, \"Total Batches Seen\": {\"sum\": 540.0, \"count\": 1, \"min\": 540, \"max\": 540}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3618.855569280802 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:26 INFO 140515461125952] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:26.515] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1233, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] # Finished training epoch 15 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] Loss (name: value) total: 6.797415932019551\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] Loss (name: value) kld: 0.0455046266110407\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] Loss (name: value) recons: 6.751911315653059\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] Loss (name: value) logppx: 6.797415932019551\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=6.797415932019551\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] patience losses:[6.851705491542816, 6.84608440928989, 6.846516668796539, 6.833452343940735, 6.819742268986172] min patience loss:6.819742268986172 current loss:6.797415932019551 absolute loss difference:0.022326336966620453\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] Timing: train: 1.23s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] #progress_metric: host=algo-2, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650605.2816195, \"EndTime\": 1623650606.5207481, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 67905.0, \"count\": 1, \"min\": 67905, \"max\": 67905}, \"Total Batches Seen\": {\"sum\": 540.0, \"count\": 1, \"min\": 540, \"max\": 540}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3652.8867891823256 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:26 INFO 140618438534976] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:27.749] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1228, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] # Finished training epoch 16 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] Loss (name: value) total: 6.786385926935408\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] Loss (name: value) kld: 0.05184902431857255\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] Loss (name: value) recons: 6.734536919328901\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] Loss (name: value) logppx: 6.786385926935408\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=6.786385926935408\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] patience losses:[6.84608440928989, 6.846516668796539, 6.833452343940735, 6.819742268986172, 6.797415932019551] min patience loss:6.797415932019551 current loss:6.786385926935408 absolute loss difference:0.011030005084143646\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] Timing: train: 1.23s, val: 0.01s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] #progress_metric: host=algo-2, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650606.5210786, \"EndTime\": 1623650607.7565727, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 72432.0, \"count\": 1, \"min\": 72432, \"max\": 72432}, \"Total Batches Seen\": {\"sum\": 576.0, \"count\": 1, \"min\": 576, \"max\": 576}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3663.179400274339 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:27 INFO 140618438534976] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:27.919] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] # Finished training epoch 16 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] Loss (name: value) total: 6.798976235919529\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] Loss (name: value) kld: 0.05546363132695357\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] Loss (name: value) recons: 6.743512564235264\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] Loss (name: value) logppx: 6.798976235919529\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.798976235919529\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] patience losses:[6.864891423119439, 6.856354859140184, 6.848663959238264, 6.8329996599091425, 6.809053785271114] min patience loss:6.809053785271114 current loss:6.798976235919529 absolute loss difference:0.010077549351585446\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650606.707788, \"EndTime\": 1623650607.9245365, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 72384.0, \"count\": 1, \"min\": 72384, \"max\": 72384}, \"Total Batches Seen\": {\"sum\": 576.0, \"count\": 1, \"min\": 576, \"max\": 576}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3716.9794270670095 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:27 INFO 140515461125952] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:29.152] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1223, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] # Finished training epoch 17 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] Loss (name: value) total: 6.788528753651513\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] Loss (name: value) kld: 0.05931717002143463\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] Loss (name: value) recons: 6.7292115688323975\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] Loss (name: value) logppx: 6.788528753651513\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.788528753651513\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] patience losses:[6.856354859140184, 6.848663959238264, 6.8329996599091425, 6.809053785271114, 6.798976235919529] min patience loss:6.798976235919529 current loss:6.788528753651513 absolute loss difference:0.01044748226801584\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650607.925273, \"EndTime\": 1623650609.1592486, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 76908.0, \"count\": 1, \"min\": 76908, \"max\": 76908}, \"Total Batches Seen\": {\"sum\": 612.0, \"count\": 1, \"min\": 612, \"max\": 612}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3665.324866237594 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:29 INFO 140515461125952] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:28.951] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1194, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] # Finished training epoch 17 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] Loss (name: value) total: 6.770702256096734\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] Loss (name: value) kld: 0.05787597281030483\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] Loss (name: value) recons: 6.712826338079241\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] Loss (name: value) logppx: 6.770702256096734\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=6.770702256096734\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] patience losses:[6.846516668796539, 6.833452343940735, 6.819742268986172, 6.797415932019551, 6.786385926935408] min patience loss:6.786385926935408 current loss:6.770702256096734 absolute loss difference:0.015683670838673613\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] #progress_metric: host=algo-2, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650607.7570565, \"EndTime\": 1623650608.957132, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 76959.0, \"count\": 1, \"min\": 76959, \"max\": 76959}, \"Total Batches Seen\": {\"sum\": 612.0, \"count\": 1, \"min\": 612, \"max\": 612}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3771.729492293998 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:28 INFO 140618438534976] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:30.195] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1237, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] # Finished training epoch 18 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] Loss (name: value) total: 6.763536499606238\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] Loss (name: value) kld: 0.060336535434342094\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] Loss (name: value) recons: 6.703200022379558\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] Loss (name: value) logppx: 6.763536499606238\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=6.763536499606238\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] patience losses:[6.833452343940735, 6.819742268986172, 6.797415932019551, 6.786385926935408, 6.770702256096734] min patience loss:6.770702256096734 current loss:6.763536499606238 absolute loss difference:0.007165756490495667\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] Timing: train: 1.24s, val: 0.01s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] #progress_metric: host=algo-2, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650608.9574678, \"EndTime\": 1623650610.20417, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 81486.0, \"count\": 1, \"min\": 81486, \"max\": 81486}, \"Total Batches Seen\": {\"sum\": 648.0, \"count\": 1, \"min\": 648, \"max\": 648}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3630.6453127049544 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:30 INFO 140618438534976] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:31.419] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1214, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] # Finished training epoch 19 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] Loss (name: value) total: 6.756346702575684\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] Loss (name: value) kld: 0.06403998399360313\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] Loss (name: value) recons: 6.692306750350529\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] Loss (name: value) logppx: 6.756346702575684\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=6.756346702575684\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] patience losses:[6.819742268986172, 6.797415932019551, 6.786385926935408, 6.770702256096734, 6.763536499606238] min patience loss:6.763536499606238 current loss:6.756346702575684 absolute loss difference:0.007189797030554779\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] #progress_metric: host=algo-2, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650610.2044892, \"EndTime\": 1623650611.425931, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 86013.0, \"count\": 1, \"min\": 86013, \"max\": 86013}, \"Total Batches Seen\": {\"sum\": 684.0, \"count\": 1, \"min\": 684, \"max\": 684}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3705.770918280119 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:31 INFO 140618438534976] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:30.447] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1286, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] # Finished training epoch 18 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] Loss (name: value) total: 6.774140881167518\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] Loss (name: value) kld: 0.060446623247116804\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] Loss (name: value) recons: 6.7136942810482445\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] Loss (name: value) logppx: 6.774140881167518\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.774140881167518\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] patience losses:[6.848663959238264, 6.8329996599091425, 6.809053785271114, 6.798976235919529, 6.788528753651513] min patience loss:6.788528753651513 current loss:6.774140881167518 absolute loss difference:0.014387872483995423\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] Timing: train: 1.29s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650609.1596658, \"EndTime\": 1623650610.4517326, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 81432.0, \"count\": 1, \"min\": 81432, \"max\": 81432}, \"Total Batches Seen\": {\"sum\": 648.0, \"count\": 1, \"min\": 648, \"max\": 648}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3500.9450775963132 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:30 INFO 140515461125952] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:31.699] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1247, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] # Finished training epoch 19 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] Loss (name: value) total: 6.765425046284993\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] Loss (name: value) kld: 0.06628148697523607\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] Loss (name: value) recons: 6.699143568674724\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] Loss (name: value) logppx: 6.765425046284993\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.765425046284993\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] patience losses:[6.8329996599091425, 6.809053785271114, 6.798976235919529, 6.788528753651513, 6.774140881167518] min patience loss:6.774140881167518 current loss:6.765425046284993 absolute loss difference:0.008715834882524476\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650610.452071, \"EndTime\": 1623650611.7062688, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 85956.0, \"count\": 1, \"min\": 85956, \"max\": 85956}, \"Total Batches Seen\": {\"sum\": 684.0, \"count\": 1, \"min\": 684, \"max\": 684}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3605.9553934076594 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:31 INFO 140515461125952] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:32.627] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1201, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] # Finished training epoch 20 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] Loss (name: value) total: 6.7467219895786705\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] Loss (name: value) kld: 0.06600676145818499\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] Loss (name: value) recons: 6.680715229776171\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] Loss (name: value) logppx: 6.7467219895786705\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=6.7467219895786705\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] patience losses:[6.797415932019551, 6.786385926935408, 6.770702256096734, 6.763536499606238, 6.756346702575684] min patience loss:6.756346702575684 current loss:6.7467219895786705 absolute loss difference:0.009624712997013063\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] #progress_metric: host=algo-2, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650611.4262533, \"EndTime\": 1623650612.6334045, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 90540.0, \"count\": 1, \"min\": 90540, \"max\": 90540}, \"Total Batches Seen\": {\"sum\": 720.0, \"count\": 1, \"min\": 720, \"max\": 720}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3749.6466038329404 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:32 INFO 140618438534976] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:32.941] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1233, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] # Finished training epoch 20 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] Loss (name: value) total: 6.761027706993951\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] Loss (name: value) kld: 0.06786401761281821\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] Loss (name: value) recons: 6.693163706196679\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] Loss (name: value) logppx: 6.761027706993951\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.761027706993951\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] patience losses:[6.809053785271114, 6.798976235919529, 6.788528753651513, 6.774140881167518, 6.765425046284993] min patience loss:6.765425046284993 current loss:6.761027706993951 absolute loss difference:0.004397339291042357\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650611.7069356, \"EndTime\": 1623650612.946663, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 90480.0, \"count\": 1, \"min\": 90480, \"max\": 90480}, \"Total Batches Seen\": {\"sum\": 720.0, \"count\": 1, \"min\": 720, \"max\": 720}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3648.636815002016 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:32 INFO 140515461125952] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:33.865] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1230, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] # Finished training epoch 21 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] Loss (name: value) total: 6.741054654121399\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] Loss (name: value) kld: 0.07118882952878873\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] Loss (name: value) recons: 6.66986577378379\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] Loss (name: value) logppx: 6.741054654121399\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=6.741054654121399\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] patience losses:[6.786385926935408, 6.770702256096734, 6.763536499606238, 6.756346702575684, 6.7467219895786705] min patience loss:6.7467219895786705 current loss:6.741054654121399 absolute loss difference:0.005667335457271605\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] Timing: train: 1.23s, val: 0.01s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] #progress_metric: host=algo-2, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650612.6338189, \"EndTime\": 1623650613.8718321, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 95067.0, \"count\": 1, \"min\": 95067, \"max\": 95067}, \"Total Batches Seen\": {\"sum\": 756.0, \"count\": 1, \"min\": 756, \"max\": 756}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3656.167367600361 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:33 INFO 140618438534976] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:34.185] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1237, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] # Finished training epoch 21 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] Loss (name: value) total: 6.757524179087745\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] Loss (name: value) kld: 0.06999651425414616\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] Loss (name: value) recons: 6.687527649932438\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] Loss (name: value) logppx: 6.757524179087745\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.757524179087745\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] patience losses:[6.798976235919529, 6.788528753651513, 6.774140881167518, 6.765425046284993, 6.761027706993951] min patience loss:6.761027706993951 current loss:6.757524179087745 absolute loss difference:0.0035035279062061164\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650612.9471755, \"EndTime\": 1623650614.1911108, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 95004.0, \"count\": 1, \"min\": 95004, \"max\": 95004}, \"Total Batches Seen\": {\"sum\": 756.0, \"count\": 1, \"min\": 756, \"max\": 756}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3636.302038510435 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:34 INFO 140515461125952] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:35.480] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1287, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] # Finished training epoch 22 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] Loss (name: value) total: 6.749085916413201\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] Loss (name: value) kld: 0.07302776646489899\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] Loss (name: value) recons: 6.676058133443196\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] Loss (name: value) logppx: 6.749085916413201\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.749085916413201\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] patience losses:[6.788528753651513, 6.774140881167518, 6.765425046284993, 6.761027706993951, 6.757524179087745] min patience loss:6.757524179087745 current loss:6.749085916413201 absolute loss difference:0.008438262674543395\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] Timing: train: 1.29s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650614.1916728, \"EndTime\": 1623650615.4858487, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 99528.0, \"count\": 1, \"min\": 99528, \"max\": 99528}, \"Total Batches Seen\": {\"sum\": 792.0, \"count\": 1, \"min\": 792, \"max\": 792}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3494.859994205633 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:35 INFO 140515461125952] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:35.128] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1255, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] # Finished training epoch 22 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] Loss (name: value) total: 6.736069666014777\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] Loss (name: value) kld: 0.07210338695181741\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] Loss (name: value) recons: 6.663966271612379\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] Loss (name: value) logppx: 6.736069666014777\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=6.736069666014777\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] patience losses:[6.770702256096734, 6.763536499606238, 6.756346702575684, 6.7467219895786705, 6.741054654121399] min patience loss:6.741054654121399 current loss:6.736069666014777 absolute loss difference:0.004984988106621735\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] #progress_metric: host=algo-2, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650613.87216, \"EndTime\": 1623650615.1328013, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 99594.0, \"count\": 1, \"min\": 99594, \"max\": 99594}, \"Total Batches Seen\": {\"sum\": 792.0, \"count\": 1, \"min\": 792, \"max\": 792}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3590.5594068308624 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:35 INFO 140618438534976] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:36.738] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1251, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] # Finished training epoch 23 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] Loss (name: value) total: 6.742422779401143\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] Loss (name: value) kld: 0.07756873509950107\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] Loss (name: value) recons: 6.664854102664524\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] Loss (name: value) logppx: 6.742422779401143\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.742422779401143\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] patience losses:[6.774140881167518, 6.765425046284993, 6.761027706993951, 6.757524179087745, 6.749085916413201] min patience loss:6.749085916413201 current loss:6.742422779401143 absolute loss difference:0.006663137012058229\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650615.486428, \"EndTime\": 1623650616.7423217, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 104052.0, \"count\": 1, \"min\": 104052, \"max\": 104052}, \"Total Batches Seen\": {\"sum\": 828.0, \"count\": 1, \"min\": 828, \"max\": 828}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3601.6010055235847 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:36 INFO 140515461125952] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:36.360] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1226, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] # Finished training epoch 23 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] Loss (name: value) total: 6.727649516529507\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] Loss (name: value) kld: 0.07520789187401533\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] Loss (name: value) recons: 6.652441581090291\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] Loss (name: value) logppx: 6.727649516529507\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=6.727649516529507\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] patience losses:[6.763536499606238, 6.756346702575684, 6.7467219895786705, 6.741054654121399, 6.736069666014777] min patience loss:6.736069666014777 current loss:6.727649516529507 absolute loss difference:0.008420149485270478\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] #progress_metric: host=algo-2, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650615.1331213, \"EndTime\": 1623650616.365944, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 104121.0, \"count\": 1, \"min\": 104121, \"max\": 104121}, \"Total Batches Seen\": {\"sum\": 828.0, \"count\": 1, \"min\": 828, \"max\": 828}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3671.5128491058385 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:36 INFO 140618438534976] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:37.626] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1259, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] # Finished training epoch 24 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] Loss (name: value) total: 6.720312754313151\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] Loss (name: value) kld: 0.07944497476435369\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] Loss (name: value) recons: 6.640867763095432\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] Loss (name: value) logppx: 6.720312754313151\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] #quality_metric: host=algo-2, epoch=24, train total_loss <loss>=6.720312754313151\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] patience losses:[6.756346702575684, 6.7467219895786705, 6.741054654121399, 6.736069666014777, 6.727649516529507] min patience loss:6.727649516529507 current loss:6.720312754313151 absolute loss difference:0.007336762216355375\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] #progress_metric: host=algo-2, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650616.3662987, \"EndTime\": 1623650617.6314228, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 108648.0, \"count\": 1, \"min\": 108648, \"max\": 108648}, \"Total Batches Seen\": {\"sum\": 864.0, \"count\": 1, \"min\": 864, \"max\": 864}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3577.834500876482 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:37 INFO 140618438534976] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:37.955] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1212, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] # Finished training epoch 24 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] Loss (name: value) total: 6.737858838505215\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] Loss (name: value) kld: 0.08178858945353164\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] Loss (name: value) recons: 6.656070239014095\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] Loss (name: value) logppx: 6.737858838505215\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.737858838505215\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] patience losses:[6.765425046284993, 6.761027706993951, 6.757524179087745, 6.749085916413201, 6.742422779401143] min patience loss:6.742422779401143 current loss:6.737858838505215 absolute loss difference:0.004563940895928376\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650616.7428255, \"EndTime\": 1623650617.9599617, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 108576.0, \"count\": 1, \"min\": 108576, \"max\": 108576}, \"Total Batches Seen\": {\"sum\": 864.0, \"count\": 1, \"min\": 864, \"max\": 864}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3716.3256980511405 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:37 INFO 140515461125952] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:38.848] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1215, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] # Finished training epoch 25 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] Loss (name: value) total: 6.712385475635529\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] Loss (name: value) kld: 0.08204905254145463\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] Loss (name: value) recons: 6.630336423714955\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] Loss (name: value) logppx: 6.712385475635529\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] #quality_metric: host=algo-2, epoch=25, train total_loss <loss>=6.712385475635529\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] patience losses:[6.7467219895786705, 6.741054654121399, 6.736069666014777, 6.727649516529507, 6.720312754313151] min patience loss:6.720312754313151 current loss:6.712385475635529 absolute loss difference:0.007927278677622773\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] #progress_metric: host=algo-2, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650617.6317515, \"EndTime\": 1623650618.8530777, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 113175.0, \"count\": 1, \"min\": 113175, \"max\": 113175}, \"Total Batches Seen\": {\"sum\": 900.0, \"count\": 1, \"min\": 900, \"max\": 900}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3706.1333005419365 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:38 INFO 140618438534976] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:39.226] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1265, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] # Finished training epoch 25 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] Loss (name: value) total: 6.727770566940308\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] Loss (name: value) kld: 0.08519747439357969\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] Loss (name: value) recons: 6.642573104964362\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] Loss (name: value) logppx: 6.727770566940308\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.727770566940308\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] patience losses:[6.761027706993951, 6.757524179087745, 6.749085916413201, 6.742422779401143, 6.737858838505215] min patience loss:6.737858838505215 current loss:6.727770566940308 absolute loss difference:0.010088271564907103\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650617.9602916, \"EndTime\": 1623650619.2317085, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 113100.0, \"count\": 1, \"min\": 113100, \"max\": 113100}, \"Total Batches Seen\": {\"sum\": 900.0, \"count\": 1, \"min\": 900, \"max\": 900}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3557.2523202245443 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:39 INFO 140515461125952] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:40.117] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1263, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] # Finished training epoch 26 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] Loss (name: value) total: 6.7030270960595875\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] Loss (name: value) kld: 0.0882182474144631\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] Loss (name: value) recons: 6.614808844195472\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] Loss (name: value) logppx: 6.7030270960595875\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] #quality_metric: host=algo-2, epoch=26, train total_loss <loss>=6.7030270960595875\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] patience losses:[6.741054654121399, 6.736069666014777, 6.727649516529507, 6.720312754313151, 6.712385475635529] min patience loss:6.712385475635529 current loss:6.7030270960595875 absolute loss difference:0.0093583795759411\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] #progress_metric: host=algo-2, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650618.8533945, \"EndTime\": 1623650620.121878, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 117702.0, \"count\": 1, \"min\": 117702, \"max\": 117702}, \"Total Batches Seen\": {\"sum\": 936.0, \"count\": 1, \"min\": 936, \"max\": 936}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3568.4330107432916 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:40 INFO 140618438534976] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:41.388] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1266, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] # Finished training epoch 27 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] Loss (name: value) total: 6.6956181857320995\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] Loss (name: value) kld: 0.092510343529284\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] Loss (name: value) recons: 6.603107863002354\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] Loss (name: value) logppx: 6.6956181857320995\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] #quality_metric: host=algo-2, epoch=27, train total_loss <loss>=6.6956181857320995\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] patience losses:[6.736069666014777, 6.727649516529507, 6.720312754313151, 6.712385475635529, 6.7030270960595875] min patience loss:6.7030270960595875 current loss:6.6956181857320995 absolute loss difference:0.0074089103274879164\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] #progress_metric: host=algo-2, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650620.122155, \"EndTime\": 1623650621.3956814, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 122229.0, \"count\": 1, \"min\": 122229, \"max\": 122229}, \"Total Batches Seen\": {\"sum\": 972.0, \"count\": 1, \"min\": 972, \"max\": 972}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3554.305285236548 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:41 INFO 140618438534976] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:40.517] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1284, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] # Finished training epoch 26 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] Loss (name: value) total: 6.7148445513513355\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] Loss (name: value) kld: 0.0883369552385476\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] Loss (name: value) recons: 6.626507573657566\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] Loss (name: value) logppx: 6.7148445513513355\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.7148445513513355\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] patience losses:[6.757524179087745, 6.749085916413201, 6.742422779401143, 6.737858838505215, 6.727770566940308] min patience loss:6.727770566940308 current loss:6.7148445513513355 absolute loss difference:0.012926015588972106\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] Timing: train: 1.29s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650619.2323198, \"EndTime\": 1623650620.5224156, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 117624.0, \"count\": 1, \"min\": 117624, \"max\": 117624}, \"Total Batches Seen\": {\"sum\": 936.0, \"count\": 1, \"min\": 936, \"max\": 936}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3506.3074247669624 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:40 INFO 140515461125952] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:41.917] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1394, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] # Finished training epoch 27 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] Loss (name: value) total: 6.704468852943844\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] Loss (name: value) kld: 0.09241462116026217\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] Loss (name: value) recons: 6.612054189046224\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] Loss (name: value) logppx: 6.704468852943844\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.704468852943844\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] patience losses:[6.749085916413201, 6.742422779401143, 6.737858838505215, 6.727770566940308, 6.7148445513513355] min patience loss:6.7148445513513355 current loss:6.704468852943844 absolute loss difference:0.01037569840749164\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] Timing: train: 1.40s, val: 0.00s, epoch: 1.40s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650620.5227125, \"EndTime\": 1623650621.9231768, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 122148.0, \"count\": 1, \"min\": 122148, \"max\": 122148}, \"Total Batches Seen\": {\"sum\": 972.0, \"count\": 1, \"min\": 972, \"max\": 972}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3229.9772882329953 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:41 INFO 140515461125952] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:42.588] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] # Finished training epoch 28 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] Loss (name: value) total: 6.690275066428715\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] Loss (name: value) kld: 0.09498870248595874\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] Loss (name: value) recons: 6.595286375946468\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] Loss (name: value) logppx: 6.690275066428715\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] #quality_metric: host=algo-2, epoch=28, train total_loss <loss>=6.690275066428715\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] patience losses:[6.727649516529507, 6.720312754313151, 6.712385475635529, 6.7030270960595875, 6.6956181857320995] min patience loss:6.6956181857320995 current loss:6.690275066428715 absolute loss difference:0.0053431193033848245\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] #progress_metric: host=algo-2, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650621.3959694, \"EndTime\": 1623650622.593993, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 126756.0, \"count\": 1, \"min\": 126756, \"max\": 126756}, \"Total Batches Seen\": {\"sum\": 1008.0, \"count\": 1, \"min\": 1008, \"max\": 1008}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3778.243973898784 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:42 INFO 140618438534976] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:43.112] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1188, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] # Finished training epoch 28 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] Loss (name: value) total: 6.704635692967309\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] Loss (name: value) kld: 0.0966111005594333\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] Loss (name: value) recons: 6.608024617036183\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] Loss (name: value) logppx: 6.704635692967309\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.704635692967309\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] patience losses:[6.742422779401143, 6.737858838505215, 6.727770566940308, 6.7148445513513355, 6.704468852943844] min patience loss:6.704468852943844 current loss:6.704635692967309 absolute loss difference:0.00016684002346512017\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650621.9235, \"EndTime\": 1623650623.1134405, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 126672.0, \"count\": 1, \"min\": 126672, \"max\": 126672}, \"Total Batches Seen\": {\"sum\": 1008.0, \"count\": 1, \"min\": 1008, \"max\": 1008}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3801.3590905120636 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:43 INFO 140515461125952] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:43.838] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1243, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] # Finished training epoch 29 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] Loss (name: value) total: 6.686985075473785\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] Loss (name: value) kld: 0.09913314133882523\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] Loss (name: value) recons: 6.587851934962803\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] Loss (name: value) logppx: 6.686985075473785\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] #quality_metric: host=algo-2, epoch=29, train total_loss <loss>=6.686985075473785\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] patience losses:[6.720312754313151, 6.712385475635529, 6.7030270960595875, 6.6956181857320995, 6.690275066428715] min patience loss:6.690275066428715 current loss:6.686985075473785 absolute loss difference:0.0032899909549293227\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] #progress_metric: host=algo-2, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650622.5943103, \"EndTime\": 1623650623.8426225, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 131283.0, \"count\": 1, \"min\": 131283, \"max\": 131283}, \"Total Batches Seen\": {\"sum\": 1044.0, \"count\": 1, \"min\": 1044, \"max\": 1044}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3626.081006401152 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:43 INFO 140618438534976] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:44.342] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1228, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] # Finished training epoch 29 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] Loss (name: value) total: 6.695860392517513\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] Loss (name: value) kld: 0.09945213639487822\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] Loss (name: value) recons: 6.596408314175076\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] Loss (name: value) logppx: 6.695860392517513\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.695860392517513\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] patience losses:[6.737858838505215, 6.727770566940308, 6.7148445513513355, 6.704468852943844, 6.704635692967309] min patience loss:6.704468852943844 current loss:6.695860392517513 absolute loss difference:0.008608460426330566\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] Timing: train: 1.23s, val: 0.01s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650623.1137114, \"EndTime\": 1623650624.3483696, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 131196.0, \"count\": 1, \"min\": 131196, \"max\": 131196}, \"Total Batches Seen\": {\"sum\": 1044.0, \"count\": 1, \"min\": 1044, \"max\": 1044}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3663.717669265408 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:44 INFO 140515461125952] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:45.666] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1317, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] # Finished training epoch 30 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] Loss (name: value) total: 6.692303862836626\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] Loss (name: value) kld: 0.1011545221424765\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] Loss (name: value) recons: 6.591149402989282\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] Loss (name: value) logppx: 6.692303862836626\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.692303862836626\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] patience losses:[6.727770566940308, 6.7148445513513355, 6.704468852943844, 6.704635692967309, 6.695860392517513] min patience loss:6.695860392517513 current loss:6.692303862836626 absolute loss difference:0.003556529680887266\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] Timing: train: 1.32s, val: 0.00s, epoch: 1.32s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650624.348683, \"EndTime\": 1623650625.6715324, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 135720.0, \"count\": 1, \"min\": 135720, \"max\": 135720}, \"Total Batches Seen\": {\"sum\": 1080.0, \"count\": 1, \"min\": 1080, \"max\": 1080}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3419.466127338005 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:45 INFO 140515461125952] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:45.125] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1282, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] # Finished training epoch 30 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] Loss (name: value) total: 6.6813921795950995\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] Loss (name: value) kld: 0.10017347863564889\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] Loss (name: value) recons: 6.5812186863687305\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] Loss (name: value) logppx: 6.6813921795950995\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] #quality_metric: host=algo-2, epoch=30, train total_loss <loss>=6.6813921795950995\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] patience losses:[6.712385475635529, 6.7030270960595875, 6.6956181857320995, 6.690275066428715, 6.686985075473785] min patience loss:6.686985075473785 current loss:6.6813921795950995 absolute loss difference:0.005592895878685944\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] Timing: train: 1.28s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] #progress_metric: host=algo-2, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650623.8429172, \"EndTime\": 1623650625.1319532, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 135810.0, \"count\": 1, \"min\": 135810, \"max\": 135810}, \"Total Batches Seen\": {\"sum\": 1080.0, \"count\": 1, \"min\": 1080, \"max\": 1080}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3511.448632924291 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:45 INFO 140618438534976] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:46.392] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 1259, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] # Finished training epoch 31 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] Loss (name: value) total: 6.672385599878099\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] Loss (name: value) kld: 0.10187322553247213\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] Loss (name: value) recons: 6.570512354373932\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] Loss (name: value) logppx: 6.672385599878099\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] #quality_metric: host=algo-2, epoch=31, train total_loss <loss>=6.672385599878099\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] patience losses:[6.7030270960595875, 6.6956181857320995, 6.690275066428715, 6.686985075473785, 6.6813921795950995] min patience loss:6.6813921795950995 current loss:6.672385599878099 absolute loss difference:0.00900657971700003\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] #progress_metric: host=algo-2, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650625.1322865, \"EndTime\": 1623650626.39818, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 140337.0, \"count\": 1, \"min\": 140337, \"max\": 140337}, \"Total Batches Seen\": {\"sum\": 1116.0, \"count\": 1, \"min\": 1116, \"max\": 1116}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3575.7430971859944 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:46 INFO 140618438534976] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:46.950] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 1277, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] # Finished training epoch 31 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] Loss (name: value) total: 6.686646580696106\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] Loss (name: value) kld: 0.10345390583905908\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] Loss (name: value) recons: 6.583192633257972\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] Loss (name: value) logppx: 6.686646580696106\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.686646580696106\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] patience losses:[6.7148445513513355, 6.704468852943844, 6.704635692967309, 6.695860392517513, 6.692303862836626] min patience loss:6.692303862836626 current loss:6.686646580696106 absolute loss difference:0.005657282140520081\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650625.6718588, \"EndTime\": 1623650626.9562888, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 140244.0, \"count\": 1, \"min\": 140244, \"max\": 140244}, \"Total Batches Seen\": {\"sum\": 1116.0, \"count\": 1, \"min\": 1116, \"max\": 1116}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3521.2168592801586 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:46 INFO 140515461125952] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:47.646] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1247, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] # Finished training epoch 32 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] Loss (name: value) total: 6.6631717218293085\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] Loss (name: value) kld: 0.10752565962158972\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] Loss (name: value) recons: 6.5556460751427545\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] Loss (name: value) logppx: 6.6631717218293085\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] #quality_metric: host=algo-2, epoch=32, train total_loss <loss>=6.6631717218293085\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] patience losses:[6.6956181857320995, 6.690275066428715, 6.686985075473785, 6.6813921795950995, 6.672385599878099] min patience loss:6.672385599878099 current loss:6.6631717218293085 absolute loss difference:0.009213878048790924\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] #progress_metric: host=algo-2, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650626.398607, \"EndTime\": 1623650627.6517272, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 144864.0, \"count\": 1, \"min\": 144864, \"max\": 144864}, \"Total Batches Seen\": {\"sum\": 1152.0, \"count\": 1, \"min\": 1152, \"max\": 1152}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3612.221650484772 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:47 INFO 140618438534976] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:48.265] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1300, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] # Finished training epoch 32 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] Loss (name: value) total: 6.679269446267022\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] Loss (name: value) kld: 0.10537489275965425\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] Loss (name: value) recons: 6.573894527223375\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] Loss (name: value) logppx: 6.679269446267022\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.679269446267022\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] patience losses:[6.704468852943844, 6.704635692967309, 6.695860392517513, 6.692303862836626, 6.686646580696106] min patience loss:6.686646580696106 current loss:6.679269446267022 absolute loss difference:0.0073771344290838314\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] Timing: train: 1.31s, val: 0.00s, epoch: 1.31s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650626.9569783, \"EndTime\": 1623650628.270155, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 144768.0, \"count\": 1, \"min\": 144768, \"max\": 144768}, \"Total Batches Seen\": {\"sum\": 1152.0, \"count\": 1, \"min\": 1152, \"max\": 1152}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3444.560153720618 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:48 INFO 140515461125952] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:48.880] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1228, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] # Finished training epoch 33 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] Loss (name: value) total: 6.658994363413917\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] Loss (name: value) kld: 0.10809994261297914\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] Loss (name: value) recons: 6.550894419352214\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] Loss (name: value) logppx: 6.658994363413917\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] #quality_metric: host=algo-2, epoch=33, train total_loss <loss>=6.658994363413917\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] patience losses:[6.690275066428715, 6.686985075473785, 6.6813921795950995, 6.672385599878099, 6.6631717218293085] min patience loss:6.6631717218293085 current loss:6.658994363413917 absolute loss difference:0.0041773584153919074\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] Timing: train: 1.23s, val: 0.01s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] #progress_metric: host=algo-2, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650627.652014, \"EndTime\": 1623650628.8876274, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 149391.0, \"count\": 1, \"min\": 149391, \"max\": 149391}, \"Total Batches Seen\": {\"sum\": 1188.0, \"count\": 1, \"min\": 1188, \"max\": 1188}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3663.161025705927 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:48 INFO 140618438534976] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:49.501] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1231, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] # Finished training epoch 33 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] Loss (name: value) total: 6.680750323666467\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] Loss (name: value) kld: 0.10841029023544656\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] Loss (name: value) recons: 6.572340044710371\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] Loss (name: value) logppx: 6.680750323666467\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.680750323666467\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] patience losses:[6.704635692967309, 6.695860392517513, 6.692303862836626, 6.686646580696106, 6.679269446267022] min patience loss:6.679269446267022 current loss:6.680750323666467 absolute loss difference:0.00148087739944458\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650628.2704332, \"EndTime\": 1623650629.503665, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 149292.0, \"count\": 1, \"min\": 149292, \"max\": 149292}, \"Total Batches Seen\": {\"sum\": 1188.0, \"count\": 1, \"min\": 1188, \"max\": 1188}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3667.9995731789354 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:49 INFO 140515461125952] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:50.119] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1231, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] # Finished training epoch 34 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] Loss (name: value) total: 6.657234172026317\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] Loss (name: value) kld: 0.1105937517972456\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] Loss (name: value) recons: 6.546640396118164\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] Loss (name: value) logppx: 6.657234172026317\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] #quality_metric: host=algo-2, epoch=34, train total_loss <loss>=6.657234172026317\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] patience losses:[6.686985075473785, 6.6813921795950995, 6.672385599878099, 6.6631717218293085, 6.658994363413917] min patience loss:6.658994363413917 current loss:6.657234172026317 absolute loss difference:0.0017601913875999742\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] Timing: train: 1.23s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] #progress_metric: host=algo-2, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650628.8880277, \"EndTime\": 1623650630.124268, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 153918.0, \"count\": 1, \"min\": 153918, \"max\": 153918}, \"Total Batches Seen\": {\"sum\": 1224.0, \"count\": 1, \"min\": 1224, \"max\": 1224}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3661.1762448309996 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:50 INFO 140618438534976] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:51.403] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1278, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] # Finished training epoch 35 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] Loss (name: value) total: 6.656618250740899\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] Loss (name: value) kld: 0.11280429508123133\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] Loss (name: value) recons: 6.543813957108392\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] Loss (name: value) logppx: 6.656618250740899\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] #quality_metric: host=algo-2, epoch=35, train total_loss <loss>=6.656618250740899\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] patience losses:[6.6813921795950995, 6.672385599878099, 6.6631717218293085, 6.658994363413917, 6.657234172026317] min patience loss:6.657234172026317 current loss:6.656618250740899 absolute loss difference:0.0006159212854175422\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] #progress_metric: host=algo-2, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650630.1248088, \"EndTime\": 1623650631.409242, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 158445.0, \"count\": 1, \"min\": 158445, \"max\": 158445}, \"Total Batches Seen\": {\"sum\": 1260.0, \"count\": 1, \"min\": 1260, \"max\": 1260}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3523.8121429232906 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:51 INFO 140618438534976] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:50.821] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1317, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] # Finished training epoch 34 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] Loss (name: value) total: 6.673586931493547\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] Loss (name: value) kld: 0.11163131406323777\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] Loss (name: value) recons: 6.561955663892958\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] Loss (name: value) logppx: 6.673586931493547\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=6.673586931493547\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] patience losses:[6.695860392517513, 6.692303862836626, 6.686646580696106, 6.679269446267022, 6.680750323666467] min patience loss:6.679269446267022 current loss:6.673586931493547 absolute loss difference:0.005682514773474701\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] Timing: train: 1.32s, val: 0.00s, epoch: 1.32s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650629.5039768, \"EndTime\": 1623650630.827506, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 153816.0, \"count\": 1, \"min\": 153816, \"max\": 153816}, \"Total Batches Seen\": {\"sum\": 1224.0, \"count\": 1, \"min\": 1224, \"max\": 1224}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3417.6160111668955 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:50 INFO 140515461125952] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:52.099] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1271, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] # Finished training epoch 35 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] Loss (name: value) total: 6.668287144766913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] Loss (name: value) kld: 0.11246862603972356\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] Loss (name: value) recons: 6.555818518002828\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] Loss (name: value) logppx: 6.668287144766913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=6.668287144766913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] patience losses:[6.692303862836626, 6.686646580696106, 6.679269446267022, 6.680750323666467, 6.673586931493547] min patience loss:6.673586931493547 current loss:6.668287144766913 absolute loss difference:0.005299786726634004\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] Timing: train: 1.27s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650630.827802, \"EndTime\": 1623650632.1042688, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 158340.0, \"count\": 1, \"min\": 158340, \"max\": 158340}, \"Total Batches Seen\": {\"sum\": 1260.0, \"count\": 1, \"min\": 1260, \"max\": 1260}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3543.739533799656 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:52 INFO 140515461125952] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:52.647] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1237, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] # Finished training epoch 36 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] Loss (name: value) total: 6.650513880782658\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] Loss (name: value) kld: 0.115335736837652\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] Loss (name: value) recons: 6.535178177886539\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] Loss (name: value) logppx: 6.650513880782658\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] #quality_metric: host=algo-2, epoch=36, train total_loss <loss>=6.650513880782658\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] patience losses:[6.672385599878099, 6.6631717218293085, 6.658994363413917, 6.657234172026317, 6.656618250740899] min patience loss:6.656618250740899 current loss:6.650513880782658 absolute loss difference:0.0061043699582414845\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] #progress_metric: host=algo-2, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650631.4095573, \"EndTime\": 1623650632.6520112, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 162972.0, \"count\": 1, \"min\": 162972, \"max\": 162972}, \"Total Batches Seen\": {\"sum\": 1296.0, \"count\": 1, \"min\": 1296, \"max\": 1296}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3643.1313085032043 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:52 INFO 140618438534976] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:53.285] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1180, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] # Finished training epoch 36 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] Loss (name: value) total: 6.6659567422337\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] Loss (name: value) kld: 0.11602888566752274\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] Loss (name: value) recons: 6.549927850564321\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] Loss (name: value) logppx: 6.6659567422337\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=6.6659567422337\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] patience losses:[6.686646580696106, 6.679269446267022, 6.680750323666467, 6.673586931493547, 6.668287144766913] min patience loss:6.668287144766913 current loss:6.6659567422337 absolute loss difference:0.0023304025332135936\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] Timing: train: 1.18s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650632.1045687, \"EndTime\": 1623650633.2901635, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 162864.0, \"count\": 1, \"min\": 162864, \"max\": 162864}, \"Total Batches Seen\": {\"sum\": 1296.0, \"count\": 1, \"min\": 1296, \"max\": 1296}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3815.292167655424 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:53 INFO 140515461125952] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:53.846] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] # Finished training epoch 37 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] Loss (name: value) total: 6.645617663860321\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] Loss (name: value) kld: 0.11844273149553272\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] Loss (name: value) recons: 6.527174936400519\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] Loss (name: value) logppx: 6.645617663860321\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] #quality_metric: host=algo-2, epoch=37, train total_loss <loss>=6.645617663860321\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] patience losses:[6.6631717218293085, 6.658994363413917, 6.657234172026317, 6.656618250740899, 6.650513880782658] min patience loss:6.650513880782658 current loss:6.645617663860321 absolute loss difference:0.004896216922336549\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] #progress_metric: host=algo-2, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650632.6523337, \"EndTime\": 1623650633.8515615, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 167499.0, \"count\": 1, \"min\": 167499, \"max\": 167499}, \"Total Batches Seen\": {\"sum\": 1332.0, \"count\": 1, \"min\": 1332, \"max\": 1332}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3774.4398770876824 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:53 INFO 140618438534976] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:54.482] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1191, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] # Finished training epoch 37 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] Loss (name: value) total: 6.66170738140742\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] Loss (name: value) kld: 0.11639874966608153\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] Loss (name: value) recons: 6.545308570067088\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] Loss (name: value) logppx: 6.66170738140742\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=6.66170738140742\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] patience losses:[6.679269446267022, 6.680750323666467, 6.673586931493547, 6.668287144766913, 6.6659567422337] min patience loss:6.6659567422337 current loss:6.66170738140742 absolute loss difference:0.004249360826279691\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650633.2904782, \"EndTime\": 1623650634.4872527, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 167388.0, \"count\": 1, \"min\": 167388, \"max\": 167388}, \"Total Batches Seen\": {\"sum\": 1332.0, \"count\": 1, \"min\": 1332, \"max\": 1332}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3779.739136878321 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:54 INFO 140515461125952] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:55.771] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1283, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] # Finished training epoch 38 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] Loss (name: value) total: 6.658493955930074\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] Loss (name: value) kld: 0.118169324265586\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] Loss (name: value) recons: 6.540324588616689\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] Loss (name: value) logppx: 6.658493955930074\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=6.658493955930074\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] patience losses:[6.680750323666467, 6.673586931493547, 6.668287144766913, 6.6659567422337, 6.66170738140742] min patience loss:6.66170738140742 current loss:6.658493955930074 absolute loss difference:0.0032134254773463766\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] Timing: train: 1.28s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650634.4874961, \"EndTime\": 1623650635.775375, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 171912.0, \"count\": 1, \"min\": 171912, \"max\": 171912}, \"Total Batches Seen\": {\"sum\": 1368.0, \"count\": 1, \"min\": 1368, \"max\": 1368}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3512.3479504766233 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:55 INFO 140515461125952] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:55.027] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1174, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] # Finished training epoch 38 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] Loss (name: value) total: 6.649336013529036\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] Loss (name: value) kld: 0.12170306551787588\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] Loss (name: value) recons: 6.527632964981927\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] Loss (name: value) logppx: 6.649336013529036\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] #quality_metric: host=algo-2, epoch=38, train total_loss <loss>=6.649336013529036\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] patience losses:[6.658994363413917, 6.657234172026317, 6.656618250740899, 6.650513880782658, 6.645617663860321] min patience loss:6.645617663860321 current loss:6.649336013529036 absolute loss difference:0.003718349668714538\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] #progress_metric: host=algo-2, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650633.8519778, \"EndTime\": 1623650635.028933, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 172026.0, \"count\": 1, \"min\": 172026, \"max\": 172026}, \"Total Batches Seen\": {\"sum\": 1368.0, \"count\": 1, \"min\": 1368, \"max\": 1368}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3845.9744274368495 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:55 INFO 140618438534976] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:56.290] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1260, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] # Finished training epoch 39 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] Loss (name: value) total: 6.638281146685283\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] Loss (name: value) kld: 0.1198143099124233\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] Loss (name: value) recons: 6.518466830253601\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] Loss (name: value) logppx: 6.638281146685283\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] #quality_metric: host=algo-2, epoch=39, train total_loss <loss>=6.638281146685283\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] patience losses:[6.657234172026317, 6.656618250740899, 6.650513880782658, 6.645617663860321, 6.649336013529036] min patience loss:6.645617663860321 current loss:6.638281146685283 absolute loss difference:0.0073365171750383595\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] #progress_metric: host=algo-2, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650635.0292127, \"EndTime\": 1623650636.2963357, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 176553.0, \"count\": 1, \"min\": 176553, \"max\": 176553}, \"Total Batches Seen\": {\"sum\": 1404.0, \"count\": 1, \"min\": 1404, \"max\": 1404}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3572.2348496763702 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:56 INFO 140618438534976] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:56.996] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:56 INFO 140515461125952] # Finished training epoch 39 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:56 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:56 INFO 140515461125952] Loss (name: value) total: 6.655793057547675\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:56 INFO 140515461125952] Loss (name: value) kld: 0.12112810659325784\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:56 INFO 140515461125952] Loss (name: value) recons: 6.534664968649547\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:56 INFO 140515461125952] Loss (name: value) logppx: 6.655793057547675\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:56 INFO 140515461125952] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=6.655793057547675\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:57 INFO 140515461125952] patience losses:[6.673586931493547, 6.668287144766913, 6.6659567422337, 6.66170738140742, 6.658493955930074] min patience loss:6.658493955930074 current loss:6.655793057547675 absolute loss difference:0.00270089838239862\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:57 INFO 140515461125952] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:57 INFO 140515461125952] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650635.7756832, \"EndTime\": 1623650637.0009851, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 176436.0, \"count\": 1, \"min\": 176436, \"max\": 176436}, \"Total Batches Seen\": {\"sum\": 1404.0, \"count\": 1, \"min\": 1404, \"max\": 1404}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:57 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3691.661158399532 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:57 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:57 INFO 140515461125952] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:57.491] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1194, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] # Finished training epoch 40 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] Loss (name: value) total: 6.640003363291423\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] Loss (name: value) kld: 0.12436343812280232\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] Loss (name: value) recons: 6.515639947520362\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] Loss (name: value) logppx: 6.640003363291423\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] #quality_metric: host=algo-2, epoch=40, train total_loss <loss>=6.640003363291423\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] patience losses:[6.656618250740899, 6.650513880782658, 6.645617663860321, 6.649336013529036, 6.638281146685283] min patience loss:6.638281146685283 current loss:6.640003363291423 absolute loss difference:0.0017222166061401367\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] #progress_metric: host=algo-2, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650636.296639, \"EndTime\": 1623650637.4938915, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 181080.0, \"count\": 1, \"min\": 181080, \"max\": 181080}, \"Total Batches Seen\": {\"sum\": 1440.0, \"count\": 1, \"min\": 1440, \"max\": 1440}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3780.636251718076 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:57 INFO 140618438534976] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:58.248] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1246, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] # Finished training epoch 40 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] Loss (name: value) total: 6.653499709235297\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] Loss (name: value) kld: 0.12279375735670328\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] Loss (name: value) recons: 6.53070596853892\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] Loss (name: value) logppx: 6.653499709235297\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=6.653499709235297\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] patience losses:[6.668287144766913, 6.6659567422337, 6.66170738140742, 6.658493955930074, 6.655793057547675] min patience loss:6.655793057547675 current loss:6.653499709235297 absolute loss difference:0.0022933483123779297\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650637.0013158, \"EndTime\": 1623650638.2535071, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 180960.0, \"count\": 1, \"min\": 180960, \"max\": 180960}, \"Total Batches Seen\": {\"sum\": 1440.0, \"count\": 1, \"min\": 1440, \"max\": 1440}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3612.391901325798 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:58 INFO 140515461125952] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:58.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1240, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] # Finished training epoch 41 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] Loss (name: value) total: 6.636732816696167\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] Loss (name: value) kld: 0.1269007261014647\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] Loss (name: value) recons: 6.509832077556187\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] Loss (name: value) logppx: 6.636732816696167\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] #quality_metric: host=algo-2, epoch=41, train total_loss <loss>=6.636732816696167\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] patience losses:[6.650513880782658, 6.645617663860321, 6.649336013529036, 6.638281146685283, 6.640003363291423] min patience loss:6.638281146685283 current loss:6.636732816696167 absolute loss difference:0.0015483299891156932\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] #progress_metric: host=algo-2, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650637.4942327, \"EndTime\": 1623650638.7413924, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 185607.0, \"count\": 1, \"min\": 185607, \"max\": 185607}, \"Total Batches Seen\": {\"sum\": 1476.0, \"count\": 1, \"min\": 1476, \"max\": 1476}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3629.3621532104394 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:58 INFO 140618438534976] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:03:59.453] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1198, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] # Finished training epoch 41 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] Loss (name: value) total: 6.651926305558947\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] Loss (name: value) kld: 0.12509044621967608\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] Loss (name: value) recons: 6.526835911803776\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] Loss (name: value) logppx: 6.651926305558947\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=6.651926305558947\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] patience losses:[6.6659567422337, 6.66170738140742, 6.658493955930074, 6.655793057547675, 6.653499709235297] min patience loss:6.653499709235297 current loss:6.651926305558947 absolute loss difference:0.0015734036763506154\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650638.253814, \"EndTime\": 1623650639.4584944, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 185484.0, \"count\": 1, \"min\": 185484, \"max\": 185484}, \"Total Batches Seen\": {\"sum\": 1476.0, \"count\": 1, \"min\": 1476, \"max\": 1476}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3754.9707609467855 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:03:59 INFO 140515461125952] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:03:59.938] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1196, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] # Finished training epoch 42 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] Loss (name: value) total: 6.631850216123793\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] Loss (name: value) kld: 0.12676415851132738\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] Loss (name: value) recons: 6.5050860908296375\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] Loss (name: value) logppx: 6.631850216123793\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] #quality_metric: host=algo-2, epoch=42, train total_loss <loss>=6.631850216123793\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] patience losses:[6.645617663860321, 6.649336013529036, 6.638281146685283, 6.640003363291423, 6.636732816696167] min patience loss:6.636732816696167 current loss:6.631850216123793 absolute loss difference:0.004882600572374329\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] Timing: train: 1.20s, val: 0.01s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] #progress_metric: host=algo-2, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650638.7417614, \"EndTime\": 1623650639.9468596, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 190134.0, \"count\": 1, \"min\": 190134, \"max\": 190134}, \"Total Batches Seen\": {\"sum\": 1512.0, \"count\": 1, \"min\": 1512, \"max\": 1512}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3755.958651965618 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:03:59 INFO 140618438534976] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:01.383] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1434, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] # Finished training epoch 43 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] Loss (name: value) total: 6.634108510282305\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] Loss (name: value) kld: 0.13020584670205912\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] Loss (name: value) recons: 6.5039026737213135\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] Loss (name: value) logppx: 6.634108510282305\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] #quality_metric: host=algo-2, epoch=43, train total_loss <loss>=6.634108510282305\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] patience losses:[6.649336013529036, 6.638281146685283, 6.640003363291423, 6.636732816696167, 6.631850216123793] min patience loss:6.631850216123793 current loss:6.634108510282305 absolute loss difference:0.0022582941585120864\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] Timing: train: 1.44s, val: 0.00s, epoch: 1.44s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] #progress_metric: host=algo-2, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650639.947165, \"EndTime\": 1623650641.384907, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 194661.0, \"count\": 1, \"min\": 194661, \"max\": 194661}, \"Total Batches Seen\": {\"sum\": 1548.0, \"count\": 1, \"min\": 1548, \"max\": 1548}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3148.366765319044 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:01 INFO 140618438534976] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:00.802] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1341, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] # Finished training epoch 42 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] Loss (name: value) total: 6.645253539085388\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] Loss (name: value) kld: 0.1266043973672721\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] Loss (name: value) recons: 6.518649107880062\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] Loss (name: value) logppx: 6.645253539085388\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=6.645253539085388\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] patience losses:[6.66170738140742, 6.658493955930074, 6.655793057547675, 6.653499709235297, 6.651926305558947] min patience loss:6.651926305558947 current loss:6.645253539085388 absolute loss difference:0.006672766473558411\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] Timing: train: 1.35s, val: 0.00s, epoch: 1.35s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650639.4587486, \"EndTime\": 1623650640.8088195, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 190008.0, \"count\": 1, \"min\": 190008, \"max\": 190008}, \"Total Batches Seen\": {\"sum\": 1512.0, \"count\": 1, \"min\": 1512, \"max\": 1512}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3350.640319287684 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:00 INFO 140515461125952] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:02.333] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1524, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] # Finished training epoch 43 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] Loss (name: value) total: 6.642745488219791\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] Loss (name: value) kld: 0.126687022857368\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] Loss (name: value) recons: 6.516058484713237\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] Loss (name: value) logppx: 6.642745488219791\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=6.642745488219791\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] patience losses:[6.658493955930074, 6.655793057547675, 6.653499709235297, 6.651926305558947, 6.645253539085388] min patience loss:6.645253539085388 current loss:6.642745488219791 absolute loss difference:0.0025080508655968003\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] Timing: train: 1.53s, val: 0.00s, epoch: 1.53s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650640.8090518, \"EndTime\": 1623650642.3400745, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 194532.0, \"count\": 1, \"min\": 194532, \"max\": 194532}, \"Total Batches Seen\": {\"sum\": 1548.0, \"count\": 1, \"min\": 1548, \"max\": 1548}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=2954.5874501166027 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:02 INFO 140515461125952] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:02.881] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1495, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] # Finished training epoch 44 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] Loss (name: value) total: 6.620958440833622\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] Loss (name: value) kld: 0.13036247053080136\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] Loss (name: value) recons: 6.490595950020684\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] Loss (name: value) logppx: 6.620958440833622\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] #quality_metric: host=algo-2, epoch=44, train total_loss <loss>=6.620958440833622\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] patience losses:[6.638281146685283, 6.640003363291423, 6.636732816696167, 6.631850216123793, 6.634108510282305] min patience loss:6.631850216123793 current loss:6.620958440833622 absolute loss difference:0.010891775290170713\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] Timing: train: 1.50s, val: 0.00s, epoch: 1.50s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] #progress_metric: host=algo-2, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650641.385191, \"EndTime\": 1623650642.8874416, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 199188.0, \"count\": 1, \"min\": 199188, \"max\": 199188}, \"Total Batches Seen\": {\"sum\": 1584.0, \"count\": 1, \"min\": 1584, \"max\": 1584}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3013.1432092531463 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:02 INFO 140618438534976] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:03.806] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1464, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] # Finished training epoch 44 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] Loss (name: value) total: 6.645845293998718\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] Loss (name: value) kld: 0.13129934409840238\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] Loss (name: value) recons: 6.514545924133724\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] Loss (name: value) logppx: 6.645845293998718\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=6.645845293998718\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] patience losses:[6.655793057547675, 6.653499709235297, 6.651926305558947, 6.645253539085388, 6.642745488219791] min patience loss:6.642745488219791 current loss:6.645845293998718 absolute loss difference:0.0030998057789268785\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] Timing: train: 1.47s, val: 0.00s, epoch: 1.47s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650642.3403988, \"EndTime\": 1623650643.8083906, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 199056.0, \"count\": 1, \"min\": 199056, \"max\": 199056}, \"Total Batches Seen\": {\"sum\": 1584.0, \"count\": 1, \"min\": 1584, \"max\": 1584}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3081.3851832304717 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:03 INFO 140515461125952] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:04.238] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1349, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] # Finished training epoch 45 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] Loss (name: value) total: 6.618664072619544\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] Loss (name: value) kld: 0.13449122632543245\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] Loss (name: value) recons: 6.484172774685754\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] Loss (name: value) logppx: 6.618664072619544\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] #quality_metric: host=algo-2, epoch=45, train total_loss <loss>=6.618664072619544\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] patience losses:[6.640003363291423, 6.636732816696167, 6.631850216123793, 6.634108510282305, 6.620958440833622] min patience loss:6.620958440833622 current loss:6.618664072619544 absolute loss difference:0.002294368214077913\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] Timing: train: 1.35s, val: 0.01s, epoch: 1.36s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] #progress_metric: host=algo-2, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650642.8877616, \"EndTime\": 1623650644.244904, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 203715.0, \"count\": 1, \"min\": 203715, \"max\": 203715}, \"Total Batches Seen\": {\"sum\": 1620.0, \"count\": 1, \"min\": 1620, \"max\": 1620}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3335.2557303257104 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:04 INFO 140618438534976] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:05.552] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1743, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] # Finished training epoch 45 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] Loss (name: value) total: 6.636375274923113\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] Loss (name: value) kld: 0.13246841583814886\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] Loss (name: value) recons: 6.503906872537401\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] Loss (name: value) logppx: 6.636375274923113\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=6.636375274923113\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] patience losses:[6.653499709235297, 6.651926305558947, 6.645253539085388, 6.642745488219791, 6.645845293998718] min patience loss:6.642745488219791 current loss:6.636375274923113 absolute loss difference:0.0063702132966785285\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] Timing: train: 1.74s, val: 0.00s, epoch: 1.75s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650643.8087363, \"EndTime\": 1623650645.5584292, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 203580.0, \"count\": 1, \"min\": 203580, \"max\": 203580}, \"Total Batches Seen\": {\"sum\": 1620.0, \"count\": 1, \"min\": 1620, \"max\": 1620}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=2585.344682217041 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:05 INFO 140515461125952] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:05.788] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1543, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] # Finished training epoch 46 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] Loss (name: value) total: 6.607987854215834\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] Loss (name: value) kld: 0.1353585799742076\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] Loss (name: value) recons: 6.472629268964131\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] Loss (name: value) logppx: 6.607987854215834\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] #quality_metric: host=algo-2, epoch=46, train total_loss <loss>=6.607987854215834\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] patience losses:[6.636732816696167, 6.631850216123793, 6.634108510282305, 6.620958440833622, 6.618664072619544] min patience loss:6.618664072619544 current loss:6.607987854215834 absolute loss difference:0.010676218403710358\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] Timing: train: 1.54s, val: 0.00s, epoch: 1.55s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] #progress_metric: host=algo-2, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650644.2452343, \"EndTime\": 1623650645.793976, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 208242.0, \"count\": 1, \"min\": 208242, \"max\": 208242}, \"Total Batches Seen\": {\"sum\": 1656.0, \"count\": 1, \"min\": 1656, \"max\": 1656}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=2922.6993138874755 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:05 INFO 140618438534976] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:06.850] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1289, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] # Finished training epoch 46 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] Loss (name: value) total: 6.633299231529236\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] Loss (name: value) kld: 0.13526543364342716\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] Loss (name: value) recons: 6.498033788469103\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] Loss (name: value) logppx: 6.633299231529236\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=6.633299231529236\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] patience losses:[6.651926305558947, 6.645253539085388, 6.642745488219791, 6.645845293998718, 6.636375274923113] min patience loss:6.636375274923113 current loss:6.633299231529236 absolute loss difference:0.003076043393877015\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] Timing: train: 1.29s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650645.5587564, \"EndTime\": 1623650646.8557398, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 208104.0, \"count\": 1, \"min\": 208104, \"max\": 208104}, \"Total Batches Seen\": {\"sum\": 1656.0, \"count\": 1, \"min\": 1656, \"max\": 1656}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3486.9745948362724 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:06 INFO 140515461125952] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:07.160] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1363, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] # Finished training epoch 47 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] Loss (name: value) total: 6.614502986272176\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] Loss (name: value) kld: 0.13775102090504435\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] Loss (name: value) recons: 6.476751983165741\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] Loss (name: value) logppx: 6.614502986272176\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] #quality_metric: host=algo-2, epoch=47, train total_loss <loss>=6.614502986272176\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] patience losses:[6.631850216123793, 6.634108510282305, 6.620958440833622, 6.618664072619544, 6.607987854215834] min patience loss:6.607987854215834 current loss:6.614502986272176 absolute loss difference:0.006515132056342132\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] Timing: train: 1.37s, val: 0.00s, epoch: 1.37s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] #progress_metric: host=algo-2, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650645.794302, \"EndTime\": 1623650647.1620471, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 212769.0, \"count\": 1, \"min\": 212769, \"max\": 212769}, \"Total Batches Seen\": {\"sum\": 1692.0, \"count\": 1, \"min\": 1692, \"max\": 1692}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3309.4561610155993 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:07 INFO 140618438534976] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:08.105] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1248, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] # Finished training epoch 47 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] Loss (name: value) total: 6.623993900087145\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] Loss (name: value) kld: 0.13552467038647997\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] Loss (name: value) recons: 6.488469262917836\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] Loss (name: value) logppx: 6.623993900087145\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=6.623993900087145\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] patience losses:[6.645253539085388, 6.642745488219791, 6.645845293998718, 6.636375274923113, 6.633299231529236] min patience loss:6.633299231529236 current loss:6.623993900087145 absolute loss difference:0.009305331442091003\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650646.8565018, \"EndTime\": 1623650648.1130087, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 212628.0, \"count\": 1, \"min\": 212628, \"max\": 212628}, \"Total Batches Seen\": {\"sum\": 1692.0, \"count\": 1, \"min\": 1692, \"max\": 1692}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3599.956989549233 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:08 INFO 140515461125952] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:08.431] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1268, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] # Finished training epoch 48 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] Loss (name: value) total: 6.6031726532512245\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] Loss (name: value) kld: 0.1399712103108565\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] Loss (name: value) recons: 6.46320147646798\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] Loss (name: value) logppx: 6.6031726532512245\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] #quality_metric: host=algo-2, epoch=48, train total_loss <loss>=6.6031726532512245\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] patience losses:[6.634108510282305, 6.620958440833622, 6.618664072619544, 6.607987854215834, 6.614502986272176] min patience loss:6.607987854215834 current loss:6.6031726532512245 absolute loss difference:0.00481520096460919\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] #progress_metric: host=algo-2, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650647.1623502, \"EndTime\": 1623650648.4373863, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 217296.0, \"count\": 1, \"min\": 217296, \"max\": 217296}, \"Total Batches Seen\": {\"sum\": 1728.0, \"count\": 1, \"min\": 1728, \"max\": 1728}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3550.0183988470994 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:08 INFO 140618438534976] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:09.313] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1199, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] # Finished training epoch 48 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] Loss (name: value) total: 6.6232131852044\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] Loss (name: value) kld: 0.13962118544926247\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:09.607] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1169, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] # Finished training epoch 49 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] Loss (name: value) total: 6.607742342684004\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] Loss (name: value) kld: 0.1422160734525985\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] Loss (name: value) recons: 6.465526243050893\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] Loss (name: value) logppx: 6.607742342684004\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] #quality_metric: host=algo-2, epoch=49, train total_loss <loss>=6.607742342684004\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] patience losses:[6.620958440833622, 6.618664072619544, 6.607987854215834, 6.614502986272176, 6.6031726532512245] min patience loss:6.6031726532512245 current loss:6.607742342684004 absolute loss difference:0.004569689432779356\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] #progress_metric: host=algo-2, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650648.4377103, \"EndTime\": 1623650649.6105788, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 221823.0, \"count\": 1, \"min\": 221823, \"max\": 221823}, \"Total Batches Seen\": {\"sum\": 1764.0, \"count\": 1, \"min\": 1764, \"max\": 1764}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3859.237451379998 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:09 INFO 140618438534976] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] Loss (name: value) recons: 6.4835920333862305\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] Loss (name: value) logppx: 6.6232131852044\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=6.6232131852044\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] patience losses:[6.642745488219791, 6.645845293998718, 6.636375274923113, 6.633299231529236, 6.623993900087145] min patience loss:6.623993900087145 current loss:6.6232131852044 absolute loss difference:0.0007807148827447818\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650648.113591, \"EndTime\": 1623650649.319952, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 217152.0, \"count\": 1, \"min\": 217152, \"max\": 217152}, \"Total Batches Seen\": {\"sum\": 1728.0, \"count\": 1, \"min\": 1728, \"max\": 1728}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3749.397093199484 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:09 INFO 140515461125952] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:10.872] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1261, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] # Finished training epoch 50 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] Loss (name: value) total: 6.605443133248223\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] Loss (name: value) kld: 0.1438953158342176\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] Loss (name: value) recons: 6.461547825071547\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] Loss (name: value) logppx: 6.605443133248223\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] #quality_metric: host=algo-2, epoch=50, train total_loss <loss>=6.605443133248223\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] patience losses:[6.618664072619544, 6.607987854215834, 6.614502986272176, 6.6031726532512245, 6.607742342684004] min patience loss:6.6031726532512245 current loss:6.605443133248223 absolute loss difference:0.0022704799969988088\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] #progress_metric: host=algo-2, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650649.6109014, \"EndTime\": 1623650650.8750656, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 226350.0, \"count\": 1, \"min\": 226350, \"max\": 226350}, \"Total Batches Seen\": {\"sum\": 1800.0, \"count\": 1, \"min\": 1800, \"max\": 1800}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3580.5365950796227 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:10 INFO 140618438534976] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:10.567] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1247, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] # Finished training epoch 49 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] Loss (name: value) total: 6.625826921727922\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] Loss (name: value) kld: 0.14093489944934845\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] Loss (name: value) recons: 6.48489205704795\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] Loss (name: value) logppx: 6.625826921727922\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=6.625826921727922\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] patience losses:[6.645845293998718, 6.636375274923113, 6.633299231529236, 6.623993900087145, 6.6232131852044] min patience loss:6.6232131852044 current loss:6.625826921727922 absolute loss difference:0.0026137365235223697\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650649.3202755, \"EndTime\": 1623650650.5694537, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 221676.0, \"count\": 1, \"min\": 221676, \"max\": 221676}, \"Total Batches Seen\": {\"sum\": 1764.0, \"count\": 1, \"min\": 1764, \"max\": 1764}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3621.117309122109 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:10 INFO 140515461125952] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:11.862] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1292, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] # Finished training epoch 50 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] Loss (name: value) total: 6.614999764495426\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] Loss (name: value) kld: 0.14214585359311765\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] Loss (name: value) recons: 6.47285395860672\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] Loss (name: value) logppx: 6.614999764495426\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=6.614999764495426\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] patience losses:[6.636375274923113, 6.633299231529236, 6.623993900087145, 6.6232131852044, 6.625826921727922] min patience loss:6.6232131852044 current loss:6.614999764495426 absolute loss difference:0.008213420708973906\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] Timing: train: 1.29s, val: 0.01s, epoch: 1.30s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650650.56976, \"EndTime\": 1623650651.8699708, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 226200.0, \"count\": 1, \"min\": 226200, \"max\": 226200}, \"Total Batches Seen\": {\"sum\": 1800.0, \"count\": 1, \"min\": 1800, \"max\": 1800}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3478.9920191413 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:11 INFO 140515461125952] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:12.222] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1346, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] # Finished training epoch 51 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] Loss (name: value) total: 6.602865225738949\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] Loss (name: value) kld: 0.14546093437820673\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] Loss (name: value) recons: 6.457404308848911\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] Loss (name: value) logppx: 6.602865225738949\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] #quality_metric: host=algo-2, epoch=51, train total_loss <loss>=6.602865225738949\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] patience losses:[6.607987854215834, 6.614502986272176, 6.6031726532512245, 6.607742342684004, 6.605443133248223] min patience loss:6.6031726532512245 current loss:6.602865225738949 absolute loss difference:0.0003074275122756376\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] Timing: train: 1.35s, val: 0.00s, epoch: 1.35s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] #progress_metric: host=algo-2, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650650.8754241, \"EndTime\": 1623650652.226806, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 50, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 230877.0, \"count\": 1, \"min\": 230877, \"max\": 230877}, \"Total Batches Seen\": {\"sum\": 1836.0, \"count\": 1, \"min\": 1836, \"max\": 1836}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 102.0, \"count\": 1, \"min\": 102, \"max\": 102}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3349.4341951933548 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:12 INFO 140618438534976] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:13.464] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1237, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] # Finished training epoch 52 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] Loss (name: value) total: 6.597774095005459\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] Loss (name: value) kld: 0.14708303618762228\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] Loss (name: value) recons: 6.4506910178396435\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] Loss (name: value) logppx: 6.597774095005459\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] #quality_metric: host=algo-2, epoch=52, train total_loss <loss>=6.597774095005459\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] patience losses:[6.614502986272176, 6.6031726532512245, 6.607742342684004, 6.605443133248223, 6.602865225738949] min patience loss:6.602865225738949 current loss:6.597774095005459 absolute loss difference:0.00509113073348999\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] #progress_metric: host=algo-2, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650652.2271035, \"EndTime\": 1623650653.4705772, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 51, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 235404.0, \"count\": 1, \"min\": 235404, \"max\": 235404}, \"Total Batches Seen\": {\"sum\": 1872.0, \"count\": 1, \"min\": 1872, \"max\": 1872}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 104.0, \"count\": 1, \"min\": 104, \"max\": 104}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3639.6675482963487 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:13 INFO 140618438534976] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:13.226] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1355, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] # Finished training epoch 51 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] Loss (name: value) total: 6.615026765399509\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] Loss (name: value) kld: 0.14552729866570896\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] Loss (name: value) recons: 6.469499462180668\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] Loss (name: value) logppx: 6.615026765399509\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] #quality_metric: host=algo-1, epoch=51, train total_loss <loss>=6.615026765399509\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] patience losses:[6.633299231529236, 6.623993900087145, 6.6232131852044, 6.625826921727922, 6.614999764495426] min patience loss:6.614999764495426 current loss:6.615026765399509 absolute loss difference:2.7000904083251953e-05\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] Timing: train: 1.36s, val: 0.00s, epoch: 1.36s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650651.8702807, \"EndTime\": 1623650653.2287223, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 50, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 230724.0, \"count\": 1, \"min\": 230724, \"max\": 230724}, \"Total Batches Seen\": {\"sum\": 1836.0, \"count\": 1, \"min\": 1836, \"max\": 1836}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 102.0, \"count\": 1, \"min\": 102, \"max\": 102}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3329.89048600527 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:13 INFO 140515461125952] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:14.732] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1260, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] # Finished training epoch 53 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] Loss (name: value) total: 6.590777821011013\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] Loss (name: value) kld: 0.1495644837203953\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] Loss (name: value) recons: 6.441213296519385\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] Loss (name: value) logppx: 6.590777821011013\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] #quality_metric: host=algo-2, epoch=53, train total_loss <loss>=6.590777821011013\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] patience losses:[6.6031726532512245, 6.607742342684004, 6.605443133248223, 6.602865225738949, 6.597774095005459] min patience loss:6.597774095005459 current loss:6.590777821011013 absolute loss difference:0.006996273994445801\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] #progress_metric: host=algo-2, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650653.471089, \"EndTime\": 1623650654.7384617, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 52, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 239931.0, \"count\": 1, \"min\": 239931, \"max\": 239931}, \"Total Batches Seen\": {\"sum\": 1908.0, \"count\": 1, \"min\": 1908, \"max\": 1908}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 106.0, \"count\": 1, \"min\": 106, \"max\": 106}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3571.478267673703 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:14 INFO 140618438534976] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:14.463] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1234, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] # Finished training epoch 52 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] Loss (name: value) total: 6.611691640483008\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] Loss (name: value) kld: 0.1469906358462241\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] Loss (name: value) recons: 6.46470104985767\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] Loss (name: value) logppx: 6.611691640483008\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] #quality_metric: host=algo-1, epoch=52, train total_loss <loss>=6.611691640483008\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] patience losses:[6.623993900087145, 6.6232131852044, 6.625826921727922, 6.614999764495426, 6.615026765399509] min patience loss:6.614999764495426 current loss:6.611691640483008 absolute loss difference:0.003308124012417757\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650653.2290494, \"EndTime\": 1623650654.4694352, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 51, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 235248.0, \"count\": 1, \"min\": 235248, \"max\": 235248}, \"Total Batches Seen\": {\"sum\": 1872.0, \"count\": 1, \"min\": 1872, \"max\": 1872}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 104.0, \"count\": 1, \"min\": 104, \"max\": 104}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3646.79118383199 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:14 INFO 140515461125952] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:15.744] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1274, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] # Finished training epoch 53 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] Loss (name: value) total: 6.606848577658336\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] Loss (name: value) kld: 0.14731755221469534\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] Loss (name: value) recons: 6.459531082047357\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] Loss (name: value) logppx: 6.606848577658336\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] #quality_metric: host=algo-1, epoch=53, train total_loss <loss>=6.606848577658336\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] patience losses:[6.6232131852044, 6.625826921727922, 6.614999764495426, 6.615026765399509, 6.611691640483008] min patience loss:6.611691640483008 current loss:6.606848577658336 absolute loss difference:0.004843062824672728\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] Timing: train: 1.28s, val: 0.01s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650654.4697201, \"EndTime\": 1623650655.752007, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 52, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 239772.0, \"count\": 1, \"min\": 239772, \"max\": 239772}, \"Total Batches Seen\": {\"sum\": 1908.0, \"count\": 1, \"min\": 1908, \"max\": 1908}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 106.0, \"count\": 1, \"min\": 106, \"max\": 106}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3527.2531760562083 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:15 INFO 140515461125952] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:16.016] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1276, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] # Finished training epoch 54 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] Loss (name: value) total: 6.595546987321642\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] Loss (name: value) kld: 0.1516106083161301\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] Loss (name: value) recons: 6.4439364141888085\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] Loss (name: value) logppx: 6.595546987321642\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] #quality_metric: host=algo-2, epoch=54, train total_loss <loss>=6.595546987321642\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] patience losses:[6.607742342684004, 6.605443133248223, 6.602865225738949, 6.597774095005459, 6.590777821011013] min patience loss:6.590777821011013 current loss:6.595546987321642 absolute loss difference:0.004769166310628847\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] #progress_metric: host=algo-2, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650654.7388089, \"EndTime\": 1623650656.0181794, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 53, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 244458.0, \"count\": 1, \"min\": 244458, \"max\": 244458}, \"Total Batches Seen\": {\"sum\": 1944.0, \"count\": 1, \"min\": 1944, \"max\": 1944}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3537.9564230735045 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:16 INFO 140618438534976] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:17.248] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1229, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] # Finished training epoch 55 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] Loss (name: value) total: 6.583533081743452\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] Loss (name: value) kld: 0.15220660385158327\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] Loss (name: value) recons: 6.431326508522034\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] Loss (name: value) logppx: 6.583533081743452\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] #quality_metric: host=algo-2, epoch=55, train total_loss <loss>=6.583533081743452\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] patience losses:[6.605443133248223, 6.602865225738949, 6.597774095005459, 6.590777821011013, 6.595546987321642] min patience loss:6.590777821011013 current loss:6.583533081743452 absolute loss difference:0.007244739267560973\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] Timing: train: 1.23s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] #progress_metric: host=algo-2, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650656.0184612, \"EndTime\": 1623650657.2541986, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 54, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 248985.0, \"count\": 1, \"min\": 248985, \"max\": 248985}, \"Total Batches Seen\": {\"sum\": 1980.0, \"count\": 1, \"min\": 1980, \"max\": 1980}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 110.0, \"count\": 1, \"min\": 110, \"max\": 110}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3662.9200532357254 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:17 INFO 140618438534976] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:16.993] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1239, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] # Finished training epoch 54 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] Loss (name: value) total: 6.606479194429186\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] Loss (name: value) kld: 0.15084928336242834\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] Loss (name: value) recons: 6.455629871951209\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] Loss (name: value) logppx: 6.606479194429186\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] #quality_metric: host=algo-1, epoch=54, train total_loss <loss>=6.606479194429186\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] patience losses:[6.625826921727922, 6.614999764495426, 6.615026765399509, 6.611691640483008, 6.606848577658336] min patience loss:6.606848577658336 current loss:6.606479194429186 absolute loss difference:0.00036938322914981114\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650655.7527533, \"EndTime\": 1623650656.9970737, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 53, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 244296.0, \"count\": 1, \"min\": 244296, \"max\": 244296}, \"Total Batches Seen\": {\"sum\": 1944.0, \"count\": 1, \"min\": 1944, \"max\": 1944}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 108.0, \"count\": 1, \"min\": 108, \"max\": 108}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3635.138676432386 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:16 INFO 140515461125952] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:18.491] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1236, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] # Finished training epoch 56 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] Loss (name: value) total: 6.581539981895023\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] Loss (name: value) kld: 0.15593071716527143\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] Loss (name: value) recons: 6.425609237617916\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] Loss (name: value) logppx: 6.581539981895023\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] #quality_metric: host=algo-2, epoch=56, train total_loss <loss>=6.581539981895023\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] patience losses:[6.602865225738949, 6.597774095005459, 6.590777821011013, 6.595546987321642, 6.583533081743452] min patience loss:6.583533081743452 current loss:6.581539981895023 absolute loss difference:0.00199309984842877\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] #progress_metric: host=algo-2, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650657.254522, \"EndTime\": 1623650658.497867, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 55, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 253512.0, \"count\": 1, \"min\": 253512, \"max\": 253512}, \"Total Batches Seen\": {\"sum\": 2016.0, \"count\": 1, \"min\": 2016, \"max\": 2016}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 112.0, \"count\": 1, \"min\": 112, \"max\": 112}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3640.491688800276 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:18 INFO 140618438534976] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:18.226] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1227, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] # Finished training epoch 55 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] Loss (name: value) total: 6.59709769487381\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] Loss (name: value) kld: 0.15126878468112814\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] Loss (name: value) recons: 6.445828941133287\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] Loss (name: value) logppx: 6.59709769487381\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] #quality_metric: host=algo-1, epoch=55, train total_loss <loss>=6.59709769487381\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] patience losses:[6.614999764495426, 6.615026765399509, 6.611691640483008, 6.606848577658336, 6.606479194429186] min patience loss:6.606479194429186 current loss:6.59709769487381 absolute loss difference:0.009381499555376038\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650656.9973881, \"EndTime\": 1623650658.2326155, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 54, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 248820.0, \"count\": 1, \"min\": 248820, \"max\": 248820}, \"Total Batches Seen\": {\"sum\": 1980.0, \"count\": 1, \"min\": 1980, \"max\": 1980}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 110.0, \"count\": 1, \"min\": 110, \"max\": 110}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3661.8913808578827 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:18 INFO 140515461125952] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:19.676] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1177, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] # Finished training epoch 57 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] Loss (name: value) total: 6.57903395096461\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] Loss (name: value) kld: 0.15519541915920046\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] Loss (name: value) recons: 6.423838535944621\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] Loss (name: value) logppx: 6.57903395096461\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] #quality_metric: host=algo-2, epoch=57, train total_loss <loss>=6.57903395096461\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] patience losses:[6.597774095005459, 6.590777821011013, 6.595546987321642, 6.583533081743452, 6.581539981895023] min patience loss:6.581539981895023 current loss:6.57903395096461 absolute loss difference:0.002506030930413239\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] #progress_metric: host=algo-2, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650658.498232, \"EndTime\": 1623650659.682178, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 56, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 258039.0, \"count\": 1, \"min\": 258039, \"max\": 258039}, \"Total Batches Seen\": {\"sum\": 2052.0, \"count\": 1, \"min\": 2052, \"max\": 2052}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 114.0, \"count\": 1, \"min\": 114, \"max\": 114}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3823.1240092153093 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:19 INFO 140618438534976] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:19.456] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1223, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] # Finished training epoch 56 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] Loss (name: value) total: 6.593268838193682\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] Loss (name: value) kld: 0.15225655937360394\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] Loss (name: value) recons: 6.441012329525417\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] Loss (name: value) logppx: 6.593268838193682\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] #quality_metric: host=algo-1, epoch=56, train total_loss <loss>=6.593268838193682\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] patience losses:[6.615026765399509, 6.611691640483008, 6.606848577658336, 6.606479194429186, 6.59709769487381] min patience loss:6.59709769487381 current loss:6.593268838193682 absolute loss difference:0.003828856680128112\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650658.232973, \"EndTime\": 1623650659.4613738, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 55, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 253344.0, \"count\": 1, \"min\": 253344, \"max\": 253344}, \"Total Batches Seen\": {\"sum\": 2016.0, \"count\": 1, \"min\": 2016, \"max\": 2016}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 112.0, \"count\": 1, \"min\": 112, \"max\": 112}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3682.346345692089 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:19 INFO 140515461125952] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:20.929] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1246, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] # Finished training epoch 58 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] Loss (name: value) total: 6.572844333118862\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] Loss (name: value) kld: 0.15975893806252214\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] Loss (name: value) recons: 6.413085374567244\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] Loss (name: value) logppx: 6.572844333118862\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] #quality_metric: host=algo-2, epoch=58, train total_loss <loss>=6.572844333118862\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] patience losses:[6.590777821011013, 6.595546987321642, 6.583533081743452, 6.581539981895023, 6.57903395096461] min patience loss:6.57903395096461 current loss:6.572844333118862 absolute loss difference:0.006189617845747897\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] #progress_metric: host=algo-2, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650659.6824949, \"EndTime\": 1623650660.9348118, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 57, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 262566.0, \"count\": 1, \"min\": 262566, \"max\": 262566}, \"Total Batches Seen\": {\"sum\": 2088.0, \"count\": 1, \"min\": 2088, \"max\": 2088}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 116.0, \"count\": 1, \"min\": 116, \"max\": 116}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3614.4109974134394 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:20 INFO 140618438534976] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:20.738] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1276, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] # Finished training epoch 57 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] Loss (name: value) total: 6.597070535024007\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] Loss (name: value) kld: 0.15630516927275392\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] Loss (name: value) recons: 6.440765374236637\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] Loss (name: value) logppx: 6.597070535024007\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] #quality_metric: host=algo-1, epoch=57, train total_loss <loss>=6.597070535024007\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] patience losses:[6.611691640483008, 6.606848577658336, 6.606479194429186, 6.59709769487381, 6.593268838193682] min patience loss:6.593268838193682 current loss:6.597070535024007 absolute loss difference:0.003801696830325163\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650659.4616978, \"EndTime\": 1623650660.7404234, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 56, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 257868.0, \"count\": 1, \"min\": 257868, \"max\": 257868}, \"Total Batches Seen\": {\"sum\": 2052.0, \"count\": 1, \"min\": 2052, \"max\": 2052}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 114.0, \"count\": 1, \"min\": 114, \"max\": 114}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3537.4508315652324 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:20 INFO 140515461125952] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:21.952] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1210, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] # Finished training epoch 58 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] Loss (name: value) total: 6.595549649662441\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] Loss (name: value) kld: 0.15815661713067028\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] Loss (name: value) recons: 6.43739308251275\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] Loss (name: value) logppx: 6.595549649662441\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] #quality_metric: host=algo-1, epoch=58, train total_loss <loss>=6.595549649662441\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] patience losses:[6.606848577658336, 6.606479194429186, 6.59709769487381, 6.593268838193682, 6.597070535024007] min patience loss:6.593268838193682 current loss:6.595549649662441 absolute loss difference:0.0022808114687595804\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650660.7407842, \"EndTime\": 1623650661.953668, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 57, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 262392.0, \"count\": 1, \"min\": 262392, \"max\": 262392}, \"Total Batches Seen\": {\"sum\": 2088.0, \"count\": 1, \"min\": 2088, \"max\": 2088}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 116.0, \"count\": 1, \"min\": 116, \"max\": 116}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3729.4477883084414 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:21 INFO 140515461125952] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:22.101] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1165, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] # Finished training epoch 59 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] Loss (name: value) total: 6.572991629441579\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] Loss (name: value) kld: 0.159138776982824\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] Loss (name: value) recons: 6.413852824105157\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] Loss (name: value) logppx: 6.572991629441579\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] #quality_metric: host=algo-2, epoch=59, train total_loss <loss>=6.572991629441579\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] patience losses:[6.595546987321642, 6.583533081743452, 6.581539981895023, 6.57903395096461, 6.572844333118862] min patience loss:6.572844333118862 current loss:6.572991629441579 absolute loss difference:0.00014729632271670567\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] #progress_metric: host=algo-2, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650660.9351404, \"EndTime\": 1623650662.1022727, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 58, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 267093.0, \"count\": 1, \"min\": 267093, \"max\": 267093}, \"Total Batches Seen\": {\"sum\": 2124.0, \"count\": 1, \"min\": 2124, \"max\": 2124}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 118.0, \"count\": 1, \"min\": 118, \"max\": 118}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3878.2406645513065 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:22 INFO 140618438534976] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:23.283] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1180, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] # Finished training epoch 60 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] Loss (name: value) total: 6.574332760439979\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] Loss (name: value) kld: 0.16455957748823696\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] Loss (name: value) recons: 6.409773137834337\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] Loss (name: value) logppx: 6.574332760439979\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] #quality_metric: host=algo-2, epoch=60, train total_loss <loss>=6.574332760439979\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] patience losses:[6.583533081743452, 6.581539981895023, 6.57903395096461, 6.572844333118862, 6.572991629441579] min patience loss:6.572844333118862 current loss:6.574332760439979 absolute loss difference:0.0014884273211164256\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] #progress_metric: host=algo-2, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650662.102536, \"EndTime\": 1623650663.2847576, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 59, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 271620.0, \"count\": 1, \"min\": 271620, \"max\": 271620}, \"Total Batches Seen\": {\"sum\": 2160.0, \"count\": 1, \"min\": 2160, \"max\": 2160}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 120.0, \"count\": 1, \"min\": 120, \"max\": 120}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3828.7246502648686 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:23 INFO 140618438534976] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:23.164] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1210, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] # Finished training epoch 59 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] Loss (name: value) total: 6.590862459606594\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] Loss (name: value) kld: 0.1604867544439104\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] Loss (name: value) recons: 6.430375701851315\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] Loss (name: value) logppx: 6.590862459606594\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] #quality_metric: host=algo-1, epoch=59, train total_loss <loss>=6.590862459606594\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] patience losses:[6.606479194429186, 6.59709769487381, 6.593268838193682, 6.597070535024007, 6.595549649662441] min patience loss:6.593268838193682 current loss:6.590862459606594 absolute loss difference:0.0024063785870875876\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] Timing: train: 1.21s, val: 0.01s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650661.9539902, \"EndTime\": 1623650663.1713321, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 58, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 266916.0, \"count\": 1, \"min\": 266916, \"max\": 266916}, \"Total Batches Seen\": {\"sum\": 2124.0, \"count\": 1, \"min\": 2124, \"max\": 2124}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 118.0, \"count\": 1, \"min\": 118, \"max\": 118}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3715.6358193359997 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:23 INFO 140515461125952] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:24.497] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] # Finished training epoch 61 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] Loss (name: value) total: 6.565851582421197\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] Loss (name: value) kld: 0.16846402982870737\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] Loss (name: value) recons: 6.397387517823113\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] Loss (name: value) logppx: 6.565851582421197\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] #quality_metric: host=algo-2, epoch=61, train total_loss <loss>=6.565851582421197\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] patience losses:[6.581539981895023, 6.57903395096461, 6.572844333118862, 6.572991629441579, 6.574332760439979] min patience loss:6.572844333118862 current loss:6.565851582421197 absolute loss difference:0.006992750697665251\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] #progress_metric: host=algo-2, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650663.2850635, \"EndTime\": 1623650664.5015087, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 60, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 276147.0, \"count\": 1, \"min\": 276147, \"max\": 276147}, \"Total Batches Seen\": {\"sum\": 2196.0, \"count\": 1, \"min\": 2196, \"max\": 2196}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 122.0, \"count\": 1, \"min\": 122, \"max\": 122}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3721.012134297012 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:24 INFO 140618438534976] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:24.385] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1212, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] # Finished training epoch 60 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] Loss (name: value) total: 6.589325626691182\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] Loss (name: value) kld: 0.16294803139236239\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] Loss (name: value) recons: 6.426377574602763\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] Loss (name: value) logppx: 6.589325626691182\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] #quality_metric: host=algo-1, epoch=60, train total_loss <loss>=6.589325626691182\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] patience losses:[6.59709769487381, 6.593268838193682, 6.597070535024007, 6.595549649662441, 6.590862459606594] min patience loss:6.590862459606594 current loss:6.589325626691182 absolute loss difference:0.0015368329154119564\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650663.171802, \"EndTime\": 1623650664.390819, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 59, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 271440.0, \"count\": 1, \"min\": 271440, \"max\": 271440}, \"Total Batches Seen\": {\"sum\": 2160.0, \"count\": 1, \"min\": 2160, \"max\": 2160}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 120.0, \"count\": 1, \"min\": 120, \"max\": 120}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3710.678130149841 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:24 INFO 140515461125952] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:25.760] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1258, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] # Finished training epoch 62 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] Loss (name: value) total: 6.566710154215495\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] Loss (name: value) kld: 0.1673126274512874\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] Loss (name: value) recons: 6.399397578504351\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] Loss (name: value) logppx: 6.566710154215495\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] #quality_metric: host=algo-2, epoch=62, train total_loss <loss>=6.566710154215495\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] patience losses:[6.57903395096461, 6.572844333118862, 6.572991629441579, 6.574332760439979, 6.565851582421197] min patience loss:6.565851582421197 current loss:6.566710154215495 absolute loss difference:0.0008585717942981574\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] #progress_metric: host=algo-2, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650664.5018225, \"EndTime\": 1623650665.7620125, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 61, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 280674.0, \"count\": 1, \"min\": 280674, \"max\": 280674}, \"Total Batches Seen\": {\"sum\": 2232.0, \"count\": 1, \"min\": 2232, \"max\": 2232}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 124.0, \"count\": 1, \"min\": 124, \"max\": 124}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3591.9328195788908 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:25 INFO 140618438534976] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:25.620] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1227, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] # Finished training epoch 61 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] Loss (name: value) total: 6.58590841293335\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] Loss (name: value) kld: 0.16589682042184803\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] Loss (name: value) recons: 6.420011599858602\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] Loss (name: value) logppx: 6.58590841293335\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] #quality_metric: host=algo-1, epoch=61, train total_loss <loss>=6.58590841293335\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] patience losses:[6.593268838193682, 6.597070535024007, 6.595549649662441, 6.590862459606594, 6.589325626691182] min patience loss:6.589325626691182 current loss:6.58590841293335 absolute loss difference:0.003417213757832549\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650664.3911328, \"EndTime\": 1623650665.6258183, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 60, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 275964.0, \"count\": 1, \"min\": 275964, \"max\": 275964}, \"Total Batches Seen\": {\"sum\": 2196.0, \"count\": 1, \"min\": 2196, \"max\": 2196}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 122.0, \"count\": 1, \"min\": 122, \"max\": 122}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3663.6016602894038 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:25 INFO 140515461125952] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:26.962] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1199, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] # Finished training epoch 63 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] Loss (name: value) total: 6.563349015182919\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] Loss (name: value) kld: 0.1702202864819103\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] Loss (name: value) recons: 6.393128706349267\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] Loss (name: value) logppx: 6.563349015182919\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] #quality_metric: host=algo-2, epoch=63, train total_loss <loss>=6.563349015182919\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] patience losses:[6.572844333118862, 6.572991629441579, 6.574332760439979, 6.565851582421197, 6.566710154215495] min patience loss:6.565851582421197 current loss:6.563349015182919 absolute loss difference:0.0025025672382783526\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] #progress_metric: host=algo-2, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650665.7622468, \"EndTime\": 1623650666.9682739, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 62, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 285201.0, \"count\": 1, \"min\": 285201, \"max\": 285201}, \"Total Batches Seen\": {\"sum\": 2268.0, \"count\": 1, \"min\": 2268, \"max\": 2268}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 126.0, \"count\": 1, \"min\": 126, \"max\": 126}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3753.1471311797827 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:26 INFO 140618438534976] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:26.866] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1240, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] # Finished training epoch 62 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] Loss (name: value) total: 6.5866846442222595\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] Loss (name: value) kld: 0.16854796237829658\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] Loss (name: value) recons: 6.418136629793379\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] Loss (name: value) logppx: 6.5866846442222595\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] #quality_metric: host=algo-1, epoch=62, train total_loss <loss>=6.5866846442222595\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] patience losses:[6.597070535024007, 6.595549649662441, 6.590862459606594, 6.589325626691182, 6.58590841293335] min patience loss:6.58590841293335 current loss:6.5866846442222595 absolute loss difference:0.0007762312889099121\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650665.6261158, \"EndTime\": 1623650666.868314, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 61, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 280488.0, \"count\": 1, \"min\": 280488, \"max\": 280488}, \"Total Batches Seen\": {\"sum\": 2232.0, \"count\": 1, \"min\": 2232, \"max\": 2232}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 124.0, \"count\": 1, \"min\": 124, \"max\": 124}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3641.518441789632 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:26 INFO 140515461125952] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:28.201] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1232, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] # Finished training epoch 64 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] Loss (name: value) total: 6.562696139017741\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] Loss (name: value) kld: 0.17299951799213886\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] Loss (name: value) recons: 6.389696611298455\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] Loss (name: value) logppx: 6.562696139017741\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] #quality_metric: host=algo-2, epoch=64, train total_loss <loss>=6.562696139017741\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] patience losses:[6.572991629441579, 6.574332760439979, 6.565851582421197, 6.566710154215495, 6.563349015182919] min patience loss:6.563349015182919 current loss:6.562696139017741 absolute loss difference:0.0006528761651773962\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] Timing: train: 1.23s, val: 0.01s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] #progress_metric: host=algo-2, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650666.9685977, \"EndTime\": 1623650668.2079937, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 63, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 289728.0, \"count\": 1, \"min\": 289728, \"max\": 289728}, \"Total Batches Seen\": {\"sum\": 2304.0, \"count\": 1, \"min\": 2304, \"max\": 2304}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 128.0, \"count\": 1, \"min\": 128, \"max\": 128}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3652.0998766131415 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:28 INFO 140618438534976] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:28.111] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1238, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] # Finished training epoch 63 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] Loss (name: value) total: 6.577978293100993\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] Loss (name: value) kld: 0.1684371523766054\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] Loss (name: value) recons: 6.409541130065918\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] Loss (name: value) logppx: 6.577978293100993\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] #quality_metric: host=algo-1, epoch=63, train total_loss <loss>=6.577978293100993\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] patience losses:[6.595549649662441, 6.590862459606594, 6.589325626691182, 6.58590841293335, 6.5866846442222595] min patience loss:6.58590841293335 current loss:6.577978293100993 absolute loss difference:0.007930119832356475\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650666.8685744, \"EndTime\": 1623650668.1171975, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 62, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 285012.0, \"count\": 1, \"min\": 285012, \"max\": 285012}, \"Total Batches Seen\": {\"sum\": 2268.0, \"count\": 1, \"min\": 2268, \"max\": 2268}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 126.0, \"count\": 1, \"min\": 126, \"max\": 126}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3622.736446596848 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:28 INFO 140515461125952] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:29.429] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] # Finished training epoch 65 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] Loss (name: value) total: 6.559272329012553\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] Loss (name: value) kld: 0.17533455023335087\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] Loss (name: value) recons: 6.383937776088715\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] Loss (name: value) logppx: 6.559272329012553\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] #quality_metric: host=algo-2, epoch=65, train total_loss <loss>=6.559272329012553\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] patience losses:[6.574332760439979, 6.565851582421197, 6.566710154215495, 6.563349015182919, 6.562696139017741] min patience loss:6.562696139017741 current loss:6.559272329012553 absolute loss difference:0.0034238100051879883\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] #progress_metric: host=algo-2, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650668.2083454, \"EndTime\": 1623650669.433699, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 64, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 294255.0, \"count\": 1, \"min\": 294255, \"max\": 294255}, \"Total Batches Seen\": {\"sum\": 2340.0, \"count\": 1, \"min\": 2340, \"max\": 2340}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 130.0, \"count\": 1, \"min\": 130, \"max\": 130}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3693.9431967848736 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:29 INFO 140618438534976] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:29.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1217, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] # Finished training epoch 64 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] Loss (name: value) total: 6.575211584568024\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] Loss (name: value) kld: 0.17038930704196295\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] Loss (name: value) recons: 6.404822289943695\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] Loss (name: value) logppx: 6.575211584568024\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] #quality_metric: host=algo-1, epoch=64, train total_loss <loss>=6.575211584568024\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] patience losses:[6.590862459606594, 6.589325626691182, 6.58590841293335, 6.5866846442222595, 6.577978293100993] min patience loss:6.577978293100993 current loss:6.575211584568024 absolute loss difference:0.002766708532969453\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650668.1175199, \"EndTime\": 1623650669.3427835, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 63, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 289536.0, \"count\": 1, \"min\": 289536, \"max\": 289536}, \"Total Batches Seen\": {\"sum\": 2304.0, \"count\": 1, \"min\": 2304, \"max\": 2304}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 128.0, \"count\": 1, \"min\": 128, \"max\": 128}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3691.1153872863606 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:29 INFO 140515461125952] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:30.658] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1224, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] # Finished training epoch 66 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] Loss (name: value) total: 6.553406218687694\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] Loss (name: value) kld: 0.17681061497165096\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] Loss (name: value) recons: 6.376595589849684\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] Loss (name: value) logppx: 6.553406218687694\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] #quality_metric: host=algo-2, epoch=66, train total_loss <loss>=6.553406218687694\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] patience losses:[6.565851582421197, 6.566710154215495, 6.563349015182919, 6.562696139017741, 6.559272329012553] min patience loss:6.559272329012553 current loss:6.553406218687694 absolute loss difference:0.005866110324859619\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] #progress_metric: host=algo-2, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650669.4340243, \"EndTime\": 1623650670.6635745, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 65, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 298782.0, \"count\": 1, \"min\": 298782, \"max\": 298782}, \"Total Batches Seen\": {\"sum\": 2376.0, \"count\": 1, \"min\": 2376, \"max\": 2376}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 132.0, \"count\": 1, \"min\": 132, \"max\": 132}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3681.3804425695503 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:30 INFO 140618438534976] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:31.858] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1194, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] # Finished training epoch 67 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] Loss (name: value) total: 6.547946049107446\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] Loss (name: value) kld: 0.177737292730146\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] Loss (name: value) recons: 6.37020879983902\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] Loss (name: value) logppx: 6.547946049107446\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] #quality_metric: host=algo-2, epoch=67, train total_loss <loss>=6.547946049107446\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] patience losses:[6.566710154215495, 6.563349015182919, 6.562696139017741, 6.559272329012553, 6.553406218687694] min patience loss:6.553406218687694 current loss:6.547946049107446 absolute loss difference:0.0054601695802478645\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] #progress_metric: host=algo-2, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650670.6638718, \"EndTime\": 1623650671.8630078, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 66, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 303309.0, \"count\": 1, \"min\": 303309, \"max\": 303309}, \"Total Batches Seen\": {\"sum\": 2412.0, \"count\": 1, \"min\": 2412, \"max\": 2412}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 134.0, \"count\": 1, \"min\": 134, \"max\": 134}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3774.688992943884 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:31 INFO 140618438534976] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:30.589] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1245, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] # Finished training epoch 65 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] Loss (name: value) total: 6.571432318952349\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] Loss (name: value) kld: 0.17127922363579273\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] Loss (name: value) recons: 6.400153040885925\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] Loss (name: value) logppx: 6.571432318952349\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] #quality_metric: host=algo-1, epoch=65, train total_loss <loss>=6.571432318952349\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] patience losses:[6.589325626691182, 6.58590841293335, 6.5866846442222595, 6.577978293100993, 6.575211584568024] min patience loss:6.575211584568024 current loss:6.571432318952349 absolute loss difference:0.003779265615674987\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650669.3435698, \"EndTime\": 1623650670.595496, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 64, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 294060.0, \"count\": 1, \"min\": 294060, \"max\": 294060}, \"Total Batches Seen\": {\"sum\": 2340.0, \"count\": 1, \"min\": 2340, \"max\": 2340}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 130.0, \"count\": 1, \"min\": 130, \"max\": 130}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3612.034325976471 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:30 INFO 140515461125952] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:31.870] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1272, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] # Finished training epoch 66 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] Loss (name: value) total: 6.566494405269623\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] Loss (name: value) kld: 0.1748166721728113\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] Loss (name: value) recons: 6.391677704122332\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] Loss (name: value) logppx: 6.566494405269623\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] #quality_metric: host=algo-1, epoch=66, train total_loss <loss>=6.566494405269623\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] patience losses:[6.58590841293335, 6.5866846442222595, 6.577978293100993, 6.575211584568024, 6.571432318952349] min patience loss:6.571432318952349 current loss:6.566494405269623 absolute loss difference:0.004937913682725892\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] Timing: train: 1.27s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650670.5962825, \"EndTime\": 1623650671.8757908, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 65, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 298584.0, \"count\": 1, \"min\": 298584, \"max\": 298584}, \"Total Batches Seen\": {\"sum\": 2376.0, \"count\": 1, \"min\": 2376, \"max\": 2376}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 132.0, \"count\": 1, \"min\": 132, \"max\": 132}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3535.059873475805 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:31 INFO 140515461125952] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:33.027] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1163, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] # Finished training epoch 68 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] Loss (name: value) total: 6.543681111600664\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] Loss (name: value) kld: 0.1796811047113604\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] Loss (name: value) recons: 6.363999995920393\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] Loss (name: value) logppx: 6.543681111600664\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] #quality_metric: host=algo-2, epoch=68, train total_loss <loss>=6.543681111600664\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] patience losses:[6.563349015182919, 6.562696139017741, 6.559272329012553, 6.553406218687694, 6.547946049107446] min patience loss:6.547946049107446 current loss:6.543681111600664 absolute loss difference:0.004264937506781585\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] #progress_metric: host=algo-2, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650671.8633313, \"EndTime\": 1623650673.0320287, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 67, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 307836.0, \"count\": 1, \"min\": 307836, \"max\": 307836}, \"Total Batches Seen\": {\"sum\": 2448.0, \"count\": 1, \"min\": 2448, \"max\": 2448}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 136.0, \"count\": 1, \"min\": 136, \"max\": 136}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3873.1602386362315 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:33 INFO 140618438534976] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:33.095] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1218, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] # Finished training epoch 67 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] Loss (name: value) total: 6.570747812589009\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] Loss (name: value) kld: 0.17773579702609116\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] Loss (name: value) recons: 6.393012033568488\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] Loss (name: value) logppx: 6.570747812589009\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] #quality_metric: host=algo-1, epoch=67, train total_loss <loss>=6.570747812589009\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] patience losses:[6.5866846442222595, 6.577978293100993, 6.575211584568024, 6.571432318952349, 6.566494405269623] min patience loss:6.566494405269623 current loss:6.570747812589009 absolute loss difference:0.004253407319386504\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650671.8760915, \"EndTime\": 1623650673.0964754, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 66, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 303108.0, \"count\": 1, \"min\": 303108, \"max\": 303108}, \"Total Batches Seen\": {\"sum\": 2412.0, \"count\": 1, \"min\": 2412, \"max\": 2412}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 134.0, \"count\": 1, \"min\": 134, \"max\": 134}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3706.647203170814 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:33 INFO 140515461125952] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:34.208] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1175, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] # Finished training epoch 69 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] Loss (name: value) total: 6.546471887164646\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] Loss (name: value) kld: 0.18124332030614218\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] Loss (name: value) recons: 6.365228599972195\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] Loss (name: value) logppx: 6.546471887164646\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] #quality_metric: host=algo-2, epoch=69, train total_loss <loss>=6.546471887164646\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] patience losses:[6.562696139017741, 6.559272329012553, 6.553406218687694, 6.547946049107446, 6.543681111600664] min patience loss:6.543681111600664 current loss:6.546471887164646 absolute loss difference:0.0027907755639819953\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] #progress_metric: host=algo-2, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650673.0322957, \"EndTime\": 1623650674.2113621, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 68, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 312363.0, \"count\": 1, \"min\": 312363, \"max\": 312363}, \"Total Batches Seen\": {\"sum\": 2484.0, \"count\": 1, \"min\": 2484, \"max\": 2484}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 138.0, \"count\": 1, \"min\": 138, \"max\": 138}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3838.8775337197494 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:34 INFO 140618438534976] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:34.285] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1186, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] # Finished training epoch 68 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] Loss (name: value) total: 6.559966868824429\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] Loss (name: value) kld: 0.17820701065162817\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] Loss (name: value) recons: 6.38175986872779\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] Loss (name: value) logppx: 6.559966868824429\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] #quality_metric: host=algo-1, epoch=68, train total_loss <loss>=6.559966868824429\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] patience losses:[6.577978293100993, 6.575211584568024, 6.571432318952349, 6.566494405269623, 6.570747812589009] min patience loss:6.566494405269623 current loss:6.559966868824429 absolute loss difference:0.006527536445194215\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650673.0967143, \"EndTime\": 1623650674.290983, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 67, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 307632.0, \"count\": 1, \"min\": 307632, \"max\": 307632}, \"Total Batches Seen\": {\"sum\": 2448.0, \"count\": 1, \"min\": 2448, \"max\": 2448}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 136.0, \"count\": 1, \"min\": 136, \"max\": 136}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3787.632496256288 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:34 INFO 140515461125952] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:35.400] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1188, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] # Finished training epoch 70 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] Loss (name: value) total: 6.5385657217767506\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] Loss (name: value) kld: 0.18518578964802954\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] Loss (name: value) recons: 6.353379911846584\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] Loss (name: value) logppx: 6.5385657217767506\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] #quality_metric: host=algo-2, epoch=70, train total_loss <loss>=6.5385657217767506\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] patience losses:[6.559272329012553, 6.553406218687694, 6.547946049107446, 6.543681111600664, 6.546471887164646] min patience loss:6.543681111600664 current loss:6.5385657217767506 absolute loss difference:0.005115389823913574\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] #progress_metric: host=algo-2, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650674.2117007, \"EndTime\": 1623650675.4072044, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 69, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 316890.0, \"count\": 1, \"min\": 316890, \"max\": 316890}, \"Total Batches Seen\": {\"sum\": 2520.0, \"count\": 1, \"min\": 2520, \"max\": 2520}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3786.146229409869 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:35 INFO 140618438534976] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:35.528] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1236, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] # Finished training epoch 69 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] Loss (name: value) total: 6.568203230698903\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] Loss (name: value) kld: 0.18273344677355555\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] Loss (name: value) recons: 6.385469774405162\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] Loss (name: value) logppx: 6.568203230698903\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] #quality_metric: host=algo-1, epoch=69, train total_loss <loss>=6.568203230698903\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] patience losses:[6.575211584568024, 6.571432318952349, 6.566494405269623, 6.570747812589009, 6.559966868824429] min patience loss:6.559966868824429 current loss:6.568203230698903 absolute loss difference:0.008236361874474518\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650674.2912872, \"EndTime\": 1623650675.530704, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 68, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 312156.0, \"count\": 1, \"min\": 312156, \"max\": 312156}, \"Total Batches Seen\": {\"sum\": 2484.0, \"count\": 1, \"min\": 2484, \"max\": 2484}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 138.0, \"count\": 1, \"min\": 138, \"max\": 138}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3649.6389500366695 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:35 INFO 140515461125952] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:36.622] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1214, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] # Finished training epoch 71 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] Loss (name: value) total: 6.53673642873764\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] Loss (name: value) kld: 0.18473941439555752\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] Loss (name: value) recons: 6.351997004614936\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] Loss (name: value) logppx: 6.53673642873764\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] #quality_metric: host=algo-2, epoch=71, train total_loss <loss>=6.53673642873764\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] patience losses:[6.553406218687694, 6.547946049107446, 6.543681111600664, 6.546471887164646, 6.5385657217767506] min patience loss:6.5385657217767506 current loss:6.53673642873764 absolute loss difference:0.0018292930391101692\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] #progress_metric: host=algo-2, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650675.4075928, \"EndTime\": 1623650676.6292627, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 70, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 321417.0, \"count\": 1, \"min\": 321417, \"max\": 321417}, \"Total Batches Seen\": {\"sum\": 2556.0, \"count\": 1, \"min\": 2556, \"max\": 2556}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 142.0, \"count\": 1, \"min\": 142, \"max\": 142}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3705.0687783464105 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:36 INFO 140618438534976] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:36.782] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1251, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] # Finished training epoch 70 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] Loss (name: value) total: 6.554595563146803\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] Loss (name: value) kld: 0.18293477470676103\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] Loss (name: value) recons: 6.37166076236301\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] Loss (name: value) logppx: 6.554595563146803\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] #quality_metric: host=algo-1, epoch=70, train total_loss <loss>=6.554595563146803\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] patience losses:[6.571432318952349, 6.566494405269623, 6.570747812589009, 6.559966868824429, 6.568203230698903] min patience loss:6.559966868824429 current loss:6.554595563146803 absolute loss difference:0.005371305677625671\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650675.5310018, \"EndTime\": 1623650676.7879992, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 69, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 316680.0, \"count\": 1, \"min\": 316680, \"max\": 316680}, \"Total Batches Seen\": {\"sum\": 2520.0, \"count\": 1, \"min\": 2520, \"max\": 2520}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3598.6645602581398 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:36 INFO 140515461125952] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:37.896] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1266, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] # Finished training epoch 72 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] Loss (name: value) total: 6.536389576064216\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] Loss (name: value) kld: 0.1889510384450356\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] Loss (name: value) recons: 6.347438520855373\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] Loss (name: value) logppx: 6.536389576064216\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] #quality_metric: host=algo-2, epoch=72, train total_loss <loss>=6.536389576064216\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] patience losses:[6.547946049107446, 6.543681111600664, 6.546471887164646, 6.5385657217767506, 6.53673642873764] min patience loss:6.53673642873764 current loss:6.536389576064216 absolute loss difference:0.0003468526734247135\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] #progress_metric: host=algo-2, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650676.629571, \"EndTime\": 1623650677.901636, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 71, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 325944.0, \"count\": 1, \"min\": 325944, \"max\": 325944}, \"Total Batches Seen\": {\"sum\": 2592.0, \"count\": 1, \"min\": 2592, \"max\": 2592}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3557.9437200867505 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:37 INFO 140618438534976] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:38.067] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1279, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] # Finished training epoch 71 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] Loss (name: value) total: 6.558237029446496\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] Loss (name: value) kld: 0.18422058183285925\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] Loss (name: value) recons: 6.374016463756561\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] Loss (name: value) logppx: 6.558237029446496\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] #quality_metric: host=algo-1, epoch=71, train total_loss <loss>=6.558237029446496\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] patience losses:[6.566494405269623, 6.570747812589009, 6.559966868824429, 6.568203230698903, 6.554595563146803] min patience loss:6.554595563146803 current loss:6.558237029446496 absolute loss difference:0.0036414662996930858\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650676.788291, \"EndTime\": 1623650678.0697813, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 70, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 321204.0, \"count\": 1, \"min\": 321204, \"max\": 321204}, \"Total Batches Seen\": {\"sum\": 2556.0, \"count\": 1, \"min\": 2556, \"max\": 2556}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 142.0, \"count\": 1, \"min\": 142, \"max\": 142}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3529.85814104862 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:38 INFO 140515461125952] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:39.106] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] # Finished training epoch 73 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] Loss (name: value) total: 6.530302147070567\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] Loss (name: value) kld: 0.190671159989304\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] Loss (name: value) recons: 6.339630987909105\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] Loss (name: value) logppx: 6.530302147070567\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] #quality_metric: host=algo-2, epoch=73, train total_loss <loss>=6.530302147070567\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] patience losses:[6.543681111600664, 6.546471887164646, 6.5385657217767506, 6.53673642873764, 6.536389576064216] min patience loss:6.536389576064216 current loss:6.530302147070567 absolute loss difference:0.006087428993648558\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] #progress_metric: host=algo-2, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650677.9021835, \"EndTime\": 1623650679.1113284, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 72, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 330471.0, \"count\": 1, \"min\": 330471, \"max\": 330471}, \"Total Batches Seen\": {\"sum\": 2628.0, \"count\": 1, \"min\": 2628, \"max\": 2628}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 146.0, \"count\": 1, \"min\": 146, \"max\": 146}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3743.398539477116 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:39 INFO 140618438534976] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:39.367] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1297, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] # Finished training epoch 72 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] Loss (name: value) total: 6.552551905314128\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] Loss (name: value) kld: 0.18881154070711798\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] Loss (name: value) recons: 6.363740377955967\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] Loss (name: value) logppx: 6.552551905314128\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] #quality_metric: host=algo-1, epoch=72, train total_loss <loss>=6.552551905314128\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] patience losses:[6.570747812589009, 6.559966868824429, 6.568203230698903, 6.554595563146803, 6.558237029446496] min patience loss:6.554595563146803 current loss:6.552551905314128 absolute loss difference:0.0020436578326750166\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] Timing: train: 1.30s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650678.0700388, \"EndTime\": 1623650679.3742716, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 71, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 325728.0, \"count\": 1, \"min\": 325728, \"max\": 325728}, \"Total Batches Seen\": {\"sum\": 2592.0, \"count\": 1, \"min\": 2592, \"max\": 2592}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 144.0, \"count\": 1, \"min\": 144, \"max\": 144}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3468.152958178805 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:39 INFO 140515461125952] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:40.387] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1275, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] # Finished training epoch 74 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] Loss (name: value) total: 6.5268412828445435\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] Loss (name: value) kld: 0.19217892115314802\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] Loss (name: value) recons: 6.334662384457058\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] Loss (name: value) logppx: 6.5268412828445435\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] #quality_metric: host=algo-2, epoch=74, train total_loss <loss>=6.5268412828445435\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] patience losses:[6.546471887164646, 6.5385657217767506, 6.53673642873764, 6.536389576064216, 6.530302147070567] min patience loss:6.530302147070567 current loss:6.5268412828445435 absolute loss difference:0.003460864226023652\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] #progress_metric: host=algo-2, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650679.1116636, \"EndTime\": 1623650680.393772, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 73, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 334998.0, \"count\": 1, \"min\": 334998, \"max\": 334998}, \"Total Batches Seen\": {\"sum\": 2664.0, \"count\": 1, \"min\": 2664, \"max\": 2664}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 148.0, \"count\": 1, \"min\": 148, \"max\": 148}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3530.396781301532 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:40 INFO 140618438534976] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:40.621] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1246, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] # Finished training epoch 73 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] Loss (name: value) total: 6.554521494441563\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] Loss (name: value) kld: 0.19079914947764742\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] Loss (name: value) recons: 6.363722311125861\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] Loss (name: value) logppx: 6.554521494441563\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] #quality_metric: host=algo-1, epoch=73, train total_loss <loss>=6.554521494441563\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] patience losses:[6.559966868824429, 6.568203230698903, 6.554595563146803, 6.558237029446496, 6.552551905314128] min patience loss:6.552551905314128 current loss:6.554521494441563 absolute loss difference:0.0019695891274347233\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650679.374645, \"EndTime\": 1623650680.6229744, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 72, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 330252.0, \"count\": 1, \"min\": 330252, \"max\": 330252}, \"Total Batches Seen\": {\"sum\": 2628.0, \"count\": 1, \"min\": 2628, \"max\": 2628}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 146.0, \"count\": 1, \"min\": 146, \"max\": 146}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3623.489818058145 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:40 INFO 140515461125952] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:41.627] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1232, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] # Finished training epoch 75 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] Loss (name: value) total: 6.529513226615058\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] Loss (name: value) kld: 0.1944994367659092\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] Loss (name: value) recons: 6.335013760460748\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] Loss (name: value) logppx: 6.529513226615058\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] #quality_metric: host=algo-2, epoch=75, train total_loss <loss>=6.529513226615058\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] patience losses:[6.5385657217767506, 6.53673642873764, 6.536389576064216, 6.530302147070567, 6.5268412828445435] min patience loss:6.5268412828445435 current loss:6.529513226615058 absolute loss difference:0.0026719437705144955\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] #progress_metric: host=algo-2, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650680.3941193, \"EndTime\": 1623650681.6289148, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 74, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 339525.0, \"count\": 1, \"min\": 339525, \"max\": 339525}, \"Total Batches Seen\": {\"sum\": 2700.0, \"count\": 1, \"min\": 2700, \"max\": 2700}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 150.0, \"count\": 1, \"min\": 150, \"max\": 150}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3665.7126137208706 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:41 INFO 140618438534976] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:41.850] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1226, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] # Finished training epoch 74 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] Loss (name: value) total: 6.550229258007473\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] Loss (name: value) kld: 0.19164821609026855\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] Loss (name: value) recons: 6.35858107275433\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] Loss (name: value) logppx: 6.550229258007473\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] #quality_metric: host=algo-1, epoch=74, train total_loss <loss>=6.550229258007473\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] patience losses:[6.568203230698903, 6.554595563146803, 6.558237029446496, 6.552551905314128, 6.554521494441563] min patience loss:6.552551905314128 current loss:6.550229258007473 absolute loss difference:0.002322647306654879\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650680.6233246, \"EndTime\": 1623650681.8552284, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 73, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 334776.0, \"count\": 1, \"min\": 334776, \"max\": 334776}, \"Total Batches Seen\": {\"sum\": 2664.0, \"count\": 1, \"min\": 2664, \"max\": 2664}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 148.0, \"count\": 1, \"min\": 148, \"max\": 148}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3671.9390197883804 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:41 INFO 140515461125952] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:42.770] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1140, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] # Finished training epoch 76 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] Loss (name: value) total: 6.526727974414825\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] Loss (name: value) kld: 0.1948181808822685\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] Loss (name: value) recons: 6.331909782356686\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] Loss (name: value) logppx: 6.526727974414825\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] #quality_metric: host=algo-2, epoch=76, train total_loss <loss>=6.526727974414825\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] patience losses:[6.53673642873764, 6.536389576064216, 6.530302147070567, 6.5268412828445435, 6.529513226615058] min patience loss:6.5268412828445435 current loss:6.526727974414825 absolute loss difference:0.00011330842971801758\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] Timing: train: 1.14s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] #progress_metric: host=algo-2, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650681.6291528, \"EndTime\": 1623650682.7747774, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 75, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 344052.0, \"count\": 1, \"min\": 344052, \"max\": 344052}, \"Total Batches Seen\": {\"sum\": 2736.0, \"count\": 1, \"min\": 2736, \"max\": 2736}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 152.0, \"count\": 1, \"min\": 152, \"max\": 152}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3951.0849177981595 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:42 INFO 140618438534976] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:43.268] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1411, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] # Finished training epoch 75 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] Loss (name: value) total: 6.544004579385121\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] Loss (name: value) kld: 0.1939311952640613\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] Loss (name: value) recons: 6.350073436896007\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] Loss (name: value) logppx: 6.544004579385121\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] #quality_metric: host=algo-1, epoch=75, train total_loss <loss>=6.544004579385121\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] patience losses:[6.554595563146803, 6.558237029446496, 6.552551905314128, 6.554521494441563, 6.550229258007473] min patience loss:6.550229258007473 current loss:6.544004579385121 absolute loss difference:0.006224678622351654\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] Timing: train: 1.41s, val: 0.00s, epoch: 1.42s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650681.8555286, \"EndTime\": 1623650683.274386, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 74, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 339300.0, \"count\": 1, \"min\": 339300, \"max\": 339300}, \"Total Batches Seen\": {\"sum\": 2700.0, \"count\": 1, \"min\": 2700, \"max\": 2700}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 150.0, \"count\": 1, \"min\": 150, \"max\": 150}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3188.1174892024223 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:43 INFO 140515461125952] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:44.537] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1262, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] # Finished training epoch 76 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] Loss (name: value) total: 6.5482259260283575\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] Loss (name: value) kld: 0.19668034619341293\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] Loss (name: value) recons: 6.351545612017314\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] Loss (name: value) logppx: 6.5482259260283575\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] #quality_metric: host=algo-1, epoch=76, train total_loss <loss>=6.5482259260283575\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] patience losses:[6.558237029446496, 6.552551905314128, 6.554521494441563, 6.550229258007473, 6.544004579385121] min patience loss:6.544004579385121 current loss:6.5482259260283575 absolute loss difference:0.004221346643236146\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650683.2747078, \"EndTime\": 1623650684.539446, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 75, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 343824.0, \"count\": 1, \"min\": 343824, \"max\": 343824}, \"Total Batches Seen\": {\"sum\": 2736.0, \"count\": 1, \"min\": 2736, \"max\": 2736}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 152.0, \"count\": 1, \"min\": 152, \"max\": 152}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3576.5411136623834 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:44 INFO 140515461125952] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:44.053] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1278, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] # Finished training epoch 77 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] Loss (name: value) total: 6.524065719710456\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] Loss (name: value) kld: 0.19622944564455086\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] Loss (name: value) recons: 6.327836281723446\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] Loss (name: value) logppx: 6.524065719710456\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] #quality_metric: host=algo-2, epoch=77, train total_loss <loss>=6.524065719710456\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] patience losses:[6.536389576064216, 6.530302147070567, 6.5268412828445435, 6.529513226615058, 6.526727974414825] min patience loss:6.526727974414825 current loss:6.524065719710456 absolute loss difference:0.0026622547043695377\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] #progress_metric: host=algo-2, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650682.77508, \"EndTime\": 1623650684.058738, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 76, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 348579.0, \"count\": 1, \"min\": 348579, \"max\": 348579}, \"Total Batches Seen\": {\"sum\": 2772.0, \"count\": 1, \"min\": 2772, \"max\": 2772}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 154.0, \"count\": 1, \"min\": 154, \"max\": 154}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3526.202046122724 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:44 INFO 140618438534976] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:45.373] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1314, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] # Finished training epoch 78 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] Loss (name: value) total: 6.5237767696380615\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] Loss (name: value) kld: 0.20002552515102756\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] Loss (name: value) recons: 6.323751191298167\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] Loss (name: value) logppx: 6.5237767696380615\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] #quality_metric: host=algo-2, epoch=78, train total_loss <loss>=6.5237767696380615\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] patience losses:[6.530302147070567, 6.5268412828445435, 6.529513226615058, 6.526727974414825, 6.524065719710456] min patience loss:6.524065719710456 current loss:6.5237767696380615 absolute loss difference:0.0002889500723943783\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] Timing: train: 1.32s, val: 0.00s, epoch: 1.32s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] #progress_metric: host=algo-2, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650684.0590584, \"EndTime\": 1623650685.3780005, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 77, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 353106.0, \"count\": 1, \"min\": 353106, \"max\": 353106}, \"Total Batches Seen\": {\"sum\": 2808.0, \"count\": 1, \"min\": 2808, \"max\": 2808}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 156.0, \"count\": 1, \"min\": 156, \"max\": 156}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3431.849652000338 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:45 INFO 140618438534976] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:45.721] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1180, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] # Finished training epoch 77 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] Loss (name: value) total: 6.550962812370724\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] Loss (name: value) kld: 0.20064379709462324\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] Loss (name: value) recons: 6.350318968296051\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] Loss (name: value) logppx: 6.550962812370724\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] #quality_metric: host=algo-1, epoch=77, train total_loss <loss>=6.550962812370724\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] patience losses:[6.552551905314128, 6.554521494441563, 6.550229258007473, 6.544004579385121, 6.5482259260283575] min patience loss:6.544004579385121 current loss:6.550962812370724 absolute loss difference:0.006958232985602386\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650684.539774, \"EndTime\": 1623650685.722413, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 76, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 348348.0, \"count\": 1, \"min\": 348348, \"max\": 348348}, \"Total Batches Seen\": {\"sum\": 2772.0, \"count\": 1, \"min\": 2772, \"max\": 2772}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 154.0, \"count\": 1, \"min\": 154, \"max\": 154}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3824.7952948746015 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:45 INFO 140515461125952] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:46.590] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] # Finished training epoch 79 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] Loss (name: value) total: 6.5179943508572045\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] Loss (name: value) kld: 0.20202842872175905\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] Loss (name: value) recons: 6.315965970357259\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] Loss (name: value) logppx: 6.5179943508572045\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] #quality_metric: host=algo-2, epoch=79, train total_loss <loss>=6.5179943508572045\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] patience losses:[6.5268412828445435, 6.529513226615058, 6.526727974414825, 6.524065719710456, 6.5237767696380615] min patience loss:6.5237767696380615 current loss:6.5179943508572045 absolute loss difference:0.005782418780857057\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] #progress_metric: host=algo-2, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650685.3783329, \"EndTime\": 1623650686.5946085, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 78, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 357633.0, \"count\": 1, \"min\": 357633, \"max\": 357633}, \"Total Batches Seen\": {\"sum\": 2844.0, \"count\": 1, \"min\": 2844, \"max\": 2844}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 158.0, \"count\": 1, \"min\": 158, \"max\": 158}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3721.5401562302472 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:46 INFO 140618438534976] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:47.012] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1289, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] # Finished training epoch 78 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] Loss (name: value) total: 6.538141210873921\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] Loss (name: value) kld: 0.19995695042113462\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] Loss (name: value) recons: 6.338184217611949\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] Loss (name: value) logppx: 6.538141210873921\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] #quality_metric: host=algo-1, epoch=78, train total_loss <loss>=6.538141210873921\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] patience losses:[6.554521494441563, 6.550229258007473, 6.544004579385121, 6.5482259260283575, 6.550962812370724] min patience loss:6.544004579385121 current loss:6.538141210873921 absolute loss difference:0.005863368511199951\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] Timing: train: 1.29s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650685.7227395, \"EndTime\": 1623650687.0201, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 77, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 352872.0, \"count\": 1, \"min\": 352872, \"max\": 352872}, \"Total Batches Seen\": {\"sum\": 2808.0, \"count\": 1, \"min\": 2808, \"max\": 2808}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 156.0, \"count\": 1, \"min\": 156, \"max\": 156}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3486.0790018605253 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:47 INFO 140515461125952] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:47.865] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1270, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] # Finished training epoch 80 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] Loss (name: value) total: 6.520845830440521\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] Loss (name: value) kld: 0.20417847422262034\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] Loss (name: value) recons: 6.316667331589593\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] Loss (name: value) logppx: 6.520845830440521\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] #quality_metric: host=algo-2, epoch=80, train total_loss <loss>=6.520845830440521\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] patience losses:[6.529513226615058, 6.526727974414825, 6.524065719710456, 6.5237767696380615, 6.5179943508572045] min patience loss:6.5179943508572045 current loss:6.520845830440521 absolute loss difference:0.002851479583316774\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] #progress_metric: host=algo-2, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650686.5949225, \"EndTime\": 1623650687.8672976, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 79, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 362160.0, \"count\": 1, \"min\": 362160, \"max\": 362160}, \"Total Batches Seen\": {\"sum\": 2880.0, \"count\": 1, \"min\": 2880, \"max\": 2880}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 160.0, \"count\": 1, \"min\": 160, \"max\": 160}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3557.3631227643077 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:47 INFO 140618438534976] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:48.248] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1227, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] # Finished training epoch 79 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] Loss (name: value) total: 6.534504883819157\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] Loss (name: value) kld: 0.20246679170264137\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] Loss (name: value) recons: 6.3320380912886725\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] Loss (name: value) logppx: 6.534504883819157\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] #quality_metric: host=algo-1, epoch=79, train total_loss <loss>=6.534504883819157\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] patience losses:[6.550229258007473, 6.544004579385121, 6.5482259260283575, 6.550962812370724, 6.538141210873921] min patience loss:6.538141210873921 current loss:6.534504883819157 absolute loss difference:0.0036363270547647986\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] Timing: train: 1.23s, val: 0.01s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] #progress_metric: host=algo-1, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650687.0207415, \"EndTime\": 1623650688.255625, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 78, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 357396.0, \"count\": 1, \"min\": 357396, \"max\": 357396}, \"Total Batches Seen\": {\"sum\": 2844.0, \"count\": 1, \"min\": 2844, \"max\": 2844}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 158.0, \"count\": 1, \"min\": 158, \"max\": 158}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3663.0500108298693 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:48 INFO 140515461125952] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:49.130] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1262, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] # Finished training epoch 81 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] Loss (name: value) total: 6.512177215682136\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] Loss (name: value) kld: 0.20601559864977995\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] Loss (name: value) recons: 6.3061615692244635\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] Loss (name: value) logppx: 6.512177215682136\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] #quality_metric: host=algo-2, epoch=81, train total_loss <loss>=6.512177215682136\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] patience losses:[6.526727974414825, 6.524065719710456, 6.5237767696380615, 6.5179943508572045, 6.520845830440521] min patience loss:6.5179943508572045 current loss:6.512177215682136 absolute loss difference:0.005817135175068877\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] #progress_metric: host=algo-2, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650687.867652, \"EndTime\": 1623650689.1350732, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 80, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 366687.0, \"count\": 1, \"min\": 366687, \"max\": 366687}, \"Total Batches Seen\": {\"sum\": 2916.0, \"count\": 1, \"min\": 2916, \"max\": 2916}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 162.0, \"count\": 1, \"min\": 162, \"max\": 162}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3571.339215090663 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:49 INFO 140618438534976] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:49.502] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1245, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] # Finished training epoch 80 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] Loss (name: value) total: 6.537922693623437\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] Loss (name: value) kld: 0.20400850743883187\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] Loss (name: value) recons: 6.333914187219408\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] Loss (name: value) logppx: 6.537922693623437\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] #quality_metric: host=algo-1, epoch=80, train total_loss <loss>=6.537922693623437\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] patience losses:[6.544004579385121, 6.5482259260283575, 6.550962812370724, 6.538141210873921, 6.534504883819157] min patience loss:6.534504883819157 current loss:6.537922693623437 absolute loss difference:0.003417809804280303\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650688.2558858, \"EndTime\": 1623650689.5042877, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 79, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 361920.0, \"count\": 1, \"min\": 361920, \"max\": 361920}, \"Total Batches Seen\": {\"sum\": 2880.0, \"count\": 1, \"min\": 2880, \"max\": 2880}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 160.0, \"count\": 1, \"min\": 160, \"max\": 160}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3623.233816487663 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:49 INFO 140515461125952] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:50.372] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1235, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] # Finished training epoch 82 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] Loss (name: value) total: 6.514056675963932\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] Loss (name: value) kld: 0.20804982466830146\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] Loss (name: value) recons: 6.3060069349077015\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] Loss (name: value) logppx: 6.514056675963932\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] #quality_metric: host=algo-2, epoch=82, train total_loss <loss>=6.514056675963932\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] patience losses:[6.524065719710456, 6.5237767696380615, 6.5179943508572045, 6.520845830440521, 6.512177215682136] min patience loss:6.512177215682136 current loss:6.514056675963932 absolute loss difference:0.001879460281796419\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] #progress_metric: host=algo-2, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650689.1369998, \"EndTime\": 1623650690.3739607, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 81, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 371214.0, \"count\": 1, \"min\": 371214, \"max\": 371214}, \"Total Batches Seen\": {\"sum\": 2952.0, \"count\": 1, \"min\": 2952, \"max\": 2952}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 164.0, \"count\": 1, \"min\": 164, \"max\": 164}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3654.6163773947337 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:50 INFO 140618438534976] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:50.740] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1235, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] # Finished training epoch 81 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] Loss (name: value) total: 6.529103696346283\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] Loss (name: value) kld: 0.20333953315599096\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] Loss (name: value) recons: 6.325764146116045\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] Loss (name: value) logppx: 6.529103696346283\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] #quality_metric: host=algo-1, epoch=81, train total_loss <loss>=6.529103696346283\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] patience losses:[6.5482259260283575, 6.550962812370724, 6.538141210873921, 6.534504883819157, 6.537922693623437] min patience loss:6.534504883819157 current loss:6.529103696346283 absolute loss difference:0.005401187472873659\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] #progress_metric: host=algo-1, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650689.5046487, \"EndTime\": 1623650690.7467444, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 80, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 366444.0, \"count\": 1, \"min\": 366444, \"max\": 366444}, \"Total Batches Seen\": {\"sum\": 2916.0, \"count\": 1, \"min\": 2916, \"max\": 2916}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 162.0, \"count\": 1, \"min\": 162, \"max\": 162}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3641.752570813978 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:50 INFO 140515461125952] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:51.675] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1301, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] # Finished training epoch 83 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] Loss (name: value) total: 6.510213103559282\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] Loss (name: value) kld: 0.20898727224104935\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] Loss (name: value) recons: 6.301225774817997\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] Loss (name: value) logppx: 6.510213103559282\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] #quality_metric: host=algo-2, epoch=83, train total_loss <loss>=6.510213103559282\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] patience losses:[6.5237767696380615, 6.5179943508572045, 6.520845830440521, 6.512177215682136, 6.514056675963932] min patience loss:6.512177215682136 current loss:6.510213103559282 absolute loss difference:0.001964112122853301\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] Timing: train: 1.30s, val: 0.01s, epoch: 1.31s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] #progress_metric: host=algo-2, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650690.3742132, \"EndTime\": 1623650691.6829102, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 82, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 375741.0, \"count\": 1, \"min\": 375741, \"max\": 375741}, \"Total Batches Seen\": {\"sum\": 2988.0, \"count\": 1, \"min\": 2988, \"max\": 2988}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 166.0, \"count\": 1, \"min\": 166, \"max\": 166}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3458.7468851883277 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:51 INFO 140618438534976] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:51.995] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1248, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] # Finished training epoch 82 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] Loss (name: value) total: 6.53684667746226\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] Loss (name: value) kld: 0.20879626274108887\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] Loss (name: value) recons: 6.328050427966648\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] Loss (name: value) logppx: 6.53684667746226\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] #quality_metric: host=algo-1, epoch=82, train total_loss <loss>=6.53684667746226\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] patience losses:[6.550962812370724, 6.538141210873921, 6.534504883819157, 6.537922693623437, 6.529103696346283] min patience loss:6.529103696346283 current loss:6.53684667746226 absolute loss difference:0.0077429811159772655\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650690.747063, \"EndTime\": 1623650691.9970393, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 81, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 370968.0, \"count\": 1, \"min\": 370968, \"max\": 370968}, \"Total Batches Seen\": {\"sum\": 2952.0, \"count\": 1, \"min\": 2952, \"max\": 2952}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 164.0, \"count\": 1, \"min\": 164, \"max\": 164}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3618.8817961677487 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:51 INFO 140515461125952] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:52.835] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1150, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] # Finished training epoch 84 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] Loss (name: value) total: 6.5073512064086065\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] Loss (name: value) kld: 0.21236728690564632\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] Loss (name: value) recons: 6.294983969794379\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] Loss (name: value) logppx: 6.5073512064086065\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] #quality_metric: host=algo-2, epoch=84, train total_loss <loss>=6.5073512064086065\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] patience losses:[6.5179943508572045, 6.520845830440521, 6.512177215682136, 6.514056675963932, 6.510213103559282] min patience loss:6.510213103559282 current loss:6.5073512064086065 absolute loss difference:0.002861897150675752\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] #progress_metric: host=algo-2, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650691.683239, \"EndTime\": 1623650692.8412764, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 83, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 380268.0, \"count\": 1, \"min\": 380268, \"max\": 380268}, \"Total Batches Seen\": {\"sum\": 3024.0, \"count\": 1, \"min\": 3024, \"max\": 3024}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 168.0, \"count\": 1, \"min\": 168, \"max\": 168}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3908.697132397351 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:52 INFO 140618438534976] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:53.256] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1259, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] # Finished training epoch 83 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] Loss (name: value) total: 6.529122081067827\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] Loss (name: value) kld: 0.2100040116864774\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] Loss (name: value) recons: 6.319118122259776\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] Loss (name: value) logppx: 6.529122081067827\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] #quality_metric: host=algo-1, epoch=83, train total_loss <loss>=6.529122081067827\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] patience losses:[6.538141210873921, 6.534504883819157, 6.537922693623437, 6.529103696346283, 6.53684667746226] min patience loss:6.529103696346283 current loss:6.529122081067827 absolute loss difference:1.8384721544251192e-05\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] #progress_metric: host=algo-1, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650691.9972742, \"EndTime\": 1623650693.2581599, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 82, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 375492.0, \"count\": 1, \"min\": 375492, \"max\": 375492}, \"Total Batches Seen\": {\"sum\": 2988.0, \"count\": 1, \"min\": 2988, \"max\": 2988}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 166.0, \"count\": 1, \"min\": 166, \"max\": 166}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3587.5877224726123 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:53 INFO 140515461125952] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:54.534] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1275, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] # Finished training epoch 84 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] Loss (name: value) total: 6.530893590715197\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] Loss (name: value) kld: 0.21078979834500286\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] Loss (name: value) recons: 6.320103810893165\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] Loss (name: value) logppx: 6.530893590715197\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] #quality_metric: host=algo-1, epoch=84, train total_loss <loss>=6.530893590715197\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] patience losses:[6.534504883819157, 6.537922693623437, 6.529103696346283, 6.53684667746226, 6.529122081067827] min patience loss:6.529103696346283 current loss:6.530893590715197 absolute loss difference:0.001789894368913636\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650693.258416, \"EndTime\": 1623650694.5373156, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 83, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 380016.0, \"count\": 1, \"min\": 380016, \"max\": 380016}, \"Total Batches Seen\": {\"sum\": 3024.0, \"count\": 1, \"min\": 3024, \"max\": 3024}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 168.0, \"count\": 1, \"min\": 168, \"max\": 168}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3537.045300947535 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:54 INFO 140515461125952] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:54.106] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1264, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] # Finished training epoch 85 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] Loss (name: value) total: 6.507407214906481\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] Loss (name: value) kld: 0.2137372533066405\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] Loss (name: value) recons: 6.293669945663876\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] Loss (name: value) logppx: 6.507407214906481\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] #quality_metric: host=algo-2, epoch=85, train total_loss <loss>=6.507407214906481\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] patience losses:[6.520845830440521, 6.512177215682136, 6.514056675963932, 6.510213103559282, 6.5073512064086065] min patience loss:6.5073512064086065 current loss:6.507407214906481 absolute loss difference:5.600849787423812e-05\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] #progress_metric: host=algo-2, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650692.84177, \"EndTime\": 1623650694.1089027, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 84, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 384795.0, \"count\": 1, \"min\": 384795, \"max\": 384795}, \"Total Batches Seen\": {\"sum\": 3060.0, \"count\": 1, \"min\": 3060, \"max\": 3060}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 170.0, \"count\": 1, \"min\": 170, \"max\": 170}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3572.154204007486 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:54 INFO 140618438534976] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:55.353] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1243, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] # Finished training epoch 86 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] Loss (name: value) total: 6.51290946536594\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] Loss (name: value) kld: 0.21774744180341563\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] Loss (name: value) recons: 6.295161955886417\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] Loss (name: value) logppx: 6.51290946536594\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] #quality_metric: host=algo-2, epoch=86, train total_loss <loss>=6.51290946536594\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] patience losses:[6.512177215682136, 6.514056675963932, 6.510213103559282, 6.5073512064086065, 6.507407214906481] min patience loss:6.5073512064086065 current loss:6.51290946536594 absolute loss difference:0.005558258957333528\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] #progress_metric: host=algo-2, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650694.1093595, \"EndTime\": 1623650695.3546734, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 85, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 389322.0, \"count\": 1, \"min\": 389322, \"max\": 389322}, \"Total Batches Seen\": {\"sum\": 3096.0, \"count\": 1, \"min\": 3096, \"max\": 3096}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 172.0, \"count\": 1, \"min\": 172, \"max\": 172}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3634.4202377083166 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:55 INFO 140618438534976] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:55.788] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1249, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] # Finished training epoch 85 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] Loss (name: value) total: 6.529048244158427\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] Loss (name: value) kld: 0.2115371660846803\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] Loss (name: value) recons: 6.317511128054725\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] Loss (name: value) logppx: 6.529048244158427\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] #quality_metric: host=algo-1, epoch=85, train total_loss <loss>=6.529048244158427\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] patience losses:[6.537922693623437, 6.529103696346283, 6.53684667746226, 6.529122081067827, 6.530893590715197] min patience loss:6.529103696346283 current loss:6.529048244158427 absolute loss difference:5.545218785574235e-05\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650694.537645, \"EndTime\": 1623650695.7934859, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 84, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 384540.0, \"count\": 1, \"min\": 384540, \"max\": 384540}, \"Total Batches Seen\": {\"sum\": 3060.0, \"count\": 1, \"min\": 3060, \"max\": 3060}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 170.0, \"count\": 1, \"min\": 170, \"max\": 170}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3601.270168455658 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:55 INFO 140515461125952] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:56.620] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1265, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] # Finished training epoch 87 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] Loss (name: value) total: 6.507667356067234\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] Loss (name: value) kld: 0.2182841052611669\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] Loss (name: value) recons: 6.28938325908449\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] Loss (name: value) logppx: 6.507667356067234\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] #quality_metric: host=algo-2, epoch=87, train total_loss <loss>=6.507667356067234\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] patience losses:[6.514056675963932, 6.510213103559282, 6.5073512064086065, 6.507407214906481, 6.51290946536594] min patience loss:6.5073512064086065 current loss:6.507667356067234 absolute loss difference:0.0003161496586274737\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] #progress_metric: host=algo-2, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650695.354921, \"EndTime\": 1623650696.6225922, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 86, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 393849.0, \"count\": 1, \"min\": 393849, \"max\": 393849}, \"Total Batches Seen\": {\"sum\": 3132.0, \"count\": 1, \"min\": 3132, \"max\": 3132}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 174.0, \"count\": 1, \"min\": 174, \"max\": 174}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3570.7146196729914 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:56 INFO 140618438534976] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:57.062] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1268, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] # Finished training epoch 86 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] Loss (name: value) total: 6.5236866805288525\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] Loss (name: value) kld: 0.21531755880763134\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] Loss (name: value) recons: 6.308369179566701\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] Loss (name: value) logppx: 6.5236866805288525\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] #quality_metric: host=algo-1, epoch=86, train total_loss <loss>=6.5236866805288525\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] patience losses:[6.529103696346283, 6.53684667746226, 6.529122081067827, 6.530893590715197, 6.529048244158427] min patience loss:6.529048244158427 current loss:6.5236866805288525 absolute loss difference:0.005361563629574739\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650695.7941024, \"EndTime\": 1623650697.0686052, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 85, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 389064.0, \"count\": 1, \"min\": 389064, \"max\": 389064}, \"Total Batches Seen\": {\"sum\": 3096.0, \"count\": 1, \"min\": 3096, \"max\": 3096}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 172.0, \"count\": 1, \"min\": 172, \"max\": 172}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3548.9590571436643 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:57 INFO 140515461125952] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:57.819] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1194, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] # Finished training epoch 88 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] Loss (name: value) total: 6.503050095505184\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] Loss (name: value) kld: 0.21907648340695435\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] Loss (name: value) recons: 6.283973554770152\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] Loss (name: value) logppx: 6.503050095505184\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] #quality_metric: host=algo-2, epoch=88, train total_loss <loss>=6.503050095505184\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] patience losses:[6.510213103559282, 6.5073512064086065, 6.507407214906481, 6.51290946536594, 6.507667356067234] min patience loss:6.5073512064086065 current loss:6.503050095505184 absolute loss difference:0.004301110903422334\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] #progress_metric: host=algo-2, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650696.6228557, \"EndTime\": 1623650697.8250253, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 87, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 398376.0, \"count\": 1, \"min\": 398376, \"max\": 398376}, \"Total Batches Seen\": {\"sum\": 3168.0, \"count\": 1, \"min\": 3168, \"max\": 3168}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 176.0, \"count\": 1, \"min\": 176, \"max\": 176}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3765.1948783742782 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:57 INFO 140618438534976] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:04:59.024] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1198, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] # Finished training epoch 89 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] Loss (name: value) total: 6.492812600400713\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] Loss (name: value) kld: 0.2199554613067044\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] Loss (name: value) recons: 6.272857142819299\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] Loss (name: value) logppx: 6.492812600400713\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] #quality_metric: host=algo-2, epoch=89, train total_loss <loss>=6.492812600400713\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] patience losses:[6.5073512064086065, 6.507407214906481, 6.51290946536594, 6.507667356067234, 6.503050095505184] min patience loss:6.503050095505184 current loss:6.492812600400713 absolute loss difference:0.01023749510447125\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] #progress_metric: host=algo-2, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650697.8253438, \"EndTime\": 1623650699.0303175, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 88, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 402903.0, \"count\": 1, \"min\": 402903, \"max\": 402903}, \"Total Batches Seen\": {\"sum\": 3204.0, \"count\": 1, \"min\": 3204, \"max\": 3204}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 178.0, \"count\": 1, \"min\": 178, \"max\": 178}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3756.399284789835 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:04:59 INFO 140618438534976] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:58.325] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1252, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] # Finished training epoch 87 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] Loss (name: value) total: 6.526461660861969\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] Loss (name: value) kld: 0.22040426399972704\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] Loss (name: value) recons: 6.306057386928135\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] Loss (name: value) logppx: 6.526461660861969\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] #quality_metric: host=algo-1, epoch=87, train total_loss <loss>=6.526461660861969\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] patience losses:[6.53684667746226, 6.529122081067827, 6.530893590715197, 6.529048244158427, 6.5236866805288525] min patience loss:6.5236866805288525 current loss:6.526461660861969 absolute loss difference:0.002774980333116517\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] #progress_metric: host=algo-1, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650697.069057, \"EndTime\": 1623650698.3274167, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 86, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 393588.0, \"count\": 1, \"min\": 393588, \"max\": 393588}, \"Total Batches Seen\": {\"sum\": 3132.0, \"count\": 1, \"min\": 3132, \"max\": 3132}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 174.0, \"count\": 1, \"min\": 174, \"max\": 174}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3594.5429143198776 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:58 INFO 140515461125952] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:04:59.584] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1255, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] # Finished training epoch 88 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] Loss (name: value) total: 6.5097971889707775\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] Loss (name: value) kld: 0.21634826850559977\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] Loss (name: value) recons: 6.2934489316410485\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] Loss (name: value) logppx: 6.5097971889707775\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] #quality_metric: host=algo-1, epoch=88, train total_loss <loss>=6.5097971889707775\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] patience losses:[6.529122081067827, 6.530893590715197, 6.529048244158427, 6.5236866805288525, 6.526461660861969] min patience loss:6.5236866805288525 current loss:6.5097971889707775 absolute loss difference:0.013889491558074951\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650698.3278754, \"EndTime\": 1623650699.5897737, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 87, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 398112.0, \"count\": 1, \"min\": 398112, \"max\": 398112}, \"Total Batches Seen\": {\"sum\": 3168.0, \"count\": 1, \"min\": 3168, \"max\": 3168}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 176.0, \"count\": 1, \"min\": 176, \"max\": 176}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3584.578591401786 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:04:59 INFO 140515461125952] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:00.305] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1274, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] # Finished training epoch 90 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] Loss (name: value) total: 6.492472052574158\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] Loss (name: value) kld: 0.22024332669874033\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] Loss (name: value) recons: 6.272228704558478\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] Loss (name: value) logppx: 6.492472052574158\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] #quality_metric: host=algo-2, epoch=90, train total_loss <loss>=6.492472052574158\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] patience losses:[6.507407214906481, 6.51290946536594, 6.507667356067234, 6.503050095505184, 6.492812600400713] min patience loss:6.492812600400713 current loss:6.492472052574158 absolute loss difference:0.0003405478265552375\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] #progress_metric: host=algo-2, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650699.0306885, \"EndTime\": 1623650700.3110707, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 89, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 407430.0, \"count\": 1, \"min\": 407430, \"max\": 407430}, \"Total Batches Seen\": {\"sum\": 3240.0, \"count\": 1, \"min\": 3240, \"max\": 3240}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3535.0555706462296 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:00 INFO 140618438534976] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:00.861] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1270, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] # Finished training epoch 89 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] Loss (name: value) total: 6.522544927067226\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] Loss (name: value) kld: 0.22059060633182526\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] Loss (name: value) recons: 6.301954262786442\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] Loss (name: value) logppx: 6.522544927067226\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] #quality_metric: host=algo-1, epoch=89, train total_loss <loss>=6.522544927067226\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] patience losses:[6.530893590715197, 6.529048244158427, 6.5236866805288525, 6.526461660861969, 6.5097971889707775] min patience loss:6.5097971889707775 current loss:6.522544927067226 absolute loss difference:0.012747738096448913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] #progress_metric: host=algo-1, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650699.590309, \"EndTime\": 1623650700.8627264, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 88, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 402636.0, \"count\": 1, \"min\": 402636, \"max\": 402636}, \"Total Batches Seen\": {\"sum\": 3204.0, \"count\": 1, \"min\": 3204, \"max\": 3204}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 178.0, \"count\": 1, \"min\": 178, \"max\": 178}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3554.030216043475 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:00 INFO 140515461125952] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:01.592] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1280, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] # Finished training epoch 91 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] Loss (name: value) total: 6.499760925769806\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] Loss (name: value) kld: 0.22547331638634205\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] Loss (name: value) recons: 6.274287548330095\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] Loss (name: value) logppx: 6.499760925769806\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] #quality_metric: host=algo-2, epoch=91, train total_loss <loss>=6.499760925769806\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] patience losses:[6.51290946536594, 6.507667356067234, 6.503050095505184, 6.492812600400713, 6.492472052574158] min patience loss:6.492472052574158 current loss:6.499760925769806 absolute loss difference:0.007288873195648193\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] #progress_metric: host=algo-2, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650700.3114479, \"EndTime\": 1623650701.5939133, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 90, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 411957.0, \"count\": 1, \"min\": 411957, \"max\": 411957}, \"Total Batches Seen\": {\"sum\": 3276.0, \"count\": 1, \"min\": 3276, \"max\": 3276}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3529.445896220137 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:01 INFO 140618438534976] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:02.986] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1391, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] # Finished training epoch 92 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] Loss (name: value) total: 6.492396997080909\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] Loss (name: value) kld: 0.22802993944949573\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] Loss (name: value) recons: 6.264367043972015\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] Loss (name: value) logppx: 6.492396997080909\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] #quality_metric: host=algo-2, epoch=92, train total_loss <loss>=6.492396997080909\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] patience losses:[6.507667356067234, 6.503050095505184, 6.492812600400713, 6.492472052574158, 6.499760925769806] min patience loss:6.492472052574158 current loss:6.492396997080909 absolute loss difference:7.505549324893224e-05\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] Timing: train: 1.39s, val: 0.00s, epoch: 1.40s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] #progress_metric: host=algo-2, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650701.594282, \"EndTime\": 1623650702.9908392, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 91, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 416484.0, \"count\": 1, \"min\": 416484, \"max\": 416484}, \"Total Batches Seen\": {\"sum\": 3312.0, \"count\": 1, \"min\": 3312, \"max\": 3312}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 184.0, \"count\": 1, \"min\": 184, \"max\": 184}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3241.1552129752627 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:02 INFO 140618438534976] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:02.309] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1445, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] # Finished training epoch 90 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] Loss (name: value) total: 6.51726829343372\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] Loss (name: value) kld: 0.22101502430935702\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] Loss (name: value) recons: 6.2962532374593945\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] Loss (name: value) logppx: 6.51726829343372\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] #quality_metric: host=algo-1, epoch=90, train total_loss <loss>=6.51726829343372\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] patience losses:[6.529048244158427, 6.5236866805288525, 6.526461660861969, 6.5097971889707775, 6.522544927067226] min patience loss:6.5097971889707775 current loss:6.51726829343372 absolute loss difference:0.00747110446294208\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] Timing: train: 1.45s, val: 0.00s, epoch: 1.45s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650700.8633616, \"EndTime\": 1623650702.3109105, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 89, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 407160.0, \"count\": 1, \"min\": 407160, \"max\": 407160}, \"Total Batches Seen\": {\"sum\": 3240.0, \"count\": 1, \"min\": 3240, \"max\": 3240}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3124.9320743816597 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:02 INFO 140515461125952] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:03.586] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1274, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] # Finished training epoch 91 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] Loss (name: value) total: 6.5162579615910845\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] Loss (name: value) kld: 0.2264147765106625\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] Loss (name: value) recons: 6.289843188391791\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] Loss (name: value) logppx: 6.5162579615910845\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] #quality_metric: host=algo-1, epoch=91, train total_loss <loss>=6.5162579615910845\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] patience losses:[6.5236866805288525, 6.526461660861969, 6.5097971889707775, 6.522544927067226, 6.51726829343372] min patience loss:6.5097971889707775 current loss:6.5162579615910845 absolute loss difference:0.006460772620306976\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] #progress_metric: host=algo-1, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650702.3111947, \"EndTime\": 1623650703.5878818, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 90, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 411684.0, \"count\": 1, \"min\": 411684, \"max\": 411684}, \"Total Batches Seen\": {\"sum\": 3276.0, \"count\": 1, \"min\": 3276, \"max\": 3276}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3543.051372759759 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:03 INFO 140515461125952] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:04.231] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1239, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] # Finished training epoch 93 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] Loss (name: value) total: 6.485638015800053\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] Loss (name: value) kld: 0.2263584118336439\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] Loss (name: value) recons: 6.259279621971978\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] Loss (name: value) logppx: 6.485638015800053\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] #quality_metric: host=algo-2, epoch=93, train total_loss <loss>=6.485638015800053\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] patience losses:[6.503050095505184, 6.492812600400713, 6.492472052574158, 6.499760925769806, 6.492396997080909] min patience loss:6.492396997080909 current loss:6.485638015800053 absolute loss difference:0.006758981280856169\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] #progress_metric: host=algo-2, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650702.9911573, \"EndTime\": 1623650704.2358422, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 92, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 421011.0, \"count\": 1, \"min\": 421011, \"max\": 421011}, \"Total Batches Seen\": {\"sum\": 3348.0, \"count\": 1, \"min\": 3348, \"max\": 3348}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 186.0, \"count\": 1, \"min\": 186, \"max\": 186}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3636.537682251926 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:04 INFO 140618438534976] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:04.892] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1304, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] # Finished training epoch 92 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] Loss (name: value) total: 6.503736919826931\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] Loss (name: value) kld: 0.22259839520686203\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] Loss (name: value) recons: 6.281138526068793\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] Loss (name: value) logppx: 6.503736919826931\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] #quality_metric: host=algo-1, epoch=92, train total_loss <loss>=6.503736919826931\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] patience losses:[6.526461660861969, 6.5097971889707775, 6.522544927067226, 6.51726829343372, 6.5162579615910845] min patience loss:6.5097971889707775 current loss:6.503736919826931 absolute loss difference:0.006060269143846497\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] Timing: train: 1.31s, val: 0.00s, epoch: 1.31s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650703.5882182, \"EndTime\": 1623650704.8979084, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 91, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 416208.0, \"count\": 1, \"min\": 416208, \"max\": 416208}, \"Total Batches Seen\": {\"sum\": 3312.0, \"count\": 1, \"min\": 3312, \"max\": 3312}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 184.0, \"count\": 1, \"min\": 184, \"max\": 184}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3453.843223989418 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:04 INFO 140515461125952] # Starting training for epoch 93\u001b[0m\n",
      "\n",
      "2021-06-14 06:05:16 Uploading - Uploading generated training model\u001b[35m[2021-06-14 06:05:05.658] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1421, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] # Finished training epoch 94 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] Loss (name: value) total: 6.486709614594777\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] Loss (name: value) kld: 0.22812016970581478\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] Loss (name: value) recons: 6.258589453167385\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] Loss (name: value) logppx: 6.486709614594777\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] #quality_metric: host=algo-2, epoch=94, train total_loss <loss>=6.486709614594777\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] patience losses:[6.492812600400713, 6.492472052574158, 6.499760925769806, 6.492396997080909, 6.485638015800053] min patience loss:6.485638015800053 current loss:6.486709614594777 absolute loss difference:0.0010715987947245154\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] Timing: train: 1.43s, val: 0.00s, epoch: 1.43s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] #progress_metric: host=algo-2, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650704.2362163, \"EndTime\": 1623650705.6677632, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 93, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 425538.0, \"count\": 1, \"min\": 425538, \"max\": 425538}, \"Total Batches Seen\": {\"sum\": 3384.0, \"count\": 1, \"min\": 3384, \"max\": 3384}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 188.0, \"count\": 1, \"min\": 188, \"max\": 188}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3161.9347490500513 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:05 INFO 140618438534976] # Starting training for epoch 95\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:06.388] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1487, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] # Finished training epoch 93 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] Loss (name: value) total: 6.510647502210405\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] Loss (name: value) kld: 0.2282121984495057\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] Loss (name: value) recons: 6.282435278097789\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] Loss (name: value) logppx: 6.510647502210405\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] #quality_metric: host=algo-1, epoch=93, train total_loss <loss>=6.510647502210405\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] patience losses:[6.5097971889707775, 6.522544927067226, 6.51726829343372, 6.5162579615910845, 6.503736919826931] min patience loss:6.503736919826931 current loss:6.510647502210405 absolute loss difference:0.006910582383474306\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] Timing: train: 1.49s, val: 0.00s, epoch: 1.49s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] #progress_metric: host=algo-1, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650704.8982, \"EndTime\": 1623650706.389641, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 92, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 420732.0, \"count\": 1, \"min\": 420732, \"max\": 420732}, \"Total Batches Seen\": {\"sum\": 3348.0, \"count\": 1, \"min\": 3348, \"max\": 3348}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 186.0, \"count\": 1, \"min\": 186, \"max\": 186}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3032.475690851959 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:06 INFO 140515461125952] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:07.270] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1602, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] # Finished training epoch 95 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] Loss (name: value) total: 6.491687973340352\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] Loss (name: value) kld: 0.23440894463823903\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] Loss (name: value) recons: 6.257279051674737\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] Loss (name: value) logppx: 6.491687973340352\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] #quality_metric: host=algo-2, epoch=95, train total_loss <loss>=6.491687973340352\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] patience losses:[6.492472052574158, 6.499760925769806, 6.492396997080909, 6.485638015800053, 6.486709614594777] min patience loss:6.485638015800053 current loss:6.491687973340352 absolute loss difference:0.0060499575402994665\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] Timing: train: 1.60s, val: 0.00s, epoch: 1.60s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] #progress_metric: host=algo-2, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650705.668042, \"EndTime\": 1623650707.2720187, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 94, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 430065.0, \"count\": 1, \"min\": 430065, \"max\": 430065}, \"Total Batches Seen\": {\"sum\": 3420.0, \"count\": 1, \"min\": 3420, \"max\": 3420}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 190.0, \"count\": 1, \"min\": 190, \"max\": 190}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=2822.0514192139412 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:07 INFO 140618438534976] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:08.705] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1431, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] # Finished training epoch 96 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] Loss (name: value) total: 6.476474563280742\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] Loss (name: value) kld: 0.2334537917955054\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] Loss (name: value) recons: 6.243020806047651\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] Loss (name: value) logppx: 6.476474563280742\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] #quality_metric: host=algo-2, epoch=96, train total_loss <loss>=6.476474563280742\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] patience losses:[6.499760925769806, 6.492396997080909, 6.485638015800053, 6.486709614594777, 6.491687973340352] min patience loss:6.485638015800053 current loss:6.476474563280742 absolute loss difference:0.009163452519310944\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] Timing: train: 1.43s, val: 0.00s, epoch: 1.44s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] #progress_metric: host=algo-2, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650707.2723417, \"EndTime\": 1623650708.7091706, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 95, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 434592.0, \"count\": 1, \"min\": 434592, \"max\": 434592}, \"Total Batches Seen\": {\"sum\": 3456.0, \"count\": 1, \"min\": 3456, \"max\": 3456}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 192.0, \"count\": 1, \"min\": 192, \"max\": 192}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3150.3109822291176 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:08 INFO 140618438534976] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:08.119] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1728, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] # Finished training epoch 94 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] Loss (name: value) total: 6.510589520136516\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] Loss (name: value) kld: 0.22900768866141638\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] Loss (name: value) recons: 6.2815818720393715\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] Loss (name: value) logppx: 6.510589520136516\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] #quality_metric: host=algo-1, epoch=94, train total_loss <loss>=6.510589520136516\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] patience losses:[6.522544927067226, 6.51726829343372, 6.5162579615910845, 6.503736919826931, 6.510647502210405] min patience loss:6.503736919826931 current loss:6.510589520136516 absolute loss difference:0.006852600309584567\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] Timing: train: 1.73s, val: 0.00s, epoch: 1.73s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650706.390212, \"EndTime\": 1623650708.1215985, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 93, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 425256.0, \"count\": 1, \"min\": 425256, \"max\": 425256}, \"Total Batches Seen\": {\"sum\": 3384.0, \"count\": 1, \"min\": 3384, \"max\": 3384}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 188.0, \"count\": 1, \"min\": 188, \"max\": 188}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=2612.670459564878 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:08 INFO 140515461125952] # Starting training for epoch 95\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:09.864] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1154, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] # Finished training epoch 97 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] Loss (name: value) total: 6.476095484362708\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] Loss (name: value) kld: 0.23431248176429006\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] Loss (name: value) recons: 6.241783022880554\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] Loss (name: value) logppx: 6.476095484362708\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] #quality_metric: host=algo-2, epoch=97, train total_loss <loss>=6.476095484362708\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] patience losses:[6.492396997080909, 6.485638015800053, 6.486709614594777, 6.491687973340352, 6.476474563280742] min patience loss:6.476474563280742 current loss:6.476095484362708 absolute loss difference:0.00037907891803357074\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] Timing: train: 1.16s, val: 0.01s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] #progress_metric: host=algo-2, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650708.7094784, \"EndTime\": 1623650709.8716807, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 96, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 439119.0, \"count\": 1, \"min\": 439119, \"max\": 439119}, \"Total Batches Seen\": {\"sum\": 3492.0, \"count\": 1, \"min\": 3492, \"max\": 3492}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 194.0, \"count\": 1, \"min\": 194, \"max\": 194}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3894.671528199406 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:09 INFO 140618438534976] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:09.372] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1247, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] # Finished training epoch 95 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] Loss (name: value) total: 6.503778139750163\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] Loss (name: value) kld: 0.23184440036614737\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] Loss (name: value) recons: 6.271933714548747\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] Loss (name: value) logppx: 6.503778139750163\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] #quality_metric: host=algo-1, epoch=95, train total_loss <loss>=6.503778139750163\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] patience losses:[6.51726829343372, 6.5162579615910845, 6.503736919826931, 6.510647502210405, 6.510589520136516] min patience loss:6.503736919826931 current loss:6.503778139750163 absolute loss difference:4.121992323202761e-05\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650708.1218896, \"EndTime\": 1623650709.3743322, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 94, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 429780.0, \"count\": 1, \"min\": 429780, \"max\": 429780}, \"Total Batches Seen\": {\"sum\": 3420.0, \"count\": 1, \"min\": 3420, \"max\": 3420}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 190.0, \"count\": 1, \"min\": 190, \"max\": 190}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3611.7661909167896 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:09 INFO 140515461125952] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:10.588] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1213, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] # Finished training epoch 96 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] Loss (name: value) total: 6.50122818019655\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] Loss (name: value) kld: 0.23035214241180155\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] Loss (name: value) recons: 6.270876010258992\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] Loss (name: value) logppx: 6.50122818019655\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] #quality_metric: host=algo-1, epoch=96, train total_loss <loss>=6.50122818019655\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] patience losses:[6.5162579615910845, 6.503736919826931, 6.510647502210405, 6.510589520136516, 6.503778139750163] min patience loss:6.503736919826931 current loss:6.50122818019655 absolute loss difference:0.002508739630380674\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650709.3745866, \"EndTime\": 1623650710.593917, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 95, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 434304.0, \"count\": 1, \"min\": 434304, \"max\": 434304}, \"Total Batches Seen\": {\"sum\": 3456.0, \"count\": 1, \"min\": 3456, \"max\": 3456}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 192.0, \"count\": 1, \"min\": 192, \"max\": 192}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3709.822066865058 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:10 INFO 140515461125952] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:11.095] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 1222, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] # Finished training epoch 98 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] Loss (name: value) total: 6.481136937936147\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] Loss (name: value) kld: 0.2392643257561657\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] Loss (name: value) recons: 6.241872588793437\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] Loss (name: value) logppx: 6.481136937936147\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] #quality_metric: host=algo-2, epoch=98, train total_loss <loss>=6.481136937936147\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] patience losses:[6.485638015800053, 6.486709614594777, 6.491687973340352, 6.476474563280742, 6.476095484362708] min patience loss:6.476095484362708 current loss:6.481136937936147 absolute loss difference:0.005041453573438659\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] #progress_metric: host=algo-2, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650709.8719842, \"EndTime\": 1623650711.0963025, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 97, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 443646.0, \"count\": 1, \"min\": 443646, \"max\": 443646}, \"Total Batches Seen\": {\"sum\": 3528.0, \"count\": 1, \"min\": 3528, \"max\": 3528}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 196.0, \"count\": 1, \"min\": 196, \"max\": 196}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3697.0193700258474 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:11 INFO 140618438534976] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:11.851] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1256, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] # Finished training epoch 97 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] Loss (name: value) total: 6.5048129691018\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] Loss (name: value) kld: 0.23505550747116408\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] Loss (name: value) recons: 6.2697574562496605\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] Loss (name: value) logppx: 6.5048129691018\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] #quality_metric: host=algo-1, epoch=97, train total_loss <loss>=6.5048129691018\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] patience losses:[6.503736919826931, 6.510647502210405, 6.510589520136516, 6.503778139750163, 6.50122818019655] min patience loss:6.50122818019655 current loss:6.5048129691018 absolute loss difference:0.003584788905249603\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] #progress_metric: host=algo-1, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650710.594254, \"EndTime\": 1623650711.8558261, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 96, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 438828.0, \"count\": 1, \"min\": 438828, \"max\": 438828}, \"Total Batches Seen\": {\"sum\": 3492.0, \"count\": 1, \"min\": 3492, \"max\": 3492}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 194.0, \"count\": 1, \"min\": 194, \"max\": 194}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3584.2237927088604 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:11 INFO 140515461125952] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:12.333] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 1236, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] # Finished training epoch 99 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] Loss (name: value) total: 6.46714907222324\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] Loss (name: value) kld: 0.23496158690088326\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] Loss (name: value) recons: 6.232187509536743\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] Loss (name: value) logppx: 6.46714907222324\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] #quality_metric: host=algo-2, epoch=99, train total_loss <loss>=6.46714907222324\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] patience losses:[6.486709614594777, 6.491687973340352, 6.476474563280742, 6.476095484362708, 6.481136937936147] min patience loss:6.476095484362708 current loss:6.46714907222324 absolute loss difference:0.00894641213946823\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] #progress_metric: host=algo-2, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650711.0966012, \"EndTime\": 1623650712.3401656, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 98, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 448173.0, \"count\": 1, \"min\": 448173, \"max\": 448173}, \"Total Batches Seen\": {\"sum\": 3564.0, \"count\": 1, \"min\": 3564, \"max\": 3564}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 198.0, \"count\": 1, \"min\": 198, \"max\": 198}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3639.856627988091 records/second\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] \u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:12 INFO 140618438534976] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:13.637] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 1296, \"num_examples\": 36, \"num_bytes\": 1053008}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] # Finished training epoch 100 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] Loss (name: value) total: 6.479018178251055\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] Loss (name: value) kld: 0.24069182947278023\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] Loss (name: value) recons: 6.238326357470618\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] Loss (name: value) logppx: 6.479018178251055\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] #quality_metric: host=algo-2, epoch=100, train total_loss <loss>=6.479018178251055\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] patience losses:[6.491687973340352, 6.476474563280742, 6.476095484362708, 6.481136937936147, 6.46714907222324] min patience loss:6.46714907222324 current loss:6.479018178251055 absolute loss difference:0.01186910602781488\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] Timing: train: 1.30s, val: 0.00s, epoch: 1.30s\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] #progress_metric: host=algo-2, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650712.3404834, \"EndTime\": 1623650713.6387632, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"epoch\": 99, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 452700.0, \"count\": 1, \"min\": 452700, \"max\": 452700}, \"Total Batches Seen\": {\"sum\": 3600.0, \"count\": 1, \"min\": 3600, \"max\": 3600}, \"Max Records Seen Between Resets\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Number of Records Since Last Reset\": {\"sum\": 4527.0, \"count\": 1, \"min\": 4527, \"max\": 4527}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:13 INFO 140618438534976] #throughput_metric: host=algo-2, train throughput=3486.517784852069 records/second\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:13.133] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 1275, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] # Finished training epoch 98 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] Loss (name: value) total: 6.4977688325775995\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] Loss (name: value) kld: 0.23421120126214293\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] Loss (name: value) recons: 6.263557599650489\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] Loss (name: value) logppx: 6.4977688325775995\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] #quality_metric: host=algo-1, epoch=98, train total_loss <loss>=6.4977688325775995\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] patience losses:[6.510647502210405, 6.510589520136516, 6.503778139750163, 6.50122818019655, 6.5048129691018] min patience loss:6.50122818019655 current loss:6.4977688325775995 absolute loss difference:0.0034593476189508365\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650711.8568702, \"EndTime\": 1623650713.1384025, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 97, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 443352.0, \"count\": 1, \"min\": 443352, \"max\": 443352}, \"Total Batches Seen\": {\"sum\": 3528.0, \"count\": 1, \"min\": 3528, \"max\": 3528}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 196.0, \"count\": 1, \"min\": 196, \"max\": 196}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3529.7576769080006 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:13 INFO 140515461125952] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:14.281] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 1142, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] # Finished training epoch 99 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] Loss (name: value) total: 6.485969768630134\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] Loss (name: value) kld: 0.23485577872229946\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] Loss (name: value) recons: 6.251114004188114\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] Loss (name: value) logppx: 6.485969768630134\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] #quality_metric: host=algo-1, epoch=99, train total_loss <loss>=6.485969768630134\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] patience losses:[6.510589520136516, 6.503778139750163, 6.50122818019655, 6.5048129691018, 6.4977688325775995] min patience loss:6.4977688325775995 current loss:6.485969768630134 absolute loss difference:0.011799063947465882\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] Timing: train: 1.14s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] #progress_metric: host=algo-1, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650713.1386814, \"EndTime\": 1623650714.2866907, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 98, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 447876.0, \"count\": 1, \"min\": 447876, \"max\": 447876}, \"Total Batches Seen\": {\"sum\": 3564.0, \"count\": 1, \"min\": 3564, \"max\": 3564}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 198.0, \"count\": 1, \"min\": 198, \"max\": 198}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=3940.1847296757405 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] \u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:14 INFO 140515461125952] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Best model based on early stopping at epoch 99. Best loss: 6.46714907222324\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Topics from epoch:final (num_topics:30) [, tu 0.66]:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1547 578 558 557 48 924 295 1592 1531 309 1734 409 410 1369 1568 287 1196 1588 1186 1960\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1128 174 225 1526 1237 183 439 511 1494 1602 1117 683 797 1859 1850 418 1425 1323 1159 1585\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1328 1475 31 104 1602 396 1601 225 1243 200 511 1556 595 245 1290 775 1007 707 207 1711\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1031 861 433 31 815 912 1801 732 1196 82 1171 120 1170 203 1007 1336 27 1534 1953 1725\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 174 183 1751 513 341 1454 761 668 1526 1185 527 153 1123 1184 1879 1676 783 440 1847 1416\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 225 1602 1855 1128 432 1127 595 1833 1110 224 1425 641 200 350 144 1636 880 1346 795 6\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 420 511 1527 1526 174 1128 793 34 1751 1947 963 1637 183 198 1290 1211 826 1306 439 341\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 513 174 1527 1131 1528 939 341 1526 668 146 411 1765 1102 1128 1751 142 928 741 770 214\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1366 1906 1327 1992 177 681 1231 891 832 1109 1768 1989 1339 1955 1111 1131 1990 689 1075 1907\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 892 355 332 1395 68 1986 1031 183 612 871 1242 703 458 1732 1513 1263 362 912 1972 1062\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 406 1908 814 405 761 124 1319 423 1751 153 1639 767 440 1729 1441 513 1940 1528 1242 94\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 476 1954 1312 1575 1584 232 1783 1174 1294 674 1574 1826 961 1293 201 481 192 713 314 960\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 27 1656 439 704 1170 1338 1029 1635 556 798 861 31 148 1751 1855 1462 1030 512 1497 1928\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1137 1022 170 135 1024 421 1147 1023 138 1146 731 976 390 134 1831 137 1298 1343 1419 1893\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1145 906 896 893 88 895 721 1856 1486 717 97 1244 892 1626 1412 133 1673 904 1487 1121\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1765 741 939 1131 1527 527 174 197 145 1526 1293 1528 1821 513 1563 1751 439 952 1170 1242\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1031 1765 753 783 1059 1338 861 792 27 71 82 885 12 815 386 1187 756 1752 751 1680\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1031 1656 556 1752 27 704 989 1030 1605 1720 1171 120 1887 31 885 274 861 1442 365 187\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 183 1815 900 1128 1526 174 1729 198 793 1131 806 1092 1117 511 1464 1751 278 562 1726 1349\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 212 472 283 1192 1825 1193 1422 734 191 1727 1894 1096 1177 1292 231 411 1705 1115 1582 481\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1765 1528 513 939 642 1751 1131 1496 741 683 1210 145 1526 793 1934 926 1527 530 507 240\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 350 595 1556 1205 104 1815 1636 1691 1092 1602 200 59 1243 1601 1432 144 853 1360 1241 1491\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 861 1031 1780 1602 1414 1812 71 350 439 1850 1366 210 1683 1431 1636 104 68 200 1029 224\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 814 767 406 664 1751 440 405 383 761 10 124 1947 755 994 1769 333 219 1533 850 279\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1440 1169 892 1196 27 1201 439 308 221 1139 414 1212 527 687 732 1780 1819 1186 1385 1414\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1141 584 1904 508 904 1578 1654 112 629 1522 171 1260 1701 294 172 1324 788 742 736 1255\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 153 308 1931 1780 642 952 838 527 1928 106 221 379 1765 647 1434 1724 246 55 1002 1116\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 767 405 814 143 406 252 761 383 1397 1412 806 1940 755 1769 1554 297 692 124 1639 423\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1751 198 174 1526 528 1585 1211 225 668 200 1533 1092 513 1527 1102 1019 1855 793 383 1306\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] 1765 145 741 1679 1355 1618 642 818 939 1631 1020 1260 1906 181 1131 1224 411 550 142 1070\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Saved checkpoint to \"/tmp/tmp61ymgof4/state-0001.params\"\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:15.377] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 130602, \"num_examples\": 1, \"num_bytes\": 34100}\u001b[0m\n",
      "\u001b[35m[2021-06-14 06:05:15.501] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 124, \"num_examples\": 18, \"num_bytes\": 509688}\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Loss (name: value) total: 6.705173043643727\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Loss (name: value) kld: 0.2158487956313526\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Loss (name: value) recons: 6.489324317258947\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] Loss (name: value) logppx: 6.705173043643727\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650715.376882, \"EndTime\": 1623650715.502231, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Max Batches Seen Between Resets\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Number of Batches Since Last Reset\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}}}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/14/2021 06:05:15 INFO 140618438534976] #test_score (algo-2) : ('log_perplexity', 6.705173043643727)\u001b[0m\n",
      "\u001b[35m#metrics {\"StartTime\": 1623650584.7635927, \"EndTime\": 1623650715.5032985, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-2\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 2676.7730712890625, \"count\": 1, \"min\": 2676.7730712890625, \"max\": 2676.7730712890625}, \"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"early_stop.time\": {\"sum\": 306.40506744384766, \"count\": 100, \"min\": 0.027179718017578125, \"max\": 6.52766227722168}, \"update.time\": {\"sum\": 126130.00917434692, \"count\": 100, \"min\": 1145.439624786377, \"max\": 1603.775978088379}, \"finalize.time\": {\"sum\": 160.81476211547852, \"count\": 1, \"min\": 160.81476211547852, \"max\": 160.81476211547852}, \"model.serialize.time\": {\"sum\": 5.132436752319336, \"count\": 1, \"min\": 5.132436752319336, \"max\": 5.132436752319336}, \"model.score.time\": {\"sum\": 125.29349327087402, \"count\": 1, \"min\": 125.29349327087402, \"max\": 125.29349327087402}, \"setuptime\": {\"sum\": 574.3310451507568, \"count\": 1, \"min\": 574.3310451507568, \"max\": 574.3310451507568}, \"totaltime\": {\"sum\": 131357.3546409607, \"count\": 1, \"min\": 131357.3546409607, \"max\": 131357.3546409607}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:15.195] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 908, \"num_examples\": 36, \"num_bytes\": 1059840}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] # Finished training epoch 100 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) total: 6.471041487322913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) kld: 0.23608101800911957\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) recons: 6.234960522916582\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) logppx: 6.471041487322913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] #quality_metric: host=algo-1, epoch=100, train total_loss <loss>=6.471041487322913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] patience losses:[6.503778139750163, 6.50122818019655, 6.5048129691018, 6.4977688325775995, 6.485969768630134] min patience loss:6.485969768630134 current loss:6.471041487322913 absolute loss difference:0.014928281307220459\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Timing: train: 0.91s, val: 0.00s, epoch: 0.91s\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650714.2869866, \"EndTime\": 1623650715.2012448, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 99, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 452400.0, \"count\": 1, \"min\": 452400, \"max\": 452400}, \"Total Batches Seen\": {\"sum\": 3600.0, \"count\": 1, \"min\": 3600, \"max\": 3600}, \"Max Records Seen Between Resets\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Max Batches Seen Between Resets\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Reset Count\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Number of Records Since Last Reset\": {\"sum\": 4524.0, \"count\": 1, \"min\": 4524, \"max\": 4524}, \"Number of Batches Since Last Reset\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] #throughput_metric: host=algo-1, train throughput=4947.361540852233 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Best model based on early stopping at epoch 100. Best loss: 6.471041487322913\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Topics from epoch:final (num_topics:30) [, tu 0.64]:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1547 558 48 578 557 1592 1531 409 295 924 309 1734 1369 410 1588 1196 1568 1186 287 1960\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 225 1128 174 1117 797 183 1947 1159 662 1526 116 439 511 1494 1986 540 795 1969 1805 1850\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 31 1475 1328 1986 837 1243 104 200 224 1601 707 573 1711 1289 1075 1969 861 225 775 260\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1031 861 1534 27 1582 1198 203 1007 753 433 815 556 82 68 380 1170 751 1801 83 1893\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 341 183 1454 1751 1526 737 1185 174 1879 1528 1306 1661 1847 1728 761 784 1729 1170 513 741\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1855 1833 225 1636 1026 1128 1602 1007 1092 595 245 432 1110 1097 641 1348 1961 738 1127 1526\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1527 511 1526 420 183 1751 969 1528 1947 174 1128 528 1847 341 198 793 963 932 1306 1637\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 513 1528 1527 1526 1131 341 174 146 668 939 411 1102 1751 1128 969 1117 1729 1123 1092 1765\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1366 681 1992 1906 1327 177 832 891 1109 1231 1768 1339 1989 1990 1907 1111 1955 1131 1075 1296\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 892 183 1985 68 808 612 1117 912 511 1774 871 458 355 1732 425 1335 1601 1696 1739 34\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 406 1908 814 405 124 1639 761 1528 1751 1926 106 1554 440 865 1319 1703 1441 1940 767 341\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 476 1312 1954 1575 1783 1584 232 1174 1294 1826 674 961 1574 713 201 808 1293 314 481 192\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 439 27 1497 751 1702 246 704 785 1007 1513 1335 68 1635 861 1314 1801 1751 1656 200 1636\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1137 1022 135 421 1024 170 1147 1023 1146 138 731 976 390 1343 1831 134 137 1419 1893 1298\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1145 893 906 721 896 88 895 1486 1856 97 717 1244 1412 133 892 1121 1673 1487 904 1054\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1765 741 939 1527 174 1131 1526 1528 145 1170 1293 513 197 527 952 1751 1679 996 1819 642\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1031 1765 753 1338 12 1801 756 808 815 27 885 1165 82 68 1497 1448 1547 816 961 783\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1031 27 556 1656 1030 1752 989 1171 1605 704 1720 31 120 1887 861 1170 274 1442 187 516\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1526 183 174 198 1815 1729 1128 1117 900 1056 511 1211 562 385 1751 1528 969 225 707 1859\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 212 1192 472 283 1193 1422 1825 734 191 1894 1727 1096 1177 1292 411 231 1705 1115 1582 1897\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1765 1528 1527 507 1751 513 1526 1102 214 530 926 1976 642 939 1555 1210 1242 1131 741 174\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 350 1556 595 1205 104 1815 1092 1636 1691 59 200 1602 1243 1432 1601 144 1491 1855 1360 1241\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1602 1812 1780 246 210 1683 861 1031 1029 1230 1414 439 1431 1456 586 200 527 1399 1007 68\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 814 406 767 761 664 440 1751 850 124 755 10 219 420 383 1178 1397 928 1976 174 1728\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 892 1440 1819 1780 527 439 308 221 973 1212 414 1336 432 1030 1765 1400 912 246 1186 1995\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1141 584 508 1904 1578 904 629 112 1654 1522 171 1260 172 742 788 1701 294 726 1324 736\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1780 308 642 1724 1928 106 838 297 952 221 1434 527 153 379 211 647 1002 530 439 1765\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 767 405 1554 252 814 143 1412 761 383 1769 1441 406 297 1319 1553 1940 1397 714 124 1094\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 174 1751 198 383 1526 1211 225 1527 319 1092 668 793 1102 528 385 1019 1117 784 200 562\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] 1765 145 741 1679 642 1631 1355 1020 939 818 1618 1224 214 1906 1260 181 550 1070 1499 1131\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Saved checkpoint to \"/tmp/tmp3r3z1z6r/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:15.349] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 128713, \"num_examples\": 1, \"num_bytes\": 34100}\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:05:15.480] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 131, \"num_examples\": 18, \"num_bytes\": 509688}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) total: 6.687334846047794\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) kld: 0.2196617678684347\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) recons: 6.467673105352065\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] Loss (name: value) logppx: 6.687334846047794\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650715.349378, \"EndTime\": 1623650715.4815207, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Max Batches Seen Between Resets\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 2263.0, \"count\": 1, \"min\": 2263, \"max\": 2263}, \"Number of Batches Since Last Reset\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:05:15 INFO 140515461125952] #test_score (algo-1) : ('log_perplexity', 6.687334846047794)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623650586.6272943, \"EndTime\": 1623650715.4825926, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 763.0667686462402, \"count\": 1, \"min\": 763.0667686462402, \"max\": 763.0667686462402}, \"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"early_stop.time\": {\"sum\": 293.3955192565918, \"count\": 100, \"min\": 0.164031982421875, \"max\": 6.089925765991211}, \"update.time\": {\"sum\": 127732.11717605591, \"count\": 100, \"min\": 914.0040874481201, \"max\": 1749.488115310669}, \"finalize.time\": {\"sum\": 138.7765407562256, \"count\": 1, \"min\": 138.7765407562256, \"max\": 138.7765407562256}, \"model.serialize.time\": {\"sum\": 5.114316940307617, \"count\": 1, \"min\": 5.114316940307617, \"max\": 5.114316940307617}, \"model.score.time\": {\"sum\": 132.08484649658203, \"count\": 1, \"min\": 132.08484649658203, \"max\": 132.08484649658203}, \"setuptime\": {\"sum\": 47.18661308288574, \"count\": 1, \"min\": 47.18661308288574, \"max\": 47.18661308288574}, \"totaltime\": {\"sum\": 128953.92298698425, \"count\": 1, \"min\": 128953.92298698425, \"max\": 128953.92298698425}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-14 06:05:39 Completed - Training job completed\n",
      "ProfilerReport-1623650381: NoIssuesFound\n",
      "Training seconds: 390\n",
      "Billable seconds: 390\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import s3_input\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key') \n",
    "ntm.fit({'train': s3_train, 'test': s3_val_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ac161",
   "metadata": {},
   "source": [
    "To deploy the model the deploy method of sagemaker.estimator.Estimator object is called. The number and type of LM instances used to host the endpoint is specified. The deployable model is made and the endpoint is launched.\n",
    "To run instances the input payload is serialized and inference output is deserialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193fa4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e1c2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import CSVSerializer, JSONDeserializer\n",
    "\n",
    "ntm_predictor.serializer = CSVSerializer()\n",
    "ntm_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95ea7b81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3a72cd3cb6da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtopicvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtopicnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopicvec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for item in np.array(vectors.todense()):\n",
    "    np.shape(item)\n",
    "    results = ntm_predictor.predict(item)\n",
    "    predictions.append(np.array([prediction['topic_weights'] for prediction in results['predictions']]))\n",
    "    \n",
    "predictions = np.array([np.ndarray.flatten(x) for x in predictions])\n",
    "topicvec = train_labels[newidx]\n",
    "topicnames = [categories[x] for x in topicvec]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf33901",
   "metadata": {},
   "source": [
    "A dictionary linking the shuffled labels to the original labels is created and the training data is stored in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef3f50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = newidx \n",
    "labeldict = dict(zip(newidx,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15c5852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape =  (11314, 30)\n",
      "train_labels shape =  (11314,)\n",
      "20newsgroups/knn/train\n",
      "uploaded training data location: s3://sagemaker-aw/20newsgroups/knn/train\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "print('train_features shape = ', predictions.shape)\n",
    "print('train_labels shape = ', labels.shape)\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, predictions, labels)\n",
    "buf.seek(0)\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "key = 'knn/train'\n",
    "fname = os.path.join(prefix, key)\n",
    "print(fname)\n",
    "boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2ed3b",
   "metadata": {},
   "source": [
    "The helper function is used to create a k-NN estimator with index_metric set to cosine to us cosine similarity for computing the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c85e0e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 06:42:03 Starting - Starting the training job...\n",
      "2021-06-14 06:42:27 Starting - Launching requested ML instancesProfilerReport-1623652923: InProgress\n",
      "......\n",
      "2021-06-14 06:43:27 Starting - Preparing the instances for training......\n",
      "2021-06-14 06:44:27 Downloading - Downloading input data\n",
      "2021-06-14 06:44:27 Training - Downloading the training image..............\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:43 INFO 139840775472960] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': 'auto', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'L2', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:43 INFO 139840775472960] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '30', 'predictor_type': 'classifier', 'sample_size': '11314', 'index_metric': 'COSINE', 'k': '10'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:43 INFO 139840775472960] Final configuration: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': '30', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'COSINE', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false', 'predictor_type': 'classifier', 'sample_size': '11314', 'k': '10'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 WARNING 139840775472960] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] Final configuration: {'_kvstore': 'dist_async', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_tuning_objective_metric': '', '_faiss_index_nprobe': '5', 'epochs': '1', 'feature_dim': '30', 'faiss_index_ivf_nlists': 'auto', 'index_metric': 'COSINE', 'index_type': 'faiss.Flat', 'mini_batch_size': '5000', '_enable_profiler': 'false', 'predictor_type': 'classifier', 'sample_size': '11314', 'k': '10'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 WARNING 139840775472960] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-112-147.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2021-06-14-06-42-03-348', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/knn-2021-06-14-06-42-03-348', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e6c8d755-b851-49b3-b702-4716178415e9', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-112-147.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2021-06-14-06-42-03-348', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/knn-2021-06-14-06-42-03-348', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e6c8d755-b851-49b3-b702-4716178415e9', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'scheduler', 'DMLC_PS_ROOT_URI': '10.2.112.147', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-112-147.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2021-06-14-06-42-03-348', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/knn-2021-06-14-06-42-03-348', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e6c8d755-b851-49b3-b702-4716178415e9', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] envs={'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-112-147.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2021-06-14-06-42-03-348', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/knn-2021-06-14-06-42-03-348', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e6c8d755-b851-49b3-b702-4716178415e9', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'server', 'DMLC_PS_ROOT_URI': '10.2.112.147', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] Environment: {'ENVROOT': '/opt/amazon', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'HOSTNAME': 'ip-10-2-112-147.ec2.internal', 'TRAINING_JOB_NAME': 'knn-2021-06-14-06-42-03-348', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:600461227162:training-job/knn-2021-06-14-06-42-03-348', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/e6c8d755-b851-49b3-b702-4716178415e9', 'CANONICAL_ENVROOT': '/opt/amazon', 'PYTHONUNBUFFERED': 'TRUE', 'NVIDIA_VISIBLE_DEVICES': 'void', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python3.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PWD': '/', 'LANG': 'en_US.utf8', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'HOME': '/root', 'SHLVL': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'OMP_NUM_THREADS': '2', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/68ba049a-a695-4398-ab8d-8efcd01f4bf2', 'SAGEMAKER_HTTP_PORT': '8080', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'DMLC_ROLE': 'worker', 'DMLC_PS_ROOT_URI': '10.2.112.147', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_SERVER': '1', 'DMLC_NUM_WORKER': '1'}\u001b[0m\n",
      "\u001b[34mProcess 49 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 58 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] nvidia-smi: took 0.035 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:49 INFO 139840775472960] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[06:46:49] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\u001b[0m\n",
      "\u001b[34m[06:46:50] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\u001b[0m\n",
      "\u001b[34m[06:46:50] ../src/base.cc:47: Please install cuda driver for GPU use.  No cuda driver detected.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 ERROR 139840775472960] nvidia-smi: failed to run (127): b'/bin/sh: nvidia-smi: command not found'/\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 WARNING 139840775472960] Could not determine free memory in MB for GPU device with ID (0).\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] Using per-worker sample size = 11314 (Available virtual memory = 6073323520 bytes, GPU free memory = 0 bytes, number of workers = 1). If an out-of-memory error occurs, choose a larger instance type, use dimension reduction, decrease sample_size, and/or decrease mini_batch_size.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] Starting cluster...\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] concurrency model: async\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] ...Cluster started\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] masquerade (NAT) address: None\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] Verifying connection to 0 peer cluster(s)...\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] passive ports: None\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] ...Verified connection to 0 peer cluster(s)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623653210.57708, \"EndTime\": 1623653210.5771413, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] >>> starting FTP server on 0.0.0.0:8999, pid=1 <<<\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:46:50.577] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 867, \"num_examples\": 1, \"num_bytes\": 1440000}\u001b[0m\n",
      "\u001b[34m[2021-06-14 06:46:50.808] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 230, \"num_examples\": 3, \"num_bytes\": 3258432}\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623653210.5778153, \"EndTime\": 1623653210.8142846, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 11314.0, \"count\": 1, \"min\": 11314, \"max\": 11314}, \"Total Batches Seen\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Max Records Seen Between Resets\": {\"sum\": 11314.0, \"count\": 1, \"min\": 11314, \"max\": 11314}, \"Max Batches Seen Between Resets\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 11314.0, \"count\": 1, \"min\": 11314, \"max\": 11314}, \"Number of Batches Since Last Reset\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] #throughput_metric: host=algo-1, train throughput=47811.65193087946 records/second\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] Getting reservoir sample from algo-1...\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] 10.2.112.147:50238-[] FTP session opened (connect)\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] 10.2.112.147:50238-[anonymous] USER 'anonymous' logged in.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] 10.2.112.147:50238-[anonymous] RETR /opt/rs completed=1 bytes=1493652 seconds=0.001\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] 10.2.112.147:50238-[anonymous] FTP session closed (disconnect).\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] ...Got reservoir sample from algo-1: data=(11314, 30), labels=(11314,), NaNs=0\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] Training index...\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] ...Finished training index in 0 second(s)\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] Adding data to index...\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] ...Finished adding data to index in 0 second(s)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623653209.7099528, \"EndTime\": 1623653210.8611834, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 851.0477542877197, \"count\": 1, \"min\": 851.0477542877197, \"max\": 851.0477542877197}, \"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"update.time\": {\"sum\": 236.1752986907959, \"count\": 1, \"min\": 236.1752986907959, \"max\": 236.1752986907959}, \"finalize.time\": {\"sum\": 40.042877197265625, \"count\": 1, \"min\": 40.042877197265625, \"max\": 40.042877197265625}, \"model.serialize.time\": {\"sum\": 6.387233734130859, \"count\": 1, \"min\": 6.387233734130859, \"max\": 6.387233734130859}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139840775472960] Test data is not provided.\u001b[0m\n",
      "\u001b[34m[06/14/2021 06:46:50 INFO 139838859519744] >>> shutting down FTP server, 0 socket(s), pid=1 <<<\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623653210.8612661, \"EndTime\": 1623653210.924157, \"Dimensions\": {\"Algorithm\": \"AWS/KNN\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 19.589662551879883, \"count\": 1, \"min\": 19.589662551879883, \"max\": 19.589662551879883}, \"totaltime\": {\"sum\": 1808.3255290985107, \"count\": 1, \"min\": 1808.3255290985107, \"max\": 1808.3255290985107}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-14 06:47:01 Uploading - Uploading generated training model\n",
      "2021-06-14 06:47:01 Completed - Training job completed\n",
      "Training seconds: 166\n",
      "Billable seconds: 166\n"
     ]
    }
   ],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.c4.xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "hyperparams = {\n",
    "    'feature_dim': predictions.shape[1],\n",
    "    'k': NUM_NEIGHBORS,\n",
    "    'sample_size': predictions.shape[0],\n",
    "    'predictor_type': 'classifier' ,\n",
    "    'index_metric':'COSINE'\n",
    "}\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/knn/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f74a7ec",
   "metadata": {},
   "source": [
    "Endpoint is launched for k-NN model which will return all the cosine distances. Test data is preprocesses to run inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53c4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
